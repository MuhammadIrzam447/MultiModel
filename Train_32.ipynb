{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "C4SHn9-5RZZn",
        "kptdeoGFRdUx"
      ],
      "authorship_tag": "ABX9TyPgVEg5JcloMS5cxpR9k7SP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MuhammadIrzam447/MultiModel/blob/master/Train_32.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Notebook for the training and evalution of ViT for Multi-Class & Multi-Label Dataset i.e MM-IMDB"
      ],
      "metadata": {
        "id": "rip0FqLeRYxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read Training Dataset"
      ],
      "metadata": {
        "id": "C4SHn9-5RZZn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJo17J8LP5N-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "image_file_paths = []\n",
        "image_genre_labels = []\n",
        "enc_file_paths = []\n",
        "enc_genre_labels = []\n",
        "\n",
        "labels_file = \"/content/Dataset(s)/mm-imdb/joint/joint_train_label.txt\"\n",
        "image_folder_add = \"/content/Dataset(s)/mm-imdb/joint/train\"\n",
        "\n",
        "with open(labels_file, 'r') as file:\n",
        "    for line in file:\n",
        "        parts = line.strip().split('|')\n",
        "        filename = parts[0].strip()\n",
        "        labels = parts[1].strip().split(', ')\n",
        "        image_path = os.path.join(image_folder_add, filename)\n",
        "\n",
        "        if filename.endswith(\"_3.png\"):\n",
        "            enc_file_paths.append(image_path)\n",
        "            enc_genre_labels.append(labels)\n",
        "\n",
        "        elif filename.endswith(\"_4.png\"):\n",
        "            image_file_paths.append(image_path)\n",
        "            image_genre_labels.append(labels)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(enc_file_paths), len(image_file_paths)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKAkggLBScIl",
        "outputId": "015f0e03-09ae-40fe-a4bd-002873b54d5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15552, 15552)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "label_counts = defaultdict(int)\n",
        "\n",
        "for labels in image_genre_labels:\n",
        "    for label in labels:\n",
        "        label_counts[label] += 1\n",
        "\n",
        "label_count_list = [(label, count) for label, count in label_counts.items()]\n",
        "sorted_label_count_list = sorted(label_count_list, key=lambda x: x[1], reverse=True)\n",
        "for label, count in sorted_label_count_list:\n",
        "    print(f\"{label}: {count}\")\n",
        "\n",
        "print(\"Total Labels: \", len(label_count_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0q3zFSp_P_8X",
        "outputId": "a199b4ae-c2dd-4c1d-9dfb-4c9837c07ddc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drama: 8424\n",
            "Comedy: 5108\n",
            "Romance: 3226\n",
            "Thriller: 3113\n",
            "Crime: 2293\n",
            "Action: 2155\n",
            "Adventure: 1611\n",
            "Horror: 1603\n",
            "Documentary: 1234\n",
            "Mystery: 1231\n",
            "Sci-Fi: 1212\n",
            "Fantasy: 1162\n",
            "Family: 978\n",
            "War: 806\n",
            "Biography: 788\n",
            "History: 680\n",
            "Music: 634\n",
            "Animation: 586\n",
            "Musical: 503\n",
            "Western: 423\n",
            "Sport: 379\n",
            "Short: 281\n",
            "Film-Noir: 202\n",
            "News: 39\n",
            "Talk-Show: 2\n",
            "Reality-TV: 1\n",
            "Total Labels:  26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "min_label_count = 40\n",
        "valid_labels = [label for label, count in label_counts.items() if count >= min_label_count]\n",
        "valid_labels = sorted(list(valid_labels))"
      ],
      "metadata": {
        "id": "rzm7XXqzQEEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_image_paths = []\n",
        "filtered_genre_labels = []\n",
        "\n",
        "for image_path, labels in zip(image_file_paths, image_genre_labels):\n",
        "    valid_labels_for_sample = [label for label in labels if label in valid_labels]\n",
        "\n",
        "    if valid_labels_for_sample:\n",
        "        filtered_image_paths.append(image_path)\n",
        "        filtered_genre_labels.append(valid_labels_for_sample)"
      ],
      "metadata": {
        "id": "fy69jMIWQHPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_enc_paths = []\n",
        "filtered_enc_labels = []\n",
        "\n",
        "for image_path, labels in zip(enc_file_paths, enc_genre_labels):\n",
        "    valid_labels_for_sample = [label for label in labels if label in valid_labels]\n",
        "\n",
        "    if valid_labels_for_sample:\n",
        "        filtered_enc_paths.append(image_path)\n",
        "        filtered_enc_labels.append(valid_labels_for_sample)"
      ],
      "metadata": {
        "id": "fyK5W87jQI9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_file_paths = filtered_image_paths\n",
        "image_genre_labels = filtered_genre_labels\n",
        "enc_file_paths = filtered_enc_paths\n",
        "enc_genre_labels = filtered_enc_labels"
      ],
      "metadata": {
        "id": "un6ZWIpsQYSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(image_file_paths), len(enc_file_paths)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6KpYkZvTMPY",
        "outputId": "c04bc186-945f-4247-d36d-c2a51124339d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15552, 15552)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "combined_data = list(zip(enc_file_paths, enc_genre_labels))\n",
        "sample_size = int(0.3 * len(combined_data))\n",
        "sampled_data = random.sample(combined_data, sample_size)\n",
        "\n",
        "\n",
        "sampled_file_paths, sampled_genre_labels = zip(*sampled_data)"
      ],
      "metadata": {
        "id": "z3qUB7PCUxRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(sampled_file_paths)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BrYrQnDU58y",
        "outputId": "1890e175-b879-44d7-eaaa-349f48d206c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4665"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_file_paths = list(sampled_file_paths)\n",
        "sampled_genre_labels = list(sampled_genre_labels)"
      ],
      "metadata": {
        "id": "lCoklGzRWeJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_file_paths = image_file_paths + sampled_file_paths\n",
        "train_genre_labels = image_genre_labels + sampled_genre_labels"
      ],
      "metadata": {
        "id": "UeMEMLwXQezp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read Test Dataset"
      ],
      "metadata": {
        "id": "kptdeoGFRdUx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "test_image_file_paths = []\n",
        "test_image_genre_labels = []\n",
        "test_enc_file_paths = []\n",
        "test_enc_genre_labels = []\n",
        "\n",
        "test_labels_file = \"/content/Dataset(s)/mm-imdb/joint/joint_test_label.txt\"\n",
        "image_folder_add = \"/content/Dataset(s)/mm-imdb/joint/test\"\n",
        "\n",
        "with open(test_labels_file, 'r') as file:\n",
        "    for line in file:\n",
        "        parts = line.strip().split('|')\n",
        "        filename = parts[0].strip()\n",
        "        labels = parts[1].strip().split(', ')\n",
        "        image_path = os.path.join(image_folder_add, filename)\n",
        "\n",
        "        if filename.endswith(\"_3.png\"):\n",
        "            test_enc_file_paths.append(image_path)\n",
        "            test_enc_genre_labels.append(labels)\n",
        "\n",
        "        elif filename.endswith(\"_4.png\"):\n",
        "            test_image_file_paths.append(image_path)\n",
        "            test_image_genre_labels.append(labels)"
      ],
      "metadata": {
        "id": "HZELsYKdQqoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_enc_file_paths), len(test_image_file_paths)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pet0k_APTf6L",
        "outputId": "8346e4a4-ad6f-4e29-e124-18a9991658c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7799, 7799)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_enc_file_paths[0:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjvm1seuTogI",
        "outputId": "414f0736-6f70-46e9-bafc-a666e9cb362a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/Dataset(s)/mm-imdb/joint/test/0078718_3.png',\n",
              " '/content/Dataset(s)/mm-imdb/joint/test/0089003_3.png']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_filtered_image_paths = []\n",
        "test_filtered_genre_labels = []\n",
        "\n",
        "for image_path, labels in zip(test_image_file_paths, test_image_genre_labels):\n",
        "    valid_labels_for_sample = [label for label in labels if label in valid_labels]\n",
        "\n",
        "    if valid_labels_for_sample:\n",
        "        test_filtered_image_paths.append(image_path)\n",
        "        test_filtered_genre_labels.append(valid_labels_for_sample)"
      ],
      "metadata": {
        "id": "Eq7rExa3QtWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_filtered_enc_paths = []\n",
        "test_filtered_enc_labels = []\n",
        "\n",
        "for image_path, labels in zip(test_enc_file_paths, test_enc_genre_labels):\n",
        "    valid_labels_for_sample = [label for label in labels if label in valid_labels]\n",
        "\n",
        "    if valid_labels_for_sample:\n",
        "        test_filtered_enc_paths.append(image_path)\n",
        "        test_filtered_enc_labels.append(valid_labels_for_sample)"
      ],
      "metadata": {
        "id": "4bb3IsB_QvTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_image_file_paths = test_filtered_image_paths\n",
        "test_image_genre_labels = test_filtered_genre_labels\n",
        "test_enc_file_paths = test_filtered_enc_paths\n",
        "test_enc_genre_labels = test_filtered_enc_labels"
      ],
      "metadata": {
        "id": "U97-8jEfQxFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_image_genre_labels), len(test_enc_genre_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vPN_duPTzUE",
        "outputId": "978000e4-515e-40c5-92bb-1209a173502f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7799, 7799)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "test_combined_data = list(zip(test_enc_file_paths, test_enc_genre_labels))\n",
        "test_sample_size = int(0.3 * len(test_combined_data))\n",
        "sampled_data = random.sample(test_combined_data, test_sample_size)\n",
        "\n",
        "\n",
        "test_sampled_file_paths, test_sampled_genre_labels = zip(*sampled_data)"
      ],
      "metadata": {
        "id": "DRohk8mUXBXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sampled_file_paths = list(test_sampled_file_paths)\n",
        "test_sampled_genre_labels = list(test_sampled_genre_labels)"
      ],
      "metadata": {
        "id": "-_ckjLQ8XcD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_file_paths = test_image_file_paths + test_sampled_file_paths\n",
        "test_genre_labels = test_image_genre_labels + test_sampled_genre_labels"
      ],
      "metadata": {
        "id": "4nRe7KaqXytl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ViT and Label Encoding"
      ],
      "metadata": {
        "id": "Gf2q9CXvQ-Ca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "!pip install transformers evaluate datasets\n",
        "import requests\n",
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import *\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "Tj7yGfsBQ-g5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model_name = \"google/vit-base-patch16-224\"\n",
        "image_processor = ViTImageProcessor.from_pretrained(model_name)\n",
        "model = ViTForImageClassification.from_pretrained(model_name).to(device)"
      ],
      "metadata": {
        "id": "KyJKaFI8Bpsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "mlb = MultiLabelBinarizer()\n",
        "mlb.fit(train_genre_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "PsAxg484RCW9",
        "outputId": "1a49c1ce-6930-437c-e0e2-2a723ca3a654"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultiLabelBinarizer()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultiLabelBinarizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultiLabelBinarizer</label><div class=\"sk-toggleable__content\"><pre>MultiLabelBinarizer()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformed_train_genre_labels = mlb.transform(train_genre_labels)"
      ],
      "metadata": {
        "id": "yqL4eMrAREdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labeling_scheme = mlb.classes_\n",
        "print(len(labeling_scheme))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yFGK_j7RLyE",
        "outputId": "bb2a3231-86b5-4fa0-c013-2a91f0ebccf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_to_class = {i: label for i, label in enumerate(labeling_scheme)}"
      ],
      "metadata": {
        "id": "yYXoHk15Y1H9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Labeling scheme (binary representations): \", labeling_scheme)\n",
        "print(\"Labeling scheme (class names): \", label_to_class)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49uHTzJxY3pk",
        "outputId": "f408bb64-e0f7-4c15-827c-1e48375b6808"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labeling scheme (binary representations):  ['Action' 'Adventure' 'Animation' 'Biography' 'Comedy' 'Crime'\n",
            " 'Documentary' 'Drama' 'Family' 'Fantasy' 'Film-Noir' 'History' 'Horror'\n",
            " 'Music' 'Musical' 'Mystery' 'Romance' 'Sci-Fi' 'Short' 'Sport' 'Thriller'\n",
            " 'War' 'Western']\n",
            "Labeling scheme (class names):  {0: 'Action', 1: 'Adventure', 2: 'Animation', 3: 'Biography', 4: 'Comedy', 5: 'Crime', 6: 'Documentary', 7: 'Drama', 8: 'Family', 9: 'Fantasy', 10: 'Film-Noir', 11: 'History', 12: 'Horror', 13: 'Music', 14: 'Musical', 15: 'Mystery', 16: 'Romance', 17: 'Sci-Fi', 18: 'Short', 19: 'Sport', 20: 'Thriller', 21: 'War', 22: 'Western'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformed_train_genre_labels[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpk94aX_YY9s",
        "outputId": "117de4d1-267e-4408-9e96-89a60612c425"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "mlb1 = MultiLabelBinarizer()\n",
        "mlb1.fit(test_genre_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "DQzxXxaVRNio",
        "outputId": "20111a84-77df-4228-a86b-2b44f6c2fd93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultiLabelBinarizer()"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultiLabelBinarizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultiLabelBinarizer</label><div class=\"sk-toggleable__content\"><pre>MultiLabelBinarizer()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformed_test_genre_labels = mlb1.transform(test_genre_labels)"
      ],
      "metadata": {
        "id": "OOa_BNY0RO_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labeling_scheme = mlb.classes_\n",
        "\n",
        "label_to_class = {i: label for i, label in enumerate(labeling_scheme)}\n",
        "\n",
        "print(\"Labeling scheme (binary representations): \", labeling_scheme)\n",
        "print(\"Labeling scheme (class names): \", label_to_class)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUYKvA_6Ygx-",
        "outputId": "2bf80b8e-d62f-40cc-f83d-9373f920905d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labeling scheme (binary representations):  ['Action' 'Adventure' 'Animation' 'Biography' 'Comedy' 'Crime'\n",
            " 'Documentary' 'Drama' 'Family' 'Fantasy' 'Film-Noir' 'History' 'Horror'\n",
            " 'Music' 'Musical' 'Mystery' 'Romance' 'Sci-Fi' 'Short' 'Sport' 'Thriller'\n",
            " 'War' 'Western']\n",
            "Labeling scheme (class names):  {0: 'Action', 1: 'Adventure', 2: 'Animation', 3: 'Biography', 4: 'Comedy', 5: 'Crime', 6: 'Documentary', 7: 'Drama', 8: 'Family', 9: 'Fantasy', 10: 'Film-Noir', 11: 'History', 12: 'Horror', 13: 'Music', 14: 'Musical', 15: 'Mystery', 16: 'Romance', 17: 'Sci-Fi', 18: 'Short', 19: 'Sport', 20: 'Thriller', 21: 'War', 22: 'Western'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformed_train_genre_labels[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDGwK_WgjkcE",
        "outputId": "cb593230-615a-400e-f859-efd59ca230d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Dataset objects"
      ],
      "metadata": {
        "id": "TIPgPoPjRrvg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "train_data = {'image': train_file_paths, 'label': transformed_train_genre_labels}\n",
        "ds_train = Dataset.from_dict(train_data)"
      ],
      "metadata": {
        "id": "xRxEQ8F5Rp4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_data = {'image': test_file_paths, 'label': transformed_test_genre_labels}\n",
        "ds_val = Dataset.from_dict(val_data)"
      ],
      "metadata": {
        "id": "gQKlOYLbRyV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import PIL.Image as pil\n",
        "\n",
        "def transform(examples):\n",
        "  inputs = image_processor([pil.open(img).convert(\"RGB\") for img in examples[\"image\"]], return_tensors=\"pt\")\n",
        "  inputs[\"labels\"] = examples[\"label\"]\n",
        "  return inputs"
      ],
      "metadata": {
        "id": "Rk1KfF0WR18e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = ds_train.with_transform(transform)\n",
        "val_dataset = ds_val.with_transform(transform)"
      ],
      "metadata": {
        "id": "IJtq42AQR2_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for item in val_dataset:\n",
        "  print(item['pixel_values'].shape)\n",
        "  print(item[\"labels\"])\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4l5qRPpR3u5",
        "outputId": "ca97f597-c0e5-402e-9174-a445026d4b68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 224, 224])\n",
            "[0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def collate_fn(batch):\n",
        "  return {\n",
        "      \"pixel_values\": torch.stack([x[\"pixel_values\"] for x in batch]),\n",
        "      \"labels\": torch.tensor([x[\"labels\"] for x in batch]),\n",
        "      # \"labels\": torch.stack([x[\"labels\"] for x in batch], dim=0),\n",
        "  }"
      ],
      "metadata": {
        "id": "j8reQSS3R6UP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the ViT model\n",
        "model = ViTForImageClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels= len(labeling_scheme),\n",
        "    label2id = {label: str(i) for i, label in enumerate(labeling_scheme)},\n",
        "    id2label = {str(i): label for i, label in enumerate(labeling_scheme)},\n",
        "    problem_type = \"multi_label_classification\",\n",
        "    ignore_mismatched_sizes=True,\n",
        ")"
      ],
      "metadata": {
        "id": "ogUDkRUiZPp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# from evaluate import load\n",
        "# from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# accuracy = load(\"accuracy\")\n",
        "# f1 = load(\"f1\")\n",
        "\n",
        "# def compute_metrics(eval_pred):\n",
        "#     raw_logits = torch.tensor(eval_pred.predictions)\n",
        "#     sigmoid_predictions = torch.sigmoid(raw_logits).numpy()\n",
        "\n",
        "#     thresholded_predictions = (sigmoid_predictions > 0.5).astype(int)\n",
        "\n",
        "#     label_ids = eval_pred.label_ids\n",
        "\n",
        "#     accuracy_score = accuracy.compute(predictions=thresholded_predictions, references=label_ids)\n",
        "#     f1_score = f1.compute(predictions=thresholded_predictions, references=label_ids, average=\"macro\")\n",
        "\n",
        "#     return {**accuracy_score, **f1_score}\n"
      ],
      "metadata": {
        "id": "P0ELmE5qZsnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import TrainingArguments\n",
        "\n",
        "# training_args = TrainingArguments(\n",
        "#   output_dir=\"/content/Model/Models-Train-32\", # output directory\n",
        "#   per_device_train_batch_size=32, # batch size per device during training\n",
        "#   evaluation_strategy=\"steps\",    # evaluation strategy to adopt during training\n",
        "#   num_train_epochs=20,             # total number of training epochs\n",
        "#   # fp16=True,                    # use mixed precision\n",
        "#   save_steps=10,                # number of update steps before saving checkpoint\n",
        "#   eval_steps=10,                # number of update steps before evaluating\n",
        "#   logging_steps=10,             # number of update steps before logging\n",
        "#   # save_steps=50,\n",
        "#   # eval_steps=50,\n",
        "#   # logging_steps=50,\n",
        "#   save_total_limit=2,             # limit the total amount of checkpoints on disk\n",
        "#   remove_unused_columns=False,    # remove unused columns from the dataset\n",
        "#   push_to_hub=False,              # do not push the model to the hub\n",
        "#   report_to='tensorboard',        # report metrics to tensorboard\n",
        "#   load_best_model_at_end=True,    # load the best model at the end of training\n",
        "# )"
      ],
      "metadata": {
        "id": "JUlbHzr9fTul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import Trainer\n",
        "\n",
        "# trainer = Trainer(\n",
        "#     model=model,                        # the instantiated ðŸ¤— Transformers model to be trained\n",
        "#     args=training_args,                 # training arguments, defined above\n",
        "#     data_collator=collate_fn,           # the data collator that will be used for batching\n",
        "#     compute_metrics=compute_metrics,    # the metrics function that will be used for evaluation\n",
        "#     train_dataset=train_dataset,        # training dataset\n",
        "#     eval_dataset=val_dataset,           # evaluation dataset\n",
        "#     tokenizer=image_processor,          # the processor that will be used for preprocessing the images\n",
        "# )"
      ],
      "metadata": {
        "id": "SBg-Z2cifdwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trainer.train()"
      ],
      "metadata": {
        "id": "j2IeEOw9fh-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7w8qdhOylYq4",
        "outputId": "af15d85c-a22a-4f03-cd96-ba58a3f5b9d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['image', 'label'],\n",
              "    num_rows: 20217\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBhl4folA1mY",
        "outputId": "bba2c4e2-dd1f-4494-f3fc-6973ee22edd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['image', 'label'],\n",
              "    num_rows: 10138\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample = train_dataset[0]"
      ],
      "metadata": {
        "id": "qukNA0USA24i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9_SoSkvA4v7",
        "outputId": "9535afd4-2eab-4a05-c84b-13f1eece74ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'pixel_values': tensor([[[-0.1529, -0.6000, -0.5765,  ..., -0.6078, -0.6078, -0.1451],\n",
              "          [-0.2471, -0.9765, -0.9686,  ..., -1.0000, -1.0000, -0.2000],\n",
              "          [-0.1529, -0.9686, -1.0000,  ..., -1.0000, -1.0000, -0.2000],\n",
              "          ...,\n",
              "          [-0.2078, -0.9843, -0.9765,  ..., -0.9922, -0.9922, -0.2000],\n",
              "          [-0.2235, -0.9765, -0.9922,  ..., -0.9922, -0.9922, -0.2078],\n",
              "          [-0.1373, -0.6000, -0.5686,  ..., -0.6078, -0.6078, -0.1529]],\n",
              " \n",
              "         [[-0.1529, -0.6000, -0.5765,  ..., -0.6078, -0.6078, -0.1451],\n",
              "          [-0.2471, -0.9765, -0.9686,  ..., -1.0000, -1.0000, -0.2000],\n",
              "          [-0.1529, -0.9686, -1.0000,  ..., -1.0000, -1.0000, -0.2000],\n",
              "          ...,\n",
              "          [-0.2078, -0.9843, -0.9765,  ..., -0.9922, -0.9922, -0.2000],\n",
              "          [-0.2235, -0.9765, -0.9922,  ..., -0.9922, -0.9922, -0.2078],\n",
              "          [-0.1373, -0.6000, -0.5686,  ..., -0.6078, -0.6078, -0.1529]],\n",
              " \n",
              "         [[-0.1529, -0.6000, -0.5765,  ..., -0.6078, -0.6078, -0.1451],\n",
              "          [-0.2471, -0.9765, -0.9686,  ..., -1.0000, -1.0000, -0.2000],\n",
              "          [-0.1529, -0.9686, -1.0000,  ..., -1.0000, -1.0000, -0.2000],\n",
              "          ...,\n",
              "          [-0.2078, -0.9843, -0.9765,  ..., -0.9922, -0.9922, -0.2000],\n",
              "          [-0.2235, -0.9765, -0.9922,  ..., -0.9922, -0.9922, -0.2078],\n",
              "          [-0.1373, -0.6000, -0.5686,  ..., -0.6078, -0.6078, -0.1529]]]),\n",
              " 'labels': [0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0]}"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting Hyperparameters"
      ],
      "metadata": {
        "id": "kCt2qpZCA_HF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "7tG1R2KNA501"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_loader = DataLoader(train_dataset, collate_fn=collate_fn, batch_size=batch_size, shuffle=True)\n",
        "valid_dataset_loader = DataLoader(val_dataset, collate_fn=collate_fn, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "03kubwrZBCQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n"
      ],
      "metadata": {
        "id": "d7ktYOQjBEff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_dir = \"/content/Model/Models-Train-32/runs\"\n",
        "summary_writer = SummaryWriter(log_dir=log_dir)"
      ],
      "metadata": {
        "id": "ETmUWzaPBF9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 30\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "pZd-lMvXBLg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_train_steps = num_epochs * len(train_dataset_loader)\n",
        "n_valid_steps = len(valid_dataset_loader)\n",
        "current_step = 0"
      ],
      "metadata": {
        "id": "2sVhNwOqBNCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_train_steps , n_valid_steps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHKRXzBhBOPM",
        "outputId": "a3ca1586-e200-4297-84bd-92c359af948d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(18960, 317)"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# logging, eval & save steps\n",
        "save_steps = 650 #3000"
      ],
      "metadata": {
        "id": "J92Upx38BOrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "import numpy as np\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    # progress_bar = tqdm(range(current_step, n_train_steps), \"Training\", dynamic_ncols=True, ncols=80)\n",
        "\n",
        "    for batch in train_dataset_loader:\n",
        "      if (current_step+1) % save_steps == 0:\n",
        "        print()\n",
        "        print(f\"Validation at step {current_step}...\")\n",
        "        print()\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        predictions, labels = [], []\n",
        "        valid_loss = 0\n",
        "\n",
        "        for batch in valid_dataset_loader:\n",
        "            pixel_values = batch[\"pixel_values\"].to(device)\n",
        "            label_ids = batch[\"labels\"].to(device).float()\n",
        "\n",
        "            outputs = model(pixel_values=pixel_values, labels=label_ids)\n",
        "\n",
        "            loss = outputs.loss\n",
        "            valid_loss += loss.item()\n",
        "\n",
        "            logits = outputs.logits.detach().cpu()\n",
        "\n",
        "            # predictions.extend((logits > 0.5).int().cpu().numpy())\n",
        "            predictions.extend(F.sigmoid(logits).cpu().numpy())\n",
        "            labels.extend(label_ids.int().cpu().numpy())\n",
        "\n",
        "        # eval_prediction = EvalPrediction(predictions=predictions, label_ids=labels)\n",
        "        # metrics = compute_metrics(eval_prediction)\n",
        "        print()\n",
        "        print(f\"Epoch: {epoch}, Step: {current_step}, Train Loss: {train_loss / save_steps:.4f}, \" +\n",
        "              f\"Valid Loss: {valid_loss / n_valid_steps:.4f}\") # , Accuracy: {metrics['accuracy']}, \" + f\"F1 Score: {metrics['f1']}\")\n",
        "        print()\n",
        "\n",
        "        # summary_writer.add_scalar(\"valid_loss\", valid_loss / n_valid_steps, global_step=current_step)\n",
        "        # summary_writer.add_scalar(\"accuracy\", metrics[\"accuracy\"], global_step=current_step)\n",
        "        # summary_writer.add_scalar(\"f1\", metrics[\"f1\"], global_step=current_step)\n",
        "\n",
        "        model.save_pretrained(f\"/content/Model/Models-Train-32/checkpoint-{current_step}\")\n",
        "        image_processor.save_pretrained(f\"/content/Model/Models-Train-32/checkpoint-{current_step}\")\n",
        "\n",
        "        predictions = np.array(predictions)\n",
        "        threshold = 0.5\n",
        "        predictions = (predictions > threshold).astype(int)\n",
        "\n",
        "        accuracy = accuracy_score(labels, predictions)\n",
        "        precision = precision_score(labels, predictions, average='macro')\n",
        "        recall = recall_score(labels, predictions, average='macro')\n",
        "        f1 = f1_score(labels, predictions, average='macro')\n",
        "\n",
        "        print(f\"Accuracy: {accuracy}\")\n",
        "        print(f\"Precision: {precision}\")\n",
        "        print(f\"Recall: {recall}\")\n",
        "        print(f\"F1-Score: {f1}\")\n",
        "        # print(classification_report(labels, predictions))\n",
        "\n",
        "        model.train()\n",
        "        train_loss, valid_loss = 0, 0\n",
        "\n",
        "      pixel_values = batch[\"pixel_values\"].to(device)\n",
        "      labels = batch[\"labels\"].to(device).float()\n",
        "\n",
        "      outputs = model(pixel_values=pixel_values, labels=labels)\n",
        "\n",
        "      loss = outputs.loss\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      loss_v = loss.item()\n",
        "      train_loss += loss_v\n",
        "\n",
        "      current_step += 1\n",
        "      # progress_bar.update(1)\n",
        "      summary_writer.add_scalar(\"train_loss\", loss_v, global_step=current_step)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jWiD2nAnBUD3",
        "outputId": "0f2f4982-b526-4afa-f8b3-2696a8a5ca4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation at step 649...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Configuration saved in /content/Model/Models-Train-32/checkpoint-649/config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1, Step: 649, Train Loss: 0.0067, Valid Loss: 0.2635\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Model weights saved in /content/Model/Models-Train-32/checkpoint-649/pytorch_model.bin\n",
            "Image processor saved in /content/Model/Models-Train-32/checkpoint-649/preprocessor_config.json\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.0952850660879858\n",
            "Precision: 0.30973493016017783\n",
            "Recall: 0.07581461645789482\n",
            "F1-Score: 0.10411469466126491\n",
            "\n",
            "Validation at step 1299...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Configuration saved in /content/Model/Models-Train-32/checkpoint-1299/config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 2, Step: 1299, Train Loss: 0.0133, Valid Loss: 0.2507\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Model weights saved in /content/Model/Models-Train-32/checkpoint-1299/pytorch_model.bin\n",
            "Image processor saved in /content/Model/Models-Train-32/checkpoint-1299/preprocessor_config.json\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.10751627539948708\n",
            "Precision: 0.387794295108956\n",
            "Recall: 0.10834029416239833\n",
            "F1-Score: 0.14739877673248944\n",
            "\n",
            "Validation at step 1949...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Configuration saved in /content/Model/Models-Train-32/checkpoint-1949/config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 3, Step: 1949, Train Loss: 0.0181, Valid Loss: 0.2460\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Model weights saved in /content/Model/Models-Train-32/checkpoint-1949/pytorch_model.bin\n",
            "Image processor saved in /content/Model/Models-Train-32/checkpoint-1949/preprocessor_config.json\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.11363188005523772\n",
            "Precision: 0.4636219506313147\n",
            "Recall: 0.12847139962298695\n",
            "F1-Score: 0.17307892211310222\n",
            "\n",
            "Validation at step 2599...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Configuration saved in /content/Model/Models-Train-32/checkpoint-2599/config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 4, Step: 2599, Train Loss: 0.0225, Valid Loss: 0.2408\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Model weights saved in /content/Model/Models-Train-32/checkpoint-2599/pytorch_model.bin\n",
            "Image processor saved in /content/Model/Models-Train-32/checkpoint-2599/preprocessor_config.json\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.11708423752219373\n",
            "Precision: 0.4565572292099116\n",
            "Recall: 0.16119365190269502\n",
            "F1-Score: 0.2134770309371284\n",
            "\n",
            "Validation at step 3249...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Configuration saved in /content/Model/Models-Train-32/checkpoint-3249/config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 5, Step: 3249, Train Loss: 0.0251, Valid Loss: 0.2415\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Model weights saved in /content/Model/Models-Train-32/checkpoint-3249/pytorch_model.bin\n",
            "Image processor saved in /content/Model/Models-Train-32/checkpoint-3249/preprocessor_config.json\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.12280528703886368\n",
            "Precision: 0.48790368118088695\n",
            "Recall: 0.1795435867099175\n",
            "F1-Score: 0.23499797370380285\n",
            "\n",
            "Validation at step 3899...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Configuration saved in /content/Model/Models-Train-32/checkpoint-3899/config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 6, Step: 3899, Train Loss: 0.0265, Valid Loss: 0.2460\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Model weights saved in /content/Model/Models-Train-32/checkpoint-3899/pytorch_model.bin\n",
            "Image processor saved in /content/Model/Models-Train-32/checkpoint-3899/preprocessor_config.json\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.12211481554547247\n",
            "Precision: 0.5383352114093737\n",
            "Recall: 0.18016422727682704\n",
            "F1-Score: 0.2361592391786265\n",
            "\n",
            "Validation at step 4549...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Configuration saved in /content/Model/Models-Train-32/checkpoint-4549/config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 7, Step: 4549, Train Loss: 0.0272, Valid Loss: 0.2492\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Model weights saved in /content/Model/Models-Train-32/checkpoint-4549/pytorch_model.bin\n",
            "Image processor saved in /content/Model/Models-Train-32/checkpoint-4549/preprocessor_config.json\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.11974748471098837\n",
            "Precision: 0.5378478140601347\n",
            "Recall: 0.18993888645896084\n",
            "F1-Score: 0.24712227428076053\n",
            "\n",
            "Validation at step 5199...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Configuration saved in /content/Model/Models-Train-32/checkpoint-5199/config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 8, Step: 5199, Train Loss: 0.0268, Valid Loss: 0.2568\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Model weights saved in /content/Model/Models-Train-32/checkpoint-5199/pytorch_model.bin\n",
            "Image processor saved in /content/Model/Models-Train-32/checkpoint-5199/preprocessor_config.json\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.11501282304202012\n",
            "Precision: 0.535462136973961\n",
            "Recall: 0.20212480221386522\n",
            "F1-Score: 0.25574670842943176\n",
            "\n",
            "Validation at step 5849...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Configuration saved in /content/Model/Models-Train-32/checkpoint-5849/config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 9, Step: 5849, Train Loss: 0.0264, Valid Loss: 0.2660\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Model weights saved in /content/Model/Models-Train-32/checkpoint-5849/pytorch_model.bin\n",
            "Image processor saved in /content/Model/Models-Train-32/checkpoint-5849/preprocessor_config.json\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.1158019333201815\n",
            "Precision: 0.513014002186957\n",
            "Recall: 0.19756595375855784\n",
            "F1-Score: 0.2547912936179068\n",
            "\n",
            "Validation at step 6499...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Configuration saved in /content/Model/Models-Train-32/checkpoint-6499/config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 10, Step: 6499, Train Loss: 0.0244, Valid Loss: 0.2773\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Model weights saved in /content/Model/Models-Train-32/checkpoint-6499/pytorch_model.bin\n",
            "Image processor saved in /content/Model/Models-Train-32/checkpoint-6499/preprocessor_config.json\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.11659104359834287\n",
            "Precision: 0.5143486015311752\n",
            "Recall: 0.1986449326615734\n",
            "F1-Score: 0.2567496066842007\n",
            "\n",
            "Validation at step 7149...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Configuration saved in /content/Model/Models-Train-32/checkpoint-7149/config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 11, Step: 7149, Train Loss: 0.0225, Valid Loss: 0.2893\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Model weights saved in /content/Model/Models-Train-32/checkpoint-7149/pytorch_model.bin\n",
            "Image processor saved in /content/Model/Models-Train-32/checkpoint-7149/preprocessor_config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.10998224501874138\n",
            "Precision: 0.5042232928669771\n",
            "Recall: 0.2064719269969848\n",
            "F1-Score: 0.26276442165647984\n",
            "\n",
            "Validation at step 7799...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Configuration saved in /content/Model/Models-Train-32/checkpoint-7799/config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 12, Step: 7799, Train Loss: 0.0204, Valid Loss: 0.3015\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Model weights saved in /content/Model/Models-Train-32/checkpoint-7799/pytorch_model.bin\n",
            "Image processor saved in /content/Model/Models-Train-32/checkpoint-7799/preprocessor_config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.11363188005523772\n",
            "Precision: 0.4771550981847934\n",
            "Recall: 0.20964888170282603\n",
            "F1-Score: 0.2688207483242097\n",
            "\n",
            "Validation at step 8449...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Configuration saved in /content/Model/Models-Train-32/checkpoint-8449/config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 13, Step: 8449, Train Loss: 0.0187, Valid Loss: 0.3167\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Model weights saved in /content/Model/Models-Train-32/checkpoint-8449/pytorch_model.bin\n",
            "Image processor saved in /content/Model/Models-Train-32/checkpoint-8449/preprocessor_config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.1112645492207536\n",
            "Precision: 0.47692407512044865\n",
            "Recall: 0.21311414550120844\n",
            "F1-Score: 0.2690295189962498\n",
            "\n",
            "Validation at step 9099...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Configuration saved in /content/Model/Models-Train-32/checkpoint-9099/config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 14, Step: 9099, Train Loss: 0.0160, Valid Loss: 0.3298\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Model weights saved in /content/Model/Models-Train-32/checkpoint-9099/pytorch_model.bin\n",
            "Image processor saved in /content/Model/Models-Train-32/checkpoint-9099/preprocessor_config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.10761491418425725\n",
            "Precision: 0.4816983409875327\n",
            "Recall: 0.21534591593788208\n",
            "F1-Score: 0.2704788390405151\n",
            "\n",
            "Validation at step 9749...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Configuration saved in /content/Model/Models-Train-32/checkpoint-9749/config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 15, Step: 9749, Train Loss: 0.0137, Valid Loss: 0.3460\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Model weights saved in /content/Model/Models-Train-32/checkpoint-9749/pytorch_model.bin\n",
            "Image processor saved in /content/Model/Models-Train-32/checkpoint-9749/preprocessor_config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.10810810810810811\n",
            "Precision: 0.46950239615242134\n",
            "Recall: 0.2102816172354912\n",
            "F1-Score: 0.2672013863062004\n",
            "\n",
            "Validation at step 10399...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Configuration saved in /content/Model/Models-Train-32/checkpoint-10399/config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 16, Step: 10399, Train Loss: 0.0119, Valid Loss: 0.3605\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Model weights saved in /content/Model/Models-Train-32/checkpoint-10399/pytorch_model.bin\n",
            "Image processor saved in /content/Model/Models-Train-32/checkpoint-10399/preprocessor_config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.10830538567764846\n",
            "Precision: 0.4641463151656434\n",
            "Recall: 0.22221299821671972\n",
            "F1-Score: 0.27725666591535225\n",
            "\n",
            "Validation at step 11049...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Configuration saved in /content/Model/Models-Train-32/checkpoint-11049/config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 17, Step: 11049, Train Loss: 0.0103, Valid Loss: 0.3775\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Model weights saved in /content/Model/Models-Train-32/checkpoint-11049/pytorch_model.bin\n",
            "Image processor saved in /content/Model/Models-Train-32/checkpoint-11049/preprocessor_config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.11047543894259222\n",
            "Precision: 0.4633313787810596\n",
            "Recall: 0.21310406278000113\n",
            "F1-Score: 0.26956357964193317\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-aec805e7de1e>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m       \u001b[0mloss_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m       \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_v\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}