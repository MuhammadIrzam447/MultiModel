{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MuhammadIrzam447/MultiModel/blob/master/vision_transformers_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ViT Implementation ðŸ”¥\n",
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class NewGELUActivation(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT). Also see\n",
        "    the Gaussian Error Linear Units paper: https://arxiv.org/abs/1606.08415\n",
        "\n",
        "    Taken from https://github.com/huggingface/transformers/blob/main/src/transformers/activations.py\n",
        "    \"\"\"\n",
        "\n",
        "    def forward(self, input):\n",
        "        return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
        "\n",
        "\n",
        "class PatchEmbeddings(nn.Module):\n",
        "    \"\"\"\n",
        "    Convert the image into patches and then project them into a vector space.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.image_size = config[\"image_size\"]\n",
        "        self.patch_size = config[\"patch_size\"]\n",
        "        self.num_channels = config[\"num_channels\"]\n",
        "        self.hidden_size = config[\"hidden_size\"]\n",
        "        # Calculate the number of patches from the image size and patch size\n",
        "        self.num_patches = (self.image_size // self.patch_size) ** 2\n",
        "        # Create a projection layer to convert the image into patches\n",
        "        # The layer projects each patch into a vector of size hidden_size\n",
        "        self.projection = nn.Conv2d(self.num_channels, self.hidden_size, kernel_size=self.patch_size, stride=self.patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (batch_size, num_channels, image_size, image_size) -> (batch_size, num_patches, hidden_size)\n",
        "        x = self.projection(x)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Embeddings(nn.Module):\n",
        "    \"\"\"\n",
        "    Combine the patch embeddings with the class token and position embeddings.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.patch_embeddings = PatchEmbeddings(config)\n",
        "        # Create a learnable [CLS] token\n",
        "        # Similar to BERT, the [CLS] token is added to the beginning of the input sequence\n",
        "        # and is used to classify the entire sequence\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, config[\"hidden_size\"]))\n",
        "        # Create position embeddings for the [CLS] token and the patch embeddings\n",
        "        # Add 1 to the sequence length for the [CLS] token\n",
        "        self.position_embeddings = \\\n",
        "            nn.Parameter(torch.randn(1, self.patch_embeddings.num_patches + 1, config[\"hidden_size\"]))\n",
        "        self.dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch_embeddings(x)\n",
        "        batch_size, _, _ = x.size()\n",
        "        # Expand the [CLS] token to the batch size\n",
        "        # (1, 1, hidden_size) -> (batch_size, 1, hidden_size)\n",
        "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
        "        # Concatenate the [CLS] token to the beginning of the input sequence\n",
        "        # This results in a sequence length of (num_patches + 1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = x + self.position_embeddings\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class AttentionHead(nn.Module):\n",
        "    \"\"\"\n",
        "    A single attention head.\n",
        "    This module is used in the MultiHeadAttention module.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_size, attention_head_size, dropout, bias=True):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.attention_head_size = attention_head_size\n",
        "        # Create the query, key, and value projection layers\n",
        "        self.query = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
        "        self.key = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
        "        self.value = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Project the input into query, key, and value\n",
        "        # The same input is used to generate the query, key, and value,\n",
        "        # so it's usually called self-attention.\n",
        "        # (batch_size, sequence_length, hidden_size) -> (batch_size, sequence_length, attention_head_size)\n",
        "        query = self.query(x)\n",
        "        key = self.key(x)\n",
        "        value = self.value(x)\n",
        "        # Calculate the attention scores\n",
        "        # softmax(Q*K.T/sqrt(head_size))*V\n",
        "        attention_scores = torch.matmul(query, key.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "        # Calculate the attention output\n",
        "        attention_output = torch.matmul(attention_probs, value)\n",
        "        return (attention_output, attention_probs)\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-head attention module.\n",
        "    This module is used in the TransformerEncoder module.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.hidden_size = config[\"hidden_size\"]\n",
        "        self.num_attention_heads = config[\"num_attention_heads\"]\n",
        "        # The attention head size is the hidden size divided by the number of attention heads\n",
        "        self.attention_head_size = self.hidden_size // self.num_attention_heads\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "        # Whether or not to use bias in the query, key, and value projection layers\n",
        "        self.qkv_bias = config[\"qkv_bias\"]\n",
        "        # Create a list of attention heads\n",
        "        self.heads = nn.ModuleList([])\n",
        "        for _ in range(self.num_attention_heads):\n",
        "            head = AttentionHead(\n",
        "                self.hidden_size,\n",
        "                self.attention_head_size,\n",
        "                config[\"attention_probs_dropout_prob\"],\n",
        "                self.qkv_bias\n",
        "            )\n",
        "            self.heads.append(head)\n",
        "        # Create a linear layer to project the attention output back to the hidden size\n",
        "        # In most cases, all_head_size and hidden_size are the same\n",
        "        self.output_projection = nn.Linear(self.all_head_size, self.hidden_size)\n",
        "        self.output_dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
        "\n",
        "    def forward(self, x, output_attentions=False):\n",
        "        # Calculate the attention output for each attention head\n",
        "        attention_outputs = [head(x) for head in self.heads]\n",
        "        # Concatenate the attention outputs from each attention head\n",
        "        attention_output = torch.cat([attention_output for attention_output, _ in attention_outputs], dim=-1)\n",
        "        # Project the concatenated attention output back to the hidden size\n",
        "        attention_output = self.output_projection(attention_output)\n",
        "        attention_output = self.output_dropout(attention_output)\n",
        "        # Return the attention output and the attention probabilities (optional)\n",
        "        if not output_attentions:\n",
        "            return (attention_output, None)\n",
        "        else:\n",
        "            attention_probs = torch.stack([attention_probs for _, attention_probs in attention_outputs], dim=1)\n",
        "            return (attention_output, attention_probs)\n",
        "\n",
        "\n",
        "class FasterMultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-head attention module with some optimizations.\n",
        "    All the heads are processed simultaneously with merged query, key, and value projections.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.hidden_size = config[\"hidden_size\"]\n",
        "        self.num_attention_heads = config[\"num_attention_heads\"]\n",
        "        # The attention head size is the hidden size divided by the number of attention heads\n",
        "        self.attention_head_size = self.hidden_size // self.num_attention_heads\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "        # Whether or not to use bias in the query, key, and value projection layers\n",
        "        self.qkv_bias = config[\"qkv_bias\"]\n",
        "        # Create a linear layer to project the query, key, and value\n",
        "        self.qkv_projection = nn.Linear(self.hidden_size, self.all_head_size * 3, bias=self.qkv_bias)\n",
        "        self.attn_dropout = nn.Dropout(config[\"attention_probs_dropout_prob\"])\n",
        "        # Create a linear layer to project the attention output back to the hidden size\n",
        "        # In most cases, all_head_size and hidden_size are the same\n",
        "        self.output_projection = nn.Linear(self.all_head_size, self.hidden_size)\n",
        "        self.output_dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
        "\n",
        "    def forward(self, x, output_attentions=False):\n",
        "        # Project the query, key, and value\n",
        "        # (batch_size, sequence_length, hidden_size) -> (batch_size, sequence_length, all_head_size * 3)\n",
        "        qkv = self.qkv_projection(x)\n",
        "        # Split the projected query, key, and value into query, key, and value\n",
        "        # (batch_size, sequence_length, all_head_size * 3) -> (batch_size, sequence_length, all_head_size)\n",
        "        query, key, value = torch.chunk(qkv, 3, dim=-1)\n",
        "        # Resize the query, key, and value to (batch_size, num_attention_heads, sequence_length, attention_head_size)\n",
        "        batch_size, sequence_length, _ = query.size()\n",
        "        query = query.view(batch_size, sequence_length, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n",
        "        key = key.view(batch_size, sequence_length, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n",
        "        value = value.view(batch_size, sequence_length, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n",
        "        # Calculate the attention scores\n",
        "        # softmax(Q*K.T/sqrt(head_size))*V\n",
        "        attention_scores = torch.matmul(query, key.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
        "        attention_probs = self.attn_dropout(attention_probs)\n",
        "        # Calculate the attention output\n",
        "        attention_output = torch.matmul(attention_probs, value)\n",
        "        # Resize the attention output\n",
        "        # from (batch_size, num_attention_heads, sequence_length, attention_head_size)\n",
        "        # To (batch_size, sequence_length, all_head_size)\n",
        "        attention_output = attention_output.transpose(1, 2) \\\n",
        "                                           .contiguous() \\\n",
        "                                           .view(batch_size, sequence_length, self.all_head_size)\n",
        "        # Project the attention output back to the hidden size\n",
        "        attention_output = self.output_projection(attention_output)\n",
        "        attention_output = self.output_dropout(attention_output)\n",
        "        # Return the attention output and the attention probabilities (optional)\n",
        "        if not output_attentions:\n",
        "            return (attention_output, None)\n",
        "        else:\n",
        "            return (attention_output, attention_probs)\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \"\"\"\n",
        "    A multi-layer perceptron module.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense_1 = nn.Linear(config[\"hidden_size\"], config[\"intermediate_size\"])\n",
        "        self.activation = NewGELUActivation()\n",
        "        self.dense_2 = nn.Linear(config[\"intermediate_size\"], config[\"hidden_size\"])\n",
        "        self.dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dense_1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.dense_2(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\"\n",
        "    A single transformer block.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.use_faster_attention = config.get(\"use_faster_attention\", False)\n",
        "        if self.use_faster_attention:\n",
        "            self.attention = FasterMultiHeadAttention(config)\n",
        "        else:\n",
        "            self.attention = MultiHeadAttention(config)\n",
        "        self.layernorm_1 = nn.LayerNorm(config[\"hidden_size\"])\n",
        "        self.mlp = MLP(config)\n",
        "        self.layernorm_2 = nn.LayerNorm(config[\"hidden_size\"])\n",
        "\n",
        "    def forward(self, x, output_attentions=False):\n",
        "        # Self-attention\n",
        "        attention_output, attention_probs = \\\n",
        "            self.attention(self.layernorm_1(x), output_attentions=output_attentions)\n",
        "        # Skip connection\n",
        "        x = x + attention_output\n",
        "        # Feed-forward network\n",
        "        mlp_output = self.mlp(self.layernorm_2(x))\n",
        "        # Skip connection\n",
        "        x = x + mlp_output\n",
        "        # Return the transformer block's output and the attention probabilities (optional)\n",
        "        if not output_attentions:\n",
        "            return (x, None)\n",
        "        else:\n",
        "            return (x, attention_probs)\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    The transformer encoder module.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        # Create a list of transformer blocks\n",
        "        self.blocks = nn.ModuleList([])\n",
        "        for _ in range(config[\"num_hidden_layers\"]):\n",
        "            block = Block(config)\n",
        "            self.blocks.append(block)\n",
        "\n",
        "    def forward(self, x, output_attentions=False):\n",
        "        # Calculate the transformer block's output for each block\n",
        "        all_attentions = []\n",
        "        for block in self.blocks:\n",
        "            x, attention_probs = block(x, output_attentions=output_attentions)\n",
        "            if output_attentions:\n",
        "                all_attentions.append(attention_probs)\n",
        "        # Return the encoder's output and the attention probabilities (optional)\n",
        "        if not output_attentions:\n",
        "            return (x, None)\n",
        "        else:\n",
        "            return (x, all_attentions)\n",
        "\n",
        "\n",
        "class ViTForClassfication(nn.Module):\n",
        "    \"\"\"\n",
        "    The ViT model for classification.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.image_size = config[\"image_size\"]\n",
        "        self.hidden_size = config[\"hidden_size\"]\n",
        "        self.num_classes = config[\"num_classes\"]\n",
        "        # Create the embedding module\n",
        "        self.embedding = Embeddings(config)\n",
        "        # Create the transformer encoder module\n",
        "        self.encoder = Encoder(config)\n",
        "        # Create a linear layer to project the encoder's output to the number of classes\n",
        "        self.classifier = nn.Linear(self.hidden_size, self.num_classes)\n",
        "        # Initialize the weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def forward(self, x, output_attentions=False):\n",
        "        # Calculate the embedding output\n",
        "        embedding_output = self.embedding(x)\n",
        "        # Calculate the encoder's output\n",
        "        encoder_output, all_attentions = self.encoder(embedding_output, output_attentions=output_attentions)\n",
        "        # Calculate the logits, take the [CLS] token's output as features for classification\n",
        "        logits = self.classifier(encoder_output[:, 0, :])\n",
        "        # Return the logits and the attention probabilities (optional)\n",
        "        if not output_attentions:\n",
        "            return (logits, None)\n",
        "        else:\n",
        "            return (logits, all_attentions)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=self.config[\"initializer_range\"])\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "        elif isinstance(module, Embeddings):\n",
        "            module.position_embeddings.data = nn.init.trunc_normal_(\n",
        "                module.position_embeddings.data.to(torch.float32),\n",
        "                mean=0.0,\n",
        "                std=self.config[\"initializer_range\"],\n",
        "            ).to(module.position_embeddings.dtype)\n",
        "\n",
        "            module.cls_token.data = nn.init.trunc_normal_(\n",
        "                module.cls_token.data.to(torch.float32),\n",
        "                mean=0.0,\n",
        "                std=self.config[\"initializer_range\"],\n",
        "            ).to(module.cls_token.dtype)"
      ],
      "metadata": {
        "id": "Xi2pjXPprRkB",
        "cellView": "form"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare Data"
      ],
      "metadata": {
        "id": "mU3W20q76vYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"/content/Dataset(s)/mmimdb/imdb-new\""
      ],
      "metadata": {
        "id": "sWcpkFPx4Rbw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60d83c57-938d-45b7-80ce-56e847f8f279"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Dataset(s)/mmimdb/imdb-new\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "image_file_paths = []\n",
        "genre_labels = []\n",
        "\n",
        "image_folder_add = \"/content/Dataset(s)/mmimdb/imdb-new/train\"\n",
        "labels_file = \"./train_label.txt\"\n",
        "\n",
        "with open(labels_file, 'r') as file:\n",
        "    for line in file:\n",
        "        parts = line.strip().split('|')\n",
        "        filename = parts[0].strip()\n",
        "        labels = parts[1].strip().split(', ')  # Split labels by comma and remove leading/trailing spaces\n",
        "        image_path = os.path.join(image_folder_add, filename)\n",
        "        image_file_paths.append(image_path)\n",
        "        genre_labels.append(labels)\n",
        "\n",
        "test_image_file_paths = []\n",
        "test_genre_labels = []\n",
        "\n",
        "image_folder_add = \"/content/Dataset(s)/mmimdb/imdb-new/test\"\n",
        "labels_file = \"./test_label.txt\"\n",
        "\n",
        "with open(labels_file, 'r') as file:\n",
        "    for line in file:\n",
        "        parts = line.strip().split('|')\n",
        "        filename = parts[0].strip()\n",
        "        labels = parts[1].strip().split(', ')  # Split labels by comma and remove leading/trailing spaces\n",
        "\n",
        "        if not (filename.endswith(\"_1.png\") or filename.endswith(\"_2.png\")):\n",
        "            image_path = os.path.join(image_folder_add, filename)\n",
        "            test_image_file_paths.append(image_path)\n",
        "            test_genre_labels.append(labels)\n",
        "\n",
        "print(\"Lenght of train: \", len(image_file_paths), len(genre_labels))\n",
        "print(\"Lenght of test: \", len(test_image_file_paths), len(test_genre_labels))"
      ],
      "metadata": {
        "id": "H3321jlT4KY9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19558047-4aed-485d-f11e-1fbdeccd7da1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lenght of train:  31104 31104\n",
            "Lenght of test:  15598 15598\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "label_counts = defaultdict(int)\n",
        "\n",
        "for labels in genre_labels:\n",
        "    for label in labels:\n",
        "        label_counts[label] += 1\n",
        "\n",
        "\n",
        "label_count_list = [(label, count) for label, count in label_counts.items()]\n",
        "sorted_label_count_list = sorted(label_count_list, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "for label, count in sorted_label_count_list:\n",
        "    print(f\"{label}: {count}\")\n",
        "\n",
        "print(\"Total Labels: \", len(label_count_list))"
      ],
      "metadata": {
        "id": "ly8mOA7Y4Yv3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3292c04-1ac6-4c23-b96d-99c5eac8ae08"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drama: 16848\n",
            "Comedy: 10216\n",
            "Romance: 6452\n",
            "Thriller: 6226\n",
            "Crime: 4586\n",
            "Action: 4310\n",
            "Adventure: 3222\n",
            "Horror: 3206\n",
            "Documentary: 2468\n",
            "Mystery: 2462\n",
            "Sci-Fi: 2424\n",
            "Fantasy: 2324\n",
            "Family: 1956\n",
            "War: 1612\n",
            "Biography: 1576\n",
            "History: 1360\n",
            "Music: 1268\n",
            "Animation: 1172\n",
            "Musical: 1006\n",
            "Western: 846\n",
            "Sport: 758\n",
            "Short: 562\n",
            "Film-Noir: 404\n",
            "News: 78\n",
            "Talk-Show: 4\n",
            "Reality-TV: 2\n",
            "Total Labels:  26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "min_label_count = 400\n",
        "valid_labels = [label for label, count in label_counts.items() if count >= min_label_count]\n",
        "valid_labels = sorted(list(valid_labels))"
      ],
      "metadata": {
        "id": "u2q0n7EM4ZWq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multi_hot_labels = []\n",
        "\n",
        "for labels in genre_labels:\n",
        "    multi_hot = [1. if label in labels else 0 for label in valid_labels]\n",
        "    multi_hot_labels.append(multi_hot)\n",
        "\n",
        "test_multi_hot_labels = []\n",
        "\n",
        "for labels in test_genre_labels:\n",
        "    multi_hot = [1. if label in labels else 0 for label in valid_labels]\n",
        "    test_multi_hot_labels.append(multi_hot)"
      ],
      "metadata": {
        "id": "HjFbaO2X4ba4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "class CustomMultiLabelDataset(Dataset):\n",
        "    def __init__(self, image_file_paths, multi_encoded_labels, transform=None):\n",
        "        self.image_file_paths = image_file_paths\n",
        "        self.multi_encoded_labels = multi_encoded_labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_file_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.image_file_paths[idx]\n",
        "        image = Image.open(image_path)\n",
        "\n",
        "        labels = self.multi_encoded_labels[idx]\n",
        "        # labels = torch.tensor(labels, dtype=torch.float32)\n",
        "        labels = torch.tensor(labels)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, labels\n",
        "\n",
        "# Define data transformations (resize, normalize, etc.)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "])"
      ],
      "metadata": {
        "id": "DWOTpksV5-oY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CustomMultiLabelDataset(image_file_paths, multi_hot_labels, transform=transform)\n",
        "test_dataset = CustomMultiLabelDataset(test_image_file_paths, test_multi_hot_labels, transform=transform)"
      ],
      "metadata": {
        "id": "m82dcJIc6VtL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbEp406Wqksp"
      },
      "outputs": [],
      "source": [
        "# #@title Prepare Data ðŸ“Š\n",
        "# # Import libraries\n",
        "# import torch\n",
        "# import torchvision\n",
        "# import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "# def prepare_data(batch_size=4, num_workers=2, train_sample_size=None, test_sample_size=None):\n",
        "#     train_transform = transforms.Compose(\n",
        "#         [transforms.ToTensor(),\n",
        "#         transforms.Resize((32, 32)),\n",
        "#         transforms.RandomHorizontalFlip(p=0.5),\n",
        "#         transforms.RandomResizedCrop((32, 32), scale=(0.8, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=2),\n",
        "#         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "#     trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "#                                             download=True, transform=train_transform)\n",
        "#     if train_sample_size is not None:\n",
        "#         # Randomly sample a subset of the training set\n",
        "#         indices = torch.randperm(len(trainset))[:train_sample_size]\n",
        "#         trainset = torch.utils.data.Subset(trainset, indices)\n",
        "\n",
        "\n",
        "\n",
        "#     trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "#                                             shuffle=True, num_workers=num_workers)\n",
        "\n",
        "#     test_transform = transforms.Compose(\n",
        "#         [transforms.ToTensor(),\n",
        "#         transforms.Resize((32, 32)),\n",
        "#         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "#     testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "#                                         download=True, transform=test_transform)\n",
        "#     if test_sample_size is not None:\n",
        "#         # Randomly sample a subset of the test set\n",
        "#         indices = torch.randperm(len(testset))[:test_sample_size]\n",
        "#         testset = torch.utils.data.Subset(testset, indices)\n",
        "\n",
        "#     testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "#                                             shuffle=False, num_workers=num_workers)\n",
        "\n",
        "#     classes = ('plane', 'car', 'bird', 'cat',\n",
        "#             'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "#     return trainloader, testloader, classes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Prepare Data ðŸ“Š { display-mode: \"code\" }\n",
        "# Import libraries\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "def prepare_data(batch_size=32, num_workers=1, train_sample_size=None, test_sample_size=None):\n",
        "\n",
        "    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
        "                                            shuffle=True, num_workers=num_workers)\n",
        "\n",
        "\n",
        "    testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
        "                                            shuffle=False, num_workers=num_workers)\n",
        "\n",
        "    classes = tuple(valid_labels)\n",
        "    return trainloader, testloader, classes"
      ],
      "metadata": {
        "id": "hLPN1Omt6_Mr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Utils ðŸ› ï¸\n",
        "import json, os, math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "def save_experiment(experiment_name, config, model, train_losses, test_losses, accuracies, base_dir=\"/content/Model/experiments\"):\n",
        "    outdir = os.path.join(base_dir, experiment_name)\n",
        "    os.makedirs(outdir, exist_ok=True)\n",
        "\n",
        "    # Save the config\n",
        "    configfile = os.path.join(outdir, 'config.json')\n",
        "    with open(configfile, 'w') as f:\n",
        "        json.dump(config, f, sort_keys=True, indent=4)\n",
        "\n",
        "    # Save the metrics\n",
        "    jsonfile = os.path.join(outdir, 'metrics.json')\n",
        "    with open(jsonfile, 'w') as f:\n",
        "        data = {\n",
        "            'train_losses': train_losses,\n",
        "            'test_losses': test_losses,\n",
        "            'accuracies': accuracies,\n",
        "        }\n",
        "        json.dump(data, f, sort_keys=True, indent=4)\n",
        "\n",
        "    # Save the model\n",
        "    save_checkpoint(experiment_name, model, \"final\", base_dir=base_dir)\n",
        "\n",
        "\n",
        "def save_checkpoint(experiment_name, model, epoch, base_dir=\"/content/Model/experiments\"):\n",
        "    outdir = os.path.join(base_dir, experiment_name)\n",
        "    os.makedirs(outdir, exist_ok=True)\n",
        "    cpfile = os.path.join(outdir, f'model_{epoch}.pt')\n",
        "    torch.save(model.state_dict(), cpfile)\n",
        "\n",
        "\n",
        "def load_experiment(experiment_name, checkpoint_name=\"model_final.pt\", base_dir=\"/content/Model/experiments\"):\n",
        "    outdir = os.path.join(base_dir, experiment_name)\n",
        "    # Load the config\n",
        "    configfile = os.path.join(outdir, 'config.json')\n",
        "    with open(configfile, 'r') as f:\n",
        "        config = json.load(f)\n",
        "    # Load the metrics\n",
        "    jsonfile = os.path.join(outdir, 'metrics.json')\n",
        "    with open(jsonfile, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    train_losses = data['train_losses']\n",
        "    test_losses = data['test_losses']\n",
        "    accuracies = data['accuracies']\n",
        "    # Load the model\n",
        "    model = ViTForClassfication(config)\n",
        "    cpfile = os.path.join(outdir, checkpoint_name)\n",
        "    model.load_state_dict(torch.load(cpfile))\n",
        "    return config, model, train_losses, test_losses, accuracies\n",
        "\n",
        "\n",
        "def visualize_images():\n",
        "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                            download=True)\n",
        "    classes = ('plane', 'car', 'bird', 'cat',\n",
        "            'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "    # Pick 30 samples randomly\n",
        "    indices = torch.randperm(len(trainset))[:30]\n",
        "    images = [np.asarray(trainset[i][0]) for i in indices]\n",
        "    labels = [trainset[i][1] for i in indices]\n",
        "    # Visualize the images using matplotlib\n",
        "    fig = plt.figure(figsize=(10, 10))\n",
        "    for i in range(30):\n",
        "        ax = fig.add_subplot(6, 5, i+1, xticks=[], yticks=[])\n",
        "        ax.imshow(images[i])\n",
        "        ax.set_title(classes[labels[i]])\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def visualize_attention(model, output=None, device=\"cuda\"):\n",
        "    \"\"\"\n",
        "    Visualize the attention maps of the first 4 images.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    # Load random images\n",
        "    num_images = 30\n",
        "    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True)\n",
        "    classes = ('plane', 'car', 'bird', 'cat',\n",
        "            'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "    # Pick 30 samples randomly\n",
        "    indices = torch.randperm(len(testset))[:num_images]\n",
        "    raw_images = [np.asarray(testset[i][0]) for i in indices]\n",
        "    labels = [testset[i][1] for i in indices]\n",
        "    # Convert the images to tensors\n",
        "    test_transform = transforms.Compose(\n",
        "        [transforms.ToTensor(),\n",
        "        transforms.Resize((32, 32)),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "    images = torch.stack([test_transform(image) for image in raw_images])\n",
        "    # Move the images to the device\n",
        "    images = images.to(device)\n",
        "    model = model.to(device)\n",
        "    # Get the attention maps from the last block\n",
        "    logits, attention_maps = model(images, output_attentions=True)\n",
        "    # Get the predictions\n",
        "    predictions = torch.argmax(logits, dim=1)\n",
        "    # Concatenate the attention maps from all blocks\n",
        "    attention_maps = torch.cat(attention_maps, dim=1)\n",
        "    # select only the attention maps of the CLS token\n",
        "    attention_maps = attention_maps[:, :, 0, 1:]\n",
        "    # Then average the attention maps of the CLS token over all the heads\n",
        "    attention_maps = attention_maps.mean(dim=1)\n",
        "    # Reshape the attention maps to a square\n",
        "    num_patches = attention_maps.size(-1)\n",
        "    size = int(math.sqrt(num_patches))\n",
        "    attention_maps = attention_maps.view(-1, size, size)\n",
        "    # Resize the map to the size of the image\n",
        "    attention_maps = attention_maps.unsqueeze(1)\n",
        "    attention_maps = F.interpolate(attention_maps, size=(32, 32), mode='bilinear', align_corners=False)\n",
        "    attention_maps = attention_maps.squeeze(1)\n",
        "    # Plot the images and the attention maps\n",
        "    fig = plt.figure(figsize=(20, 10))\n",
        "    mask = np.concatenate([np.ones((32, 32)), np.zeros((32, 32))], axis=1)\n",
        "    for i in range(num_images):\n",
        "        ax = fig.add_subplot(6, 5, i+1, xticks=[], yticks=[])\n",
        "        img = np.concatenate((raw_images[i], raw_images[i]), axis=1)\n",
        "        ax.imshow(img)\n",
        "        # Mask out the attention map of the left image\n",
        "        extended_attention_map = np.concatenate((np.zeros((32, 32)), attention_maps[i].cpu()), axis=1)\n",
        "        extended_attention_map = np.ma.masked_where(mask==1, extended_attention_map)\n",
        "        ax.imshow(extended_attention_map, alpha=0.5, cmap='jet')\n",
        "        # Show the ground truth and the prediction\n",
        "        gt = classes[labels[i]]\n",
        "        pred = classes[predictions[i]]\n",
        "        ax.set_title(f\"gt: {gt} / pred: {pred}\", color=(\"green\" if gt==pred else \"red\"))\n",
        "    if output is not None:\n",
        "        plt.savefig(output)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "djY52Edqug3M"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Train ViT ðŸ§  ðŸ‹ðŸ½\n",
        "#@title String fields\n",
        "\n",
        "exp_name = 'vit-with-100-epochs' #@param {type:\"string\"}\n",
        "batch_size = 32 #@param {type: \"integer\"}\n",
        "epochs = 100 #@param {type: \"integer\"}\n",
        "lr = 1e-2  #@param {type: \"number\"}\n",
        "save_model_every = 10 #@param {type: \"integer\"}\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "config = {\n",
        "    \"patch_size\": 16,  # Input image size: 32x32 -> 8x8 patches\n",
        "    \"hidden_size\": 768,\n",
        "    \"num_hidden_layers\": 12,\n",
        "    \"num_attention_heads\": 12,\n",
        "    \"intermediate_size\": 4 * 768, # 4 * hidden_size\n",
        "    \"hidden_dropout_prob\": 0.0,\n",
        "    \"attention_probs_dropout_prob\": 0.0,\n",
        "    \"initializer_range\": 0.02,\n",
        "    \"image_size\": 224,\n",
        "    \"num_classes\": len(valid_labels), # num_classes of CIFAR10\n",
        "    \"num_channels\": 3,\n",
        "    \"qkv_bias\": True,\n",
        "    \"use_faster_attention\": True,\n",
        "}\n",
        "# These are not hard constraints, but are used to prevent misconfigurations\n",
        "assert config[\"hidden_size\"] % config[\"num_attention_heads\"] == 0\n",
        "assert config['intermediate_size'] == 4 * config['hidden_size']\n",
        "assert config['image_size'] % config['patch_size'] == 0\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    \"\"\"\n",
        "    The simple trainer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, optimizer, loss_fn, exp_name, device):\n",
        "        self.model = model.to(device)\n",
        "        self.optimizer = optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "        self.exp_name = exp_name\n",
        "        self.device = device\n",
        "\n",
        "    def train(self, trainloader, testloader, epochs, save_model_every_n_epochs=0):\n",
        "        \"\"\"\n",
        "        Train the model for the specified number of epochs.\n",
        "        \"\"\"\n",
        "        # Keep track of the losses and accuracies\n",
        "        train_losses, test_losses, accuracies = [], [], []\n",
        "        # Train the model\n",
        "        for i in range(epochs):\n",
        "            train_loss = self.train_epoch(trainloader)\n",
        "            accuracy, test_loss, f1 = self.evaluate(testloader)\n",
        "            train_losses.append(train_loss)\n",
        "            test_losses.append(test_loss)\n",
        "            accuracies.append(accuracy)\n",
        "            print(f\"Epoch: {i+1}, Train loss: {train_loss:.4f}, Test loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}, F1: {f1:.4f}\")\n",
        "            if save_model_every_n_epochs > 0 and (i+1) % save_model_every_n_epochs == 0 and i+1 != epochs:\n",
        "                print('\\tSave checkpoint at epoch', i+1)\n",
        "                save_checkpoint(self.exp_name, self.model, i+1)\n",
        "        # Save the experiment\n",
        "        save_experiment(self.exp_name, config, self.model, train_losses, test_losses, accuracies)\n",
        "\n",
        "    def train_epoch(self, trainloader):\n",
        "        \"\"\"\n",
        "        Train the model for one epoch.\n",
        "        \"\"\"\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        for batch in trainloader:\n",
        "            # Move the batch to the device\n",
        "            batch = [t.to(self.device) for t in batch]\n",
        "            images, labels = batch\n",
        "            # Zero the gradients\n",
        "            self.optimizer.zero_grad()\n",
        "            # Calculate the loss\n",
        "            loss = self.loss_fn(self.model(images)[0], labels)\n",
        "            # Backpropagate the loss\n",
        "            loss.backward()\n",
        "            # Update the model's parameters\n",
        "            self.optimizer.step()\n",
        "            total_loss += loss.item() * len(images)\n",
        "        return total_loss / len(trainloader.dataset)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def evaluate(self, testloader):\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        predicted_labels = []\n",
        "        true_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in testloader:\n",
        "                # Move the batch to the device\n",
        "                batch = [t.to(self.device) for t in batch]\n",
        "                images, labels = batch\n",
        "\n",
        "                # Get predictions\n",
        "                logits, _ = self.model(images)\n",
        "\n",
        "                # Calculate the loss\n",
        "                loss = self.loss_fn(logits, labels)\n",
        "                total_loss += loss.item() * len(images)\n",
        "\n",
        "                # Calculate the accuracy\n",
        "                # predictions = torch.argmax(logits, dim=1)\n",
        "\n",
        "                # logits_tensor = torch.tensor(logits)\n",
        "                # sigmoid_predictions = torch.sigmoid(logits_tensor)\n",
        "                # predictions = (sigmoid_predictions > 0.5).cpu().numpy().astype(int)\n",
        "\n",
        "                # predicted_labels.extend(predictions.cpu().numpy())\n",
        "                # true_labels.extend(labels.int().cpu().numpy())\n",
        "\n",
        "                predicted_labels.extend(F.sigmoid(logits).cpu().numpy())\n",
        "                true_labels.extend(labels.int().cpu().numpy())\n",
        "\n",
        "                # correct += torch.sum(predictions == labels).item()\n",
        "\n",
        "        # accuracy = correct / len(testloader.dataset)\n",
        "        predicted_labels = np.array(predicted_labels)\n",
        "        predictions = (predicted_labels > 0.5).astype(int)\n",
        "\n",
        "        accuracy = accuracy_score(true_labels, predictions)\n",
        "        f1_macro = f1_score(true_labels, predictions, average='macro')\n",
        "        avg_loss = total_loss / len(testloader.dataset)\n",
        "\n",
        "        # print(\"Accuracy: \", accuracy)\n",
        "        # print(\"F1_Macro: \", f1_macro)\n",
        "\n",
        "        return accuracy, avg_loss, f1_macro\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Training parameters\n",
        "    save_model_every_n_epochs = save_model_every\n",
        "    # Load the CIFAR10 dataset\n",
        "    trainloader, testloader, _ = prepare_data(batch_size=batch_size)\n",
        "    # Create the model, optimizer, loss function and trainer\n",
        "    model = ViTForClassfication(config)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-2)\n",
        "    # loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    loss_fn = nn.BCEWithLogitsLoss() # Binary Cross-Entropy Loss\n",
        "\n",
        "    trainer = Trainer(model, optimizer, loss_fn, exp_name, device=device)\n",
        "    trainer.train(trainloader, testloader, epochs, save_model_every_n_epochs=save_model_every_n_epochs)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "qGHq-Nl0rrFF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        },
        "outputId": "73956aa3-930a-417c-c8db-80fbaabeb49d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Train loss: 2243.9770, Test loss: 3850.0744, Accuracy: 0.0142, F1: 0.0872\n",
            "Epoch: 2, Train loss: 1290.5396, Test loss: 289.7416, Accuracy: 0.0331, F1: 0.0533\n",
            "Epoch: 3, Train loss: 79.1122, Test loss: 17.7873, Accuracy: 0.0413, F1: 0.0772\n",
            "Epoch: 4, Train loss: 12.7244, Test loss: 8.2309, Accuracy: 0.0044, F1: 0.0876\n",
            "Epoch: 5, Train loss: 6.6969, Test loss: 5.4460, Accuracy: 0.0005, F1: 0.1090\n",
            "Epoch: 6, Train loss: 3356.1153, Test loss: 214.6964, Accuracy: 0.0230, F1: 0.0689\n",
            "Epoch: 7, Train loss: 137.0683, Test loss: 54.2292, Accuracy: 0.0044, F1: 0.0627\n",
            "Epoch: 8, Train loss: 28.0615, Test loss: 18.7524, Accuracy: 0.0073, F1: 0.0798\n",
            "Epoch: 9, Train loss: 14.5090, Test loss: 12.6130, Accuracy: 0.0430, F1: 0.0445\n",
            "Epoch: 10, Train loss: 12.4461, Test loss: 3.7094, Accuracy: 0.0266, F1: 0.0780\n",
            "\tSave checkpoint at epoch 10\n",
            "Epoch: 11, Train loss: 11.0532, Test loss: 10.7284, Accuracy: 0.0039, F1: 0.0952\n",
            "Epoch: 12, Train loss: 11.1358, Test loss: 0.8163, Accuracy: 0.0316, F1: 0.0523\n",
            "Epoch: 13, Train loss: 0.5827, Test loss: 0.4786, Accuracy: 0.0451, F1: 0.0800\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-7b1de2249961>\u001b[0m in \u001b[0;36m<cell line: 158>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-7b1de2249961>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_model_every_n_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_model_every_n_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-7b1de2249961>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, trainloader, testloader, epochs, save_model_every_n_epochs)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-7b1de2249961>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(self, trainloader)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;31m# Update the model's parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Visualize Dataset\n",
        "# Show some training images\n",
        "visualize_images()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "nJdJTQrIvAat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Plot training Results\n",
        "config, model, train_losses, test_losses, accuracies = load_experiment(f\"/content/Model/experiments/{exp_name}/\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# Create two subplots of train/test losses and accuracies\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "ax1.plot(train_losses, label=\"Train loss\")\n",
        "ax1.plot(test_losses, label=\"Test loss\")\n",
        "ax1.set_xlabel(\"Epoch\")\n",
        "ax1.set_ylabel(\"Loss\")\n",
        "ax1.legend()\n",
        "ax2.plot(accuracies)\n",
        "ax2.set_xlabel(\"Epoch\")\n",
        "ax2.set_ylabel(\"Accuracy\")\n",
        "plt.savefig(\"metrics.png\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UTuHlgNgzfWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Visualize Attetion\n",
        "visualize_attention(model, \"attention.png\")"
      ],
      "metadata": {
        "id": "xFiXAL9BzvTV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}