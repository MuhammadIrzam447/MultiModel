{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MuhammadIrzam447/MultiModel/blob/master/Train_08.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -q kaggle"
      ],
      "metadata": {
        "id": "wrEzqa88WCio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !chmod 600 /root/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "iUU9y2VlWCLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "\n",
        "# # Specify the path to the uploaded kaggle.json file\n",
        "# kaggle_json_path = \"/root/.kaggle/kaggle.json\"\n",
        "# os.environ['KAGGLE_CONFIG_PATH'] = kaggle_json_path"
      ],
      "metadata": {
        "id": "0h7RUN9oWB-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !kaggle datasets download -d gianmarco96/upmcfood101"
      ],
      "metadata": {
        "id": "vC1hFEcxWBkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd /content/Dataset(s)/"
      ],
      "metadata": {
        "id": "QTT-ZWuon4YL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip /content/upmcfood101.zip"
      ],
      "metadata": {
        "id": "fIoZ7lU1iskh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ResNet-101 on Food-101 (Imgs only)"
      ],
      "metadata": {
        "id": "4H3pu3V8Tp-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObgVtPmABT7d"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision.models as models\n",
        "from torchvision.datasets import ImageFolder\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from PIL import UnidentifiedImageError\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "pziXvdPgU58B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d158e5ee-c9a6-4f91-ce56-8880a5812c91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Training Dataset and Preprocessing"
      ],
      "metadata": {
        "id": "00vlqtj_0TXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "11gcKFPXGx4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainingDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_dir):\n",
        "        self.data_dir = data_dir\n",
        "        self.dataset = datasets.ImageFolder(data_dir, transform=transform)\n",
        "        self.classes = sorted(os.listdir(data_dir))\n",
        "        self.class_lengths = self._compute_class_lengths()\n",
        "        self.num_classes = len(self.dataset.classes)\n",
        "\n",
        "    def _compute_class_lengths(self):\n",
        "        class_lengths = {cls: 0 for cls in self.classes}\n",
        "\n",
        "        for cls in self.classes:\n",
        "            cls_dir = os.path.join(self.data_dir, cls)\n",
        "            if os.path.isdir(cls_dir):\n",
        "                class_lengths[cls] = len(os.listdir(cls_dir))\n",
        "\n",
        "        return class_lengths\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image, label = self.dataset[index]\n",
        "        return image, label\n",
        "\n",
        "    # def __getitem__(self, index):\n",
        "    #     while True:\n",
        "    #         try:\n",
        "    #             image, label = self.dataset[index]\n",
        "    #             return image, label\n",
        "    #         except (UnidentifiedImageError, FileNotFoundError) as e:\n",
        "    #             print(f\"Error loading image at index {index}: {e}\")\n",
        "    #             index += 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def get_num_classes(self):\n",
        "        return self.num_classes"
      ],
      "metadata": {
        "id": "SxiZcLMwIdP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = \"/content/Dataset(s)/images-food-101/train\"\n",
        "dataset = TrainingDataset(data_dir)"
      ],
      "metadata": {
        "id": "2vHn670_GyKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of samples:\", len(dataset))\n",
        "print(\"Number of classes:\", len(dataset.classes))"
      ],
      "metadata": {
        "id": "S9kouQGoUpJN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "771447eb-9ee0-4277-824b-c9b070bf6ae4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples: 67988\n",
            "Number of classes: 101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "num_workers = 1\n",
        "data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)"
      ],
      "metadata": {
        "id": "Cs0LSvvPGyTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Validation Data Loader"
      ],
      "metadata": {
        "id": "LN_1_n4nScPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "class ValidationDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_dir):\n",
        "        self.data_dir = data_dir\n",
        "        self.dataset = datasets.ImageFolder(data_dir, transform=val_transform)\n",
        "        self.classes = sorted(os.listdir(data_dir))\n",
        "        self.class_lengths = self._compute_class_lengths()\n",
        "\n",
        "    def _compute_class_lengths(self):\n",
        "        class_lengths = {cls: 0 for cls in self.classes}\n",
        "\n",
        "        for cls in self.classes:\n",
        "            cls_dir = os.path.join(self.data_dir, cls)\n",
        "            if os.path.isdir(cls_dir):\n",
        "                class_lengths[cls] = len(os.listdir(cls_dir))\n",
        "\n",
        "        return class_lengths\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image, label = self.dataset[index]\n",
        "        return image, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)"
      ],
      "metadata": {
        "id": "hycPMtNySeXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valPath = \"/content/Dataset(s)/images-food-101/test\"\n",
        "val_dataset = ValidationDataset(valPath)"
      ],
      "metadata": {
        "id": "F1ZXM95XSz3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "validation_data_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "re5Gun34Se9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of samples:\", len(val_dataset))\n",
        "print(\"Number of classes:\", len(val_dataset.classes))"
      ],
      "metadata": {
        "id": "rp6bP4sLS3DE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ac42a23-e479-4757-fb3b-29883368ebb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples: 22716\n",
            "Number of classes: 101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EDA"
      ],
      "metadata": {
        "id": "vsW7ms9iFQIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_length = dataset.class_lengths\n",
        "class_labels = list(class_length.keys())"
      ],
      "metadata": {
        "id": "VtJYA2vJ_lzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms import ToPILImage\n",
        "\n",
        "def plot_sample_images(dataset):\n",
        "\n",
        "\n",
        "    # Create a grid layout based on the number of classes\n",
        "    num_classes = len(class_length)\n",
        "    grid_cols = 4  # Number of columns in the grid\n",
        "    grid_rows = (num_classes + grid_cols - 1) // grid_cols  # Number of rows in the grid\n",
        "    plt.figure(figsize=(10, 5 * grid_rows))\n",
        "\n",
        "    # Create a transform to convert the tensor to PIL Image\n",
        "    to_pil = ToPILImage()\n",
        "\n",
        "    # Iterate over each class\n",
        "    for i, class_label in enumerate(class_labels):\n",
        "        # Get a random image index from the class\n",
        "        image_index = np.random.choice(class_length[class_label])\n",
        "\n",
        "        # Get the image and label from the dataset\n",
        "        image, label = dataset[image_index]\n",
        "\n",
        "        # Convert the image tensor to PIL Image\n",
        "        image = to_pil(image)\n",
        "\n",
        "        # Plot the image\n",
        "        plt.subplot(grid_rows, grid_cols, i + 1)\n",
        "        plt.imshow(image, interpolation='none')  # Use 'RGB' interpolation\n",
        "        plt.title(class_label)\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "mM85nfZA_4KL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_sample_images(dataset)"
      ],
      "metadata": {
        "id": "vmHEUpE2AlP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_lengths_dict = dataset.class_lengths\n",
        "total_sum = sum(class_lengths_dict.values())\n",
        "dict_length = len(class_lengths_dict)\n",
        "\n",
        "# Print the length\n",
        "print(\"Dictionary length:\", dict_length)\n",
        "# Print the total sum\n",
        "print(\"Total sum:\", total_sum)\n",
        "print(class_lengths_dict.values())"
      ],
      "metadata": {
        "id": "g-0uV8a5pPvX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21bc1dae-45e7-4620-e970-f3f3b4944c6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dictionary length: 101\n",
            "Total sum: 67988\n",
            "dict_values([701, 662, 676, 664, 672, 671, 669, 673, 677, 642, 692, 679, 689, 659, 693, 678, 672, 696, 665, 659, 655, 694, 672, 688, 669, 658, 661, 674, 672, 682, 685, 693, 682, 690, 638, 603, 684, 636, 624, 645, 634, 668, 711, 633, 685, 698, 669, 699, 667, 685, 675, 684, 696, 698, 608, 701, 669, 701, 694, 696, 591, 645, 713, 690, 678, 700, 672, 691, 650, 667, 690, 684, 701, 672, 630, 652, 702, 678, 630, 658, 669, 695, 705, 679, 708, 643, 677, 682, 655, 681, 638, 651, 688, 686, 669, 710, 696, 668, 695, 631, 703])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract class labels and counts from the dictionary\n",
        "class_labels = list(class_lengths_dict.keys())\n",
        "class_counts = list(class_lengths_dict.values())\n",
        "\n",
        "# Create a count plot\n",
        "plt.figure(figsize=(4, 5))\n",
        "sns.barplot(x=class_labels, y=class_counts)\n",
        "plt.xticks(rotation=90)\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Number of samples')\n",
        "plt.title('Count of Instances in Each Class')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-hU1isiYpR8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading ResNet-101"
      ],
      "metadata": {
        "id": "GZDNW0tS225-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = dataset.get_num_classes()\n",
        "print(\"Number of classes:\", num_classes)"
      ],
      "metadata": {
        "id": "EH_KW2uuWpXi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b5378ef-8d64-4848-bf04-7a508591b261"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of classes: 101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resnet = torchvision.models.resnet101(pretrained=True)\n",
        "num_features = resnet.fc.in_features\n",
        "resnet.fc = nn.Linear(num_features, num_classes)  # num_classes is the number of classes in your dataset\n",
        "resnet.to(device)\n",
        "print(resnet)"
      ],
      "metadata": {
        "id": "iFFVJbgHN5b-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tuning"
      ],
      "metadata": {
        "id": "tH3B0YYuXbh3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(resnet.parameters(), lr=0.001, momentum=0.9)\n",
        "num_epochs = 20"
      ],
      "metadata": {
        "id": "-16XvcH9OXyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_loss = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Set the model to training mode\n",
        "    resnet.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    # Iterate over the data loader\n",
        "    for images, labels in data_loader:\n",
        "        # Move the images and labels to the GPU if available\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Clear the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = resnet(images)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the running loss\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "    # Calculate the average loss for the epoch\n",
        "    epoch_loss = running_loss / len(data_loader.dataset)\n",
        "    training_loss.append(epoch_loss)\n",
        "    # Print the epoch loss\n",
        "    print(\"Training Loss==========================>>\")\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} Training Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    resnet.eval()\n",
        "    predicted_classes = []\n",
        "    actual_labels = []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in validation_data_loader:\n",
        "            # Move the images and labels to the GPU if available\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = resnet(images)\n",
        "\n",
        "            # Get the predicted labels\n",
        "            _, predicted_label = torch.max(outputs, 1)\n",
        "\n",
        "            # Store the predicted and true labels\n",
        "            predicted_classes.extend(predicted_label.cpu().tolist())\n",
        "            actual_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "    accuracy = accuracy_score(actual_labels, predicted_classes)\n",
        "    precision = precision_score(actual_labels, predicted_classes, average='weighted')\n",
        "    recall = recall_score(actual_labels, predicted_classes, average='weighted')\n",
        "    f1 = f1_score(actual_labels, predicted_classes, average='weighted')\n",
        "\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1-score:\", f1)\n",
        "\n",
        "    print(classification_report(actual_labels, predicted_classes))\n",
        "    print(\"Confusion Matrix: \")\n",
        "    cm = confusion_matrix(actual_labels, predicted_classes)\n",
        "    print(cm)\n",
        "    # auroc = roc_auc_score(actual_labels, predicted_classes)\n",
        "    # print(\"AUROC:\", auroc)\n",
        "\n",
        "    save_dir = \"/content/Model/Models-Train-08/\"\n",
        "    os.makedirs(save_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
        "\n",
        "    model_name = str(epoch+1) + \"_model.pth\"\n",
        "    save_path = os.path.join(save_dir, model_name)  # Specify the complete path to the model file\n",
        "    torch.save(resnet.state_dict(), save_path)"
      ],
      "metadata": {
        "id": "nkOQcQCnGygk",
        "outputId": "a9d42549-1793-4e91-c75f-1193bd386cc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss==========================>>\n",
            "Epoch 1/20 Training Loss: 2.4152\n",
            "Accuracy: 0.5942947702060222\n",
            "Precision: 0.6120863515459704\n",
            "Recall: 0.5942947702060222\n",
            "F1-score: 0.5907770429822937\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.43      0.49       234\n",
            "           1       0.76      0.71      0.73       221\n",
            "           2       0.70      0.77      0.74       226\n",
            "           3       0.81      0.51      0.63       222\n",
            "           4       0.50      0.44      0.47       225\n",
            "           5       0.63      0.75      0.68       224\n",
            "           6       0.78      0.57      0.66       224\n",
            "           7       0.47      0.60      0.53       225\n",
            "           8       0.52      0.54      0.53       226\n",
            "           9       0.61      0.62      0.61       214\n",
            "          10       0.55      0.71      0.62       231\n",
            "          11       0.83      0.62      0.71       227\n",
            "          12       0.64      0.53      0.58       230\n",
            "          13       0.80      0.64      0.71       220\n",
            "          14       0.72      0.53      0.61       231\n",
            "          15       0.55      0.59      0.56       227\n",
            "          16       0.41      0.58      0.48       224\n",
            "          17       0.61      0.61      0.61       233\n",
            "          18       0.44      0.65      0.52       222\n",
            "          19       0.61      0.58      0.59       220\n",
            "          20       0.68      0.85      0.76       219\n",
            "          21       0.68      0.75      0.71       232\n",
            "          22       0.46      0.65      0.54       224\n",
            "          23       0.76      0.73      0.74       230\n",
            "          24       0.57      0.62      0.60       224\n",
            "          25       0.57      0.70      0.63       220\n",
            "          26       0.48      0.76      0.59       221\n",
            "          27       0.76      0.73      0.74       225\n",
            "          28       0.82      0.64      0.72       224\n",
            "          29       0.61      0.71      0.66       228\n",
            "          30       0.85      0.91      0.88       229\n",
            "          31       0.65      0.65      0.65       232\n",
            "          32       0.58      0.25      0.35       228\n",
            "          33       0.43      0.67      0.52       231\n",
            "          34       0.71      0.77      0.74       213\n",
            "          35       0.28      0.46      0.35       202\n",
            "          36       0.63      0.61      0.62       228\n",
            "          37       0.42      0.67      0.52       212\n",
            "          38       0.40      0.61      0.49       208\n",
            "          39       0.45      0.27      0.34       216\n",
            "          40       0.65      0.67      0.66       212\n",
            "          41       0.90      0.57      0.70       223\n",
            "          42       0.63      0.42      0.51       238\n",
            "          43       0.62      0.54      0.58       212\n",
            "          44       0.41      0.79      0.54       229\n",
            "          45       0.50      0.46      0.48       233\n",
            "          46       0.51      0.63      0.56       224\n",
            "          47       0.53      0.41      0.46       234\n",
            "          48       0.50      0.76      0.61       223\n",
            "          49       0.66      0.76      0.71       229\n",
            "          50       0.75      0.55      0.63       225\n",
            "          51       0.76      0.79      0.77       228\n",
            "          52       0.53      0.43      0.47       232\n",
            "          53       0.73      0.29      0.41       233\n",
            "          54       0.44      0.54      0.48       203\n",
            "          55       0.11      0.09      0.10       234\n",
            "          56       0.58      0.57      0.58       223\n",
            "          57       0.62      0.58      0.60       234\n",
            "          58       0.52      0.34      0.41       232\n",
            "          59       0.68      0.79      0.73       233\n",
            "          60       0.62      0.60      0.61       197\n",
            "          61       0.27      0.38      0.32       215\n",
            "          62       0.55      0.62      0.58       238\n",
            "          63       0.75      0.81      0.78       231\n",
            "          64       0.68      0.57      0.62       227\n",
            "          65       0.75      0.80      0.77       234\n",
            "          66       0.59      0.63      0.61       224\n",
            "          67       0.66      0.55      0.60       231\n",
            "          68       0.80      0.76      0.78       217\n",
            "          69       0.70      0.45      0.55       223\n",
            "          70       0.54      0.71      0.61       230\n",
            "          71       0.61      0.68      0.64       228\n",
            "          72       0.73      0.67      0.70       234\n",
            "          73       0.67      0.71      0.69       225\n",
            "          74       0.54      0.38      0.44       211\n",
            "          75       0.61      0.61      0.61       218\n",
            "          76       0.68      0.60      0.64       235\n",
            "          77       0.59      0.32      0.41       227\n",
            "          78       0.62      0.62      0.62       210\n",
            "          79       0.75      0.69      0.72       220\n",
            "          80       0.76      0.76      0.76       224\n",
            "          81       0.59      0.15      0.24       232\n",
            "          82       0.49      0.36      0.42       236\n",
            "          83       0.64      0.75      0.69       227\n",
            "          84       0.55      0.36      0.43       236\n",
            "          85       0.77      0.66      0.71       215\n",
            "          86       0.33      0.45      0.38       226\n",
            "          87       0.56      0.55      0.55       228\n",
            "          88       0.71      0.56      0.63       219\n",
            "          89       0.68      0.45      0.54       228\n",
            "          90       0.56      0.85      0.67       213\n",
            "          91       0.83      0.67      0.74       218\n",
            "          92       0.78      0.63      0.70       230\n",
            "          93       0.60      0.24      0.34       229\n",
            "          94       0.74      0.76      0.75       223\n",
            "          95       0.55      0.68      0.61       237\n",
            "          96       0.58      0.57      0.57       233\n",
            "          97       0.44      0.63      0.52       223\n",
            "          98       0.80      0.69      0.74       232\n",
            "          99       0.48      0.38      0.42       211\n",
            "         100       0.67      0.78      0.72       235\n",
            "\n",
            "    accuracy                           0.59     22716\n",
            "   macro avg       0.61      0.59      0.59     22716\n",
            "weighted avg       0.61      0.59      0.59     22716\n",
            "\n",
            "Confusion Matrix: \n",
            "[[101   0   8 ...   0   0  12]\n",
            " [  0 156   2 ...   0   0   0]\n",
            " [  3   0 175 ...   0   0   0]\n",
            " ...\n",
            " [  3   0   0 ... 161   0   1]\n",
            " [  0   0   0 ...   0  80   1]\n",
            " [  2   0   1 ...   0   0 184]]\n",
            "Training Loss==========================>>\n",
            "Epoch 2/20 Training Loss: 1.4843\n",
            "Accuracy: 0.6384046487057581\n",
            "Precision: 0.6485675144065766\n",
            "Recall: 0.6384046487057581\n",
            "F1-score: 0.6370532012753518\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.57      0.60       234\n",
            "           1       0.87      0.59      0.71       221\n",
            "           2       0.81      0.80      0.80       226\n",
            "           3       0.76      0.65      0.70       222\n",
            "           4       0.55      0.48      0.51       225\n",
            "           5       0.78      0.75      0.77       224\n",
            "           6       0.71      0.66      0.69       224\n",
            "           7       0.73      0.54      0.62       225\n",
            "           8       0.46      0.68      0.54       226\n",
            "           9       0.73      0.56      0.63       214\n",
            "          10       0.69      0.74      0.71       231\n",
            "          11       0.64      0.84      0.72       227\n",
            "          12       0.74      0.57      0.65       230\n",
            "          13       0.73      0.77      0.75       220\n",
            "          14       0.51      0.66      0.58       231\n",
            "          15       0.62      0.60      0.61       227\n",
            "          16       0.39      0.54      0.45       224\n",
            "          17       0.62      0.66      0.64       233\n",
            "          18       0.65      0.62      0.64       222\n",
            "          19       0.86      0.46      0.60       220\n",
            "          20       0.79      0.87      0.83       219\n",
            "          21       0.76      0.70      0.73       232\n",
            "          22       0.56      0.60      0.58       224\n",
            "          23       0.86      0.70      0.78       230\n",
            "          24       0.57      0.72      0.64       224\n",
            "          25       0.61      0.73      0.66       220\n",
            "          26       0.62      0.72      0.67       221\n",
            "          27       0.76      0.76      0.76       225\n",
            "          28       0.69      0.75      0.72       224\n",
            "          29       0.71      0.61      0.66       228\n",
            "          30       0.89      0.92      0.90       229\n",
            "          31       0.72      0.70      0.71       232\n",
            "          32       0.53      0.43      0.48       228\n",
            "          33       0.70      0.52      0.59       231\n",
            "          34       0.88      0.71      0.79       213\n",
            "          35       0.61      0.36      0.45       202\n",
            "          36       0.59      0.64      0.61       228\n",
            "          37       0.70      0.49      0.57       212\n",
            "          38       0.62      0.56      0.59       208\n",
            "          39       0.38      0.39      0.38       216\n",
            "          40       0.72      0.66      0.69       212\n",
            "          41       0.75      0.74      0.74       223\n",
            "          42       0.64      0.53      0.58       238\n",
            "          43       0.77      0.54      0.64       212\n",
            "          44       0.58      0.75      0.65       229\n",
            "          45       0.45      0.62      0.52       233\n",
            "          46       0.54      0.68      0.60       224\n",
            "          47       0.60      0.47      0.53       234\n",
            "          48       0.72      0.67      0.69       223\n",
            "          49       0.72      0.71      0.71       229\n",
            "          50       0.69      0.65      0.67       225\n",
            "          51       0.74      0.84      0.78       228\n",
            "          52       0.51      0.57      0.54       232\n",
            "          53       0.56      0.41      0.48       233\n",
            "          54       0.55      0.52      0.53       203\n",
            "          55       0.22      0.11      0.15       234\n",
            "          56       0.71      0.52      0.60       223\n",
            "          57       0.68      0.65      0.67       234\n",
            "          58       0.48      0.41      0.44       232\n",
            "          59       0.66      0.82      0.73       233\n",
            "          60       0.58      0.61      0.59       197\n",
            "          61       0.31      0.43      0.36       215\n",
            "          62       0.59      0.61      0.60       238\n",
            "          63       0.89      0.77      0.83       231\n",
            "          64       0.62      0.71      0.66       227\n",
            "          65       0.72      0.85      0.78       234\n",
            "          66       0.58      0.76      0.66       224\n",
            "          67       0.67      0.64      0.65       231\n",
            "          68       0.78      0.79      0.79       217\n",
            "          69       0.55      0.51      0.53       223\n",
            "          70       0.66      0.76      0.71       230\n",
            "          71       0.74      0.70      0.72       228\n",
            "          72       0.71      0.74      0.72       234\n",
            "          73       0.58      0.78      0.66       225\n",
            "          74       0.35      0.55      0.43       211\n",
            "          75       0.61      0.68      0.65       218\n",
            "          76       0.64      0.73      0.68       235\n",
            "          77       0.49      0.53      0.51       227\n",
            "          78       0.66      0.69      0.67       210\n",
            "          79       0.56      0.84      0.67       220\n",
            "          80       0.83      0.79      0.81       224\n",
            "          81       0.53      0.44      0.48       232\n",
            "          82       0.59      0.41      0.48       236\n",
            "          83       0.76      0.74      0.75       227\n",
            "          84       0.54      0.58      0.56       236\n",
            "          85       0.76      0.67      0.71       215\n",
            "          86       0.36      0.47      0.41       226\n",
            "          87       0.63      0.60      0.61       228\n",
            "          88       0.69      0.63      0.66       219\n",
            "          89       0.69      0.57      0.63       228\n",
            "          90       0.75      0.85      0.79       213\n",
            "          91       0.83      0.79      0.81       218\n",
            "          92       0.77      0.67      0.72       230\n",
            "          93       0.57      0.36      0.44       229\n",
            "          94       0.73      0.81      0.77       223\n",
            "          95       0.69      0.64      0.67       237\n",
            "          96       0.59      0.63      0.61       233\n",
            "          97       0.54      0.67      0.60       223\n",
            "          98       0.63      0.80      0.70       232\n",
            "          99       0.57      0.43      0.49       211\n",
            "         100       0.86      0.75      0.80       235\n",
            "\n",
            "    accuracy                           0.64     22716\n",
            "   macro avg       0.65      0.64      0.64     22716\n",
            "weighted avg       0.65      0.64      0.64     22716\n",
            "\n",
            "Confusion Matrix: \n",
            "[[134   0   3 ...   2   0   1]\n",
            " [  0 131   1 ...   1   0   0]\n",
            " [  2   0 181 ...   1   0   0]\n",
            " ...\n",
            " [  0   0   0 ... 185   0   0]\n",
            " [  1   0   0 ...   0  90   1]\n",
            " [  3   0   1 ...   3   1 177]]\n",
            "Training Loss==========================>>\n",
            "Epoch 3/20 Training Loss: 1.0752\n",
            "Accuracy: 0.6444356400774784\n",
            "Precision: 0.6613350129335318\n",
            "Recall: 0.6444356400774784\n",
            "F1-score: 0.6460103220041824\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.55      0.60       234\n",
            "           1       0.76      0.66      0.70       221\n",
            "           2       0.73      0.84      0.78       226\n",
            "           3       0.59      0.75      0.66       222\n",
            "           4       0.57      0.46      0.51       225\n",
            "           5       0.73      0.74      0.73       224\n",
            "           6       0.60      0.69      0.64       224\n",
            "           7       0.76      0.53      0.62       225\n",
            "           8       0.56      0.64      0.60       226\n",
            "           9       0.51      0.68      0.59       214\n",
            "          10       0.77      0.69      0.73       231\n",
            "          11       0.80      0.74      0.77       227\n",
            "          12       0.55      0.70      0.62       230\n",
            "          13       0.79      0.72      0.75       220\n",
            "          14       0.70      0.64      0.67       231\n",
            "          15       0.63      0.61      0.62       227\n",
            "          16       0.57      0.52      0.55       224\n",
            "          17       0.66      0.64      0.65       233\n",
            "          18       0.66      0.56      0.60       222\n",
            "          19       0.67      0.55      0.61       220\n",
            "          20       0.84      0.82      0.83       219\n",
            "          21       0.75      0.69      0.72       232\n",
            "          22       0.63      0.64      0.64       224\n",
            "          23       0.85      0.73      0.78       230\n",
            "          24       0.54      0.76      0.64       224\n",
            "          25       0.79      0.66      0.72       220\n",
            "          26       0.69      0.71      0.70       221\n",
            "          27       0.80      0.74      0.77       225\n",
            "          28       0.57      0.78      0.66       224\n",
            "          29       0.78      0.50      0.61       228\n",
            "          30       0.93      0.93      0.93       229\n",
            "          31       0.60      0.72      0.66       232\n",
            "          32       0.56      0.39      0.46       228\n",
            "          33       0.49      0.68      0.57       231\n",
            "          34       0.82      0.77      0.79       213\n",
            "          35       0.42      0.50      0.45       202\n",
            "          36       0.79      0.57      0.66       228\n",
            "          37       0.74      0.49      0.59       212\n",
            "          38       0.60      0.55      0.57       208\n",
            "          39       0.41      0.41      0.41       216\n",
            "          40       0.78      0.64      0.70       212\n",
            "          41       0.68      0.78      0.73       223\n",
            "          42       0.62      0.56      0.59       238\n",
            "          43       0.65      0.72      0.69       212\n",
            "          44       0.64      0.67      0.65       229\n",
            "          45       0.49      0.49      0.49       233\n",
            "          46       0.41      0.75      0.53       224\n",
            "          47       0.48      0.63      0.54       234\n",
            "          48       0.73      0.67      0.70       223\n",
            "          49       0.69      0.76      0.72       229\n",
            "          50       0.73      0.66      0.69       225\n",
            "          51       0.84      0.81      0.82       228\n",
            "          52       0.58      0.50      0.54       232\n",
            "          53       0.51      0.43      0.47       233\n",
            "          54       0.51      0.61      0.55       203\n",
            "          55       0.23      0.29      0.26       234\n",
            "          56       0.63      0.67      0.65       223\n",
            "          57       0.71      0.59      0.64       234\n",
            "          58       0.43      0.48      0.45       232\n",
            "          59       0.68      0.80      0.74       233\n",
            "          60       0.61      0.61      0.61       197\n",
            "          61       0.45      0.38      0.41       215\n",
            "          62       0.46      0.71      0.56       238\n",
            "          63       0.82      0.79      0.80       231\n",
            "          64       0.76      0.64      0.70       227\n",
            "          65       0.84      0.80      0.82       234\n",
            "          66       0.55      0.75      0.64       224\n",
            "          67       0.67      0.64      0.66       231\n",
            "          68       0.82      0.72      0.77       217\n",
            "          69       0.60      0.55      0.57       223\n",
            "          70       0.71      0.72      0.71       230\n",
            "          71       0.87      0.68      0.76       228\n",
            "          72       0.81      0.67      0.73       234\n",
            "          73       0.56      0.84      0.67       225\n",
            "          74       0.38      0.57      0.46       211\n",
            "          75       0.75      0.61      0.67       218\n",
            "          76       0.67      0.74      0.70       235\n",
            "          77       0.62      0.34      0.44       227\n",
            "          78       0.70      0.69      0.69       210\n",
            "          79       0.71      0.78      0.75       220\n",
            "          80       0.81      0.79      0.80       224\n",
            "          81       0.51      0.56      0.53       232\n",
            "          82       0.55      0.48      0.51       236\n",
            "          83       0.80      0.71      0.75       227\n",
            "          84       0.54      0.62      0.58       236\n",
            "          85       0.82      0.65      0.72       215\n",
            "          86       0.57      0.39      0.46       226\n",
            "          87       0.67      0.55      0.60       228\n",
            "          88       0.71      0.66      0.68       219\n",
            "          89       0.73      0.50      0.59       228\n",
            "          90       0.71      0.83      0.77       213\n",
            "          91       0.88      0.77      0.82       218\n",
            "          92       0.74      0.77      0.75       230\n",
            "          93       0.38      0.52      0.44       229\n",
            "          94       0.74      0.81      0.77       223\n",
            "          95       0.72      0.65      0.68       237\n",
            "          96       0.82      0.48      0.61       233\n",
            "          97       0.60      0.72      0.65       223\n",
            "          98       0.74      0.78      0.76       232\n",
            "          99       0.57      0.44      0.50       211\n",
            "         100       0.92      0.74      0.82       235\n",
            "\n",
            "    accuracy                           0.64     22716\n",
            "   macro avg       0.66      0.64      0.65     22716\n",
            "weighted avg       0.66      0.64      0.65     22716\n",
            "\n",
            "Confusion Matrix: \n",
            "[[128   0   6 ...   2   0   1]\n",
            " [  0 145   0 ...   2   0   0]\n",
            " [  3   0 190 ...   1   0   0]\n",
            " ...\n",
            " [  0   0   1 ... 181   0   0]\n",
            " [  0   0   0 ...   0  93   0]\n",
            " [  4   0   2 ...   3   0 173]]\n",
            "Training Loss==========================>>\n",
            "Epoch 4/20 Training Loss: 0.7554\n",
            "Accuracy: 0.6562334918119387\n",
            "Precision: 0.6667920459210385\n",
            "Recall: 0.6562334918119387\n",
            "F1-score: 0.6557193817261289\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.63      0.60       234\n",
            "           1       0.77      0.72      0.74       221\n",
            "           2       0.75      0.86      0.80       226\n",
            "           3       0.82      0.66      0.73       222\n",
            "           4       0.51      0.46      0.49       225\n",
            "           5       0.83      0.75      0.78       224\n",
            "           6       0.58      0.71      0.64       224\n",
            "           7       0.72      0.61      0.66       225\n",
            "           8       0.49      0.65      0.56       226\n",
            "           9       0.69      0.61      0.65       214\n",
            "          10       0.66      0.74      0.70       231\n",
            "          11       0.82      0.74      0.78       227\n",
            "          12       0.59      0.64      0.62       230\n",
            "          13       0.69      0.77      0.73       220\n",
            "          14       0.56      0.64      0.59       231\n",
            "          15       0.46      0.76      0.58       227\n",
            "          16       0.69      0.53      0.60       224\n",
            "          17       0.78      0.56      0.65       233\n",
            "          18       0.68      0.54      0.60       222\n",
            "          19       0.72      0.59      0.65       220\n",
            "          20       0.80      0.84      0.82       219\n",
            "          21       0.59      0.82      0.69       232\n",
            "          22       0.57      0.62      0.59       224\n",
            "          23       0.70      0.84      0.76       230\n",
            "          24       0.64      0.63      0.63       224\n",
            "          25       0.79      0.63      0.70       220\n",
            "          26       0.64      0.71      0.68       221\n",
            "          27       0.78      0.76      0.77       225\n",
            "          28       0.81      0.72      0.76       224\n",
            "          29       0.64      0.65      0.64       228\n",
            "          30       0.92      0.92      0.92       229\n",
            "          31       0.66      0.69      0.68       232\n",
            "          32       0.46      0.48      0.47       228\n",
            "          33       0.71      0.57      0.63       231\n",
            "          34       0.82      0.79      0.80       213\n",
            "          35       0.53      0.45      0.49       202\n",
            "          36       0.63      0.63      0.63       228\n",
            "          37       0.71      0.53      0.61       212\n",
            "          38       0.63      0.54      0.59       208\n",
            "          39       0.51      0.39      0.44       216\n",
            "          40       0.74      0.63      0.68       212\n",
            "          41       0.68      0.80      0.74       223\n",
            "          42       0.59      0.60      0.60       238\n",
            "          43       0.69      0.73      0.71       212\n",
            "          44       0.59      0.72      0.65       229\n",
            "          45       0.51      0.58      0.54       233\n",
            "          46       0.64      0.66      0.65       224\n",
            "          47       0.60      0.60      0.60       234\n",
            "          48       0.68      0.73      0.70       223\n",
            "          49       0.53      0.89      0.67       229\n",
            "          50       0.76      0.72      0.74       225\n",
            "          51       0.71      0.85      0.77       228\n",
            "          52       0.52      0.55      0.53       232\n",
            "          53       0.50      0.45      0.48       233\n",
            "          54       0.53      0.62      0.57       203\n",
            "          55       0.40      0.41      0.40       234\n",
            "          56       0.75      0.61      0.67       223\n",
            "          57       0.57      0.69      0.63       234\n",
            "          58       0.57      0.33      0.42       232\n",
            "          59       0.72      0.76      0.74       233\n",
            "          60       0.53      0.68      0.59       197\n",
            "          61       0.34      0.40      0.36       215\n",
            "          62       0.58      0.65      0.62       238\n",
            "          63       0.90      0.74      0.81       231\n",
            "          64       0.81      0.61      0.70       227\n",
            "          65       0.89      0.80      0.84       234\n",
            "          66       0.71      0.70      0.70       224\n",
            "          67       0.74      0.58      0.65       231\n",
            "          68       0.87      0.67      0.76       217\n",
            "          69       0.62      0.57      0.59       223\n",
            "          70       0.73      0.67      0.70       230\n",
            "          71       0.80      0.72      0.76       228\n",
            "          72       0.70      0.73      0.72       234\n",
            "          73       0.76      0.76      0.76       225\n",
            "          74       0.50      0.52      0.51       211\n",
            "          75       0.70      0.62      0.66       218\n",
            "          76       0.48      0.80      0.60       235\n",
            "          77       0.52      0.51      0.51       227\n",
            "          78       0.73      0.68      0.70       210\n",
            "          79       0.85      0.74      0.79       220\n",
            "          80       0.73      0.81      0.77       224\n",
            "          81       0.50      0.58      0.54       232\n",
            "          82       0.72      0.46      0.56       236\n",
            "          83       0.71      0.77      0.74       227\n",
            "          84       0.64      0.56      0.60       236\n",
            "          85       0.90      0.58      0.70       215\n",
            "          86       0.63      0.37      0.46       226\n",
            "          87       0.62      0.57      0.59       228\n",
            "          88       0.73      0.70      0.72       219\n",
            "          89       0.60      0.63      0.61       228\n",
            "          90       0.77      0.82      0.80       213\n",
            "          91       0.79      0.88      0.83       218\n",
            "          92       0.70      0.78      0.74       230\n",
            "          93       0.51      0.52      0.52       229\n",
            "          94       0.63      0.86      0.73       223\n",
            "          95       0.67      0.65      0.66       237\n",
            "          96       0.68      0.56      0.62       233\n",
            "          97       0.70      0.71      0.71       223\n",
            "          98       0.78      0.68      0.73       232\n",
            "          99       0.58      0.49      0.53       211\n",
            "         100       0.81      0.81      0.81       235\n",
            "\n",
            "    accuracy                           0.66     22716\n",
            "   macro avg       0.67      0.66      0.66     22716\n",
            "weighted avg       0.67      0.66      0.66     22716\n",
            "\n",
            "Confusion Matrix: \n",
            "[[148   0   6 ...   1   0   6]\n",
            " [  0 159   0 ...   1   0   0]\n",
            " [  5   0 195 ...   0   1   1]\n",
            " ...\n",
            " [  2   0   0 ... 157   0   0]\n",
            " [  0   1   0 ...   0 104   2]\n",
            " [  0   1   0 ...   0   0 191]]\n",
            "Training Loss==========================>>\n",
            "Epoch 5/20 Training Loss: 0.5193\n",
            "Accuracy: 0.6566737101602395\n",
            "Precision: 0.665471349294062\n",
            "Recall: 0.6566737101602395\n",
            "F1-score: 0.6559057923561832\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.62      0.60       234\n",
            "           1       0.74      0.76      0.75       221\n",
            "           2       0.79      0.78      0.79       226\n",
            "           3       0.69      0.72      0.70       222\n",
            "           4       0.57      0.39      0.46       225\n",
            "           5       0.67      0.79      0.73       224\n",
            "           6       0.61      0.70      0.65       224\n",
            "           7       0.51      0.67      0.57       225\n",
            "           8       0.42      0.75      0.54       226\n",
            "           9       0.73      0.57      0.64       214\n",
            "          10       0.64      0.76      0.70       231\n",
            "          11       0.63      0.79      0.70       227\n",
            "          12       0.72      0.63      0.67       230\n",
            "          13       0.71      0.78      0.74       220\n",
            "          14       0.75      0.58      0.66       231\n",
            "          15       0.70      0.52      0.60       227\n",
            "          16       0.54      0.54      0.54       224\n",
            "          17       0.69      0.66      0.67       233\n",
            "          18       0.61      0.63      0.62       222\n",
            "          19       0.66      0.61      0.63       220\n",
            "          20       0.85      0.82      0.83       219\n",
            "          21       0.69      0.75      0.71       232\n",
            "          22       0.63      0.63      0.63       224\n",
            "          23       0.86      0.77      0.81       230\n",
            "          24       0.75      0.65      0.70       224\n",
            "          25       0.72      0.76      0.74       220\n",
            "          26       0.69      0.69      0.69       221\n",
            "          27       0.70      0.77      0.73       225\n",
            "          28       0.72      0.71      0.72       224\n",
            "          29       0.61      0.68      0.64       228\n",
            "          30       0.90      0.88      0.89       229\n",
            "          31       0.67      0.64      0.66       232\n",
            "          32       0.50      0.46      0.48       228\n",
            "          33       0.54      0.71      0.61       231\n",
            "          34       0.82      0.78      0.80       213\n",
            "          35       0.68      0.36      0.47       202\n",
            "          36       0.64      0.59      0.62       228\n",
            "          37       0.74      0.59      0.66       212\n",
            "          38       0.55      0.53      0.54       208\n",
            "          39       0.40      0.44      0.42       216\n",
            "          40       0.67      0.68      0.68       212\n",
            "          41       0.66      0.79      0.72       223\n",
            "          42       0.59      0.58      0.58       238\n",
            "          43       0.83      0.67      0.74       212\n",
            "          44       0.66      0.68      0.67       229\n",
            "          45       0.52      0.50      0.51       233\n",
            "          46       0.72      0.59      0.65       224\n",
            "          47       0.52      0.61      0.56       234\n",
            "          48       0.64      0.73      0.68       223\n",
            "          49       0.77      0.76      0.76       229\n",
            "          50       0.75      0.68      0.71       225\n",
            "          51       0.77      0.79      0.78       228\n",
            "          52       0.59      0.51      0.55       232\n",
            "          53       0.51      0.49      0.50       233\n",
            "          54       0.59      0.53      0.56       203\n",
            "          55       0.48      0.42      0.45       234\n",
            "          56       0.73      0.60      0.66       223\n",
            "          57       0.63      0.70      0.66       234\n",
            "          58       0.42      0.53      0.47       232\n",
            "          59       0.63      0.85      0.72       233\n",
            "          60       0.69      0.62      0.65       197\n",
            "          61       0.30      0.48      0.37       215\n",
            "          62       0.63      0.58      0.60       238\n",
            "          63       0.84      0.74      0.79       231\n",
            "          64       0.76      0.66      0.71       227\n",
            "          65       0.76      0.85      0.80       234\n",
            "          66       0.64      0.76      0.70       224\n",
            "          67       0.56      0.70      0.63       231\n",
            "          68       0.80      0.76      0.78       217\n",
            "          69       0.64      0.55      0.59       223\n",
            "          70       0.59      0.77      0.67       230\n",
            "          71       0.64      0.80      0.71       228\n",
            "          72       0.76      0.75      0.76       234\n",
            "          73       0.73      0.75      0.74       225\n",
            "          74       0.44      0.55      0.49       211\n",
            "          75       0.62      0.72      0.67       218\n",
            "          76       0.73      0.68      0.70       235\n",
            "          77       0.67      0.42      0.51       227\n",
            "          78       0.69      0.69      0.69       210\n",
            "          79       0.78      0.83      0.80       220\n",
            "          80       0.73      0.85      0.79       224\n",
            "          81       0.54      0.51      0.52       232\n",
            "          82       0.60      0.48      0.54       236\n",
            "          83       0.84      0.67      0.74       227\n",
            "          84       0.79      0.38      0.51       236\n",
            "          85       0.75      0.69      0.72       215\n",
            "          86       0.52      0.42      0.46       226\n",
            "          87       0.69      0.57      0.62       228\n",
            "          88       0.74      0.68      0.71       219\n",
            "          89       0.63      0.61      0.62       228\n",
            "          90       0.75      0.84      0.79       213\n",
            "          91       0.75      0.85      0.80       218\n",
            "          92       0.79      0.75      0.77       230\n",
            "          93       0.54      0.46      0.50       229\n",
            "          94       0.79      0.81      0.80       223\n",
            "          95       0.72      0.63      0.67       237\n",
            "          96       0.55      0.62      0.59       233\n",
            "          97       0.72      0.65      0.68       223\n",
            "          98       0.78      0.78      0.78       232\n",
            "          99       0.61      0.48      0.54       211\n",
            "         100       0.79      0.78      0.79       235\n",
            "\n",
            "    accuracy                           0.66     22716\n",
            "   macro avg       0.67      0.66      0.66     22716\n",
            "weighted avg       0.67      0.66      0.66     22716\n",
            "\n",
            "Confusion Matrix: \n",
            "[[144   0   2 ...   1   0   3]\n",
            " [  0 169   0 ...   1   0   0]\n",
            " [  2   0 177 ...   0   0   0]\n",
            " ...\n",
            " [  2   0   0 ... 182   0   2]\n",
            " [  0   0   1 ...   0 102   1]\n",
            " [  0   0   1 ...   1   0 184]]\n",
            "Training Loss==========================>>\n",
            "Epoch 6/20 Training Loss: 0.3590\n",
            "Accuracy: 0.6445677055819686\n",
            "Precision: 0.6569727908522812\n",
            "Recall: 0.6445677055819686\n",
            "F1-score: 0.6452305898484729\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.48      0.64      0.55       234\n",
            "           1       0.75      0.72      0.74       221\n",
            "           2       0.74      0.81      0.78       226\n",
            "           3       0.58      0.70      0.63       222\n",
            "           4       0.48      0.42      0.45       225\n",
            "           5       0.77      0.71      0.74       224\n",
            "           6       0.51      0.68      0.59       224\n",
            "           7       0.69      0.60      0.64       225\n",
            "           8       0.62      0.53      0.57       226\n",
            "           9       0.64      0.62      0.63       214\n",
            "          10       0.67      0.69      0.68       231\n",
            "          11       0.83      0.68      0.75       227\n",
            "          12       0.73      0.59      0.65       230\n",
            "          13       0.67      0.82      0.74       220\n",
            "          14       0.71      0.58      0.64       231\n",
            "          15       0.63      0.54      0.58       227\n",
            "          16       0.57      0.47      0.51       224\n",
            "          17       0.82      0.54      0.65       233\n",
            "          18       0.51      0.67      0.58       222\n",
            "          19       0.54      0.70      0.61       220\n",
            "          20       0.73      0.86      0.79       219\n",
            "          21       0.67      0.70      0.69       232\n",
            "          22       0.60      0.57      0.58       224\n",
            "          23       0.85      0.75      0.80       230\n",
            "          24       0.69      0.74      0.71       224\n",
            "          25       0.69      0.80      0.74       220\n",
            "          26       0.62      0.71      0.66       221\n",
            "          27       0.64      0.80      0.71       225\n",
            "          28       0.75      0.68      0.71       224\n",
            "          29       0.59      0.65      0.62       228\n",
            "          30       0.92      0.93      0.93       229\n",
            "          31       0.60      0.71      0.65       232\n",
            "          32       0.46      0.52      0.49       228\n",
            "          33       0.57      0.63      0.60       231\n",
            "          34       0.81      0.77      0.79       213\n",
            "          35       0.47      0.44      0.45       202\n",
            "          36       0.58      0.65      0.62       228\n",
            "          37       0.65      0.57      0.61       212\n",
            "          38       0.48      0.63      0.54       208\n",
            "          39       0.47      0.44      0.45       216\n",
            "          40       0.79      0.61      0.69       212\n",
            "          41       0.85      0.65      0.73       223\n",
            "          42       0.57      0.60      0.58       238\n",
            "          43       0.71      0.69      0.70       212\n",
            "          44       0.75      0.50      0.60       229\n",
            "          45       0.47      0.49      0.48       233\n",
            "          46       0.67      0.54      0.60       224\n",
            "          47       0.64      0.46      0.53       234\n",
            "          48       0.75      0.68      0.71       223\n",
            "          49       0.77      0.73      0.75       229\n",
            "          50       0.82      0.62      0.71       225\n",
            "          51       0.74      0.82      0.78       228\n",
            "          52       0.71      0.41      0.52       232\n",
            "          53       0.50      0.46      0.48       233\n",
            "          54       0.59      0.59      0.59       203\n",
            "          55       0.51      0.43      0.47       234\n",
            "          56       0.70      0.61      0.65       223\n",
            "          57       0.54      0.71      0.61       234\n",
            "          58       0.36      0.52      0.42       232\n",
            "          59       0.80      0.74      0.77       233\n",
            "          60       0.64      0.57      0.60       197\n",
            "          61       0.32      0.42      0.36       215\n",
            "          62       0.55      0.63      0.59       238\n",
            "          63       0.75      0.77      0.76       231\n",
            "          64       0.69      0.71      0.70       227\n",
            "          65       0.88      0.81      0.85       234\n",
            "          66       0.59      0.73      0.65       224\n",
            "          67       0.54      0.68      0.60       231\n",
            "          68       0.81      0.76      0.78       217\n",
            "          69       0.69      0.51      0.58       223\n",
            "          70       0.74      0.68      0.71       230\n",
            "          71       0.71      0.76      0.74       228\n",
            "          72       0.67      0.71      0.69       234\n",
            "          73       0.64      0.75      0.69       225\n",
            "          74       0.44      0.49      0.46       211\n",
            "          75       0.65      0.67      0.66       218\n",
            "          76       0.65      0.72      0.68       235\n",
            "          77       0.49      0.61      0.54       227\n",
            "          78       0.80      0.65      0.72       210\n",
            "          79       0.84      0.76      0.80       220\n",
            "          80       0.78      0.79      0.78       224\n",
            "          81       0.58      0.46      0.51       232\n",
            "          82       0.50      0.55      0.52       236\n",
            "          83       0.68      0.78      0.73       227\n",
            "          84       0.76      0.39      0.52       236\n",
            "          85       0.65      0.73      0.68       215\n",
            "          86       0.42      0.50      0.46       226\n",
            "          87       0.58      0.59      0.58       228\n",
            "          88       0.71      0.68      0.70       219\n",
            "          89       0.76      0.53      0.63       228\n",
            "          90       0.88      0.81      0.85       213\n",
            "          91       0.87      0.79      0.83       218\n",
            "          92       0.78      0.78      0.78       230\n",
            "          93       0.50      0.46      0.48       229\n",
            "          94       0.62      0.85      0.71       223\n",
            "          95       0.75      0.61      0.67       237\n",
            "          96       0.67      0.55      0.60       233\n",
            "          97       0.66      0.67      0.67       223\n",
            "          98       0.72      0.74      0.73       232\n",
            "          99       0.51      0.55      0.53       211\n",
            "         100       0.86      0.77      0.81       235\n",
            "\n",
            "    accuracy                           0.64     22716\n",
            "   macro avg       0.66      0.64      0.65     22716\n",
            "weighted avg       0.66      0.64      0.65     22716\n",
            "\n",
            "Confusion Matrix: \n",
            "[[150   0   2 ...   1   0   1]\n",
            " [  0 159   0 ...   1   0   0]\n",
            " [  6   0 183 ...   1   0   0]\n",
            " ...\n",
            " [  3   0   0 ... 171   0   1]\n",
            " [  0   1   0 ...   0 115   0]\n",
            " [  1   0   1 ...   0   0 182]]\n",
            "Training Loss==========================>>\n",
            "Epoch 7/20 Training Loss: 0.2393\n",
            "Accuracy: 0.6546046839232259\n",
            "Precision: 0.6621624805041578\n",
            "Recall: 0.6546046839232259\n",
            "F1-score: 0.6548089561190983\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.56      0.59       234\n",
            "           1       0.63      0.78      0.69       221\n",
            "           2       0.77      0.85      0.81       226\n",
            "           3       0.72      0.65      0.68       222\n",
            "           4       0.53      0.41      0.46       225\n",
            "           5       0.77      0.70      0.73       224\n",
            "           6       0.63      0.67      0.65       224\n",
            "           7       0.61      0.66      0.63       225\n",
            "           8       0.58      0.65      0.61       226\n",
            "           9       0.65      0.63      0.64       214\n",
            "          10       0.70      0.72      0.71       231\n",
            "          11       0.73      0.78      0.75       227\n",
            "          12       0.69      0.61      0.65       230\n",
            "          13       0.79      0.72      0.75       220\n",
            "          14       0.65      0.61      0.63       231\n",
            "          15       0.65      0.62      0.64       227\n",
            "          16       0.50      0.50      0.50       224\n",
            "          17       0.67      0.64      0.65       233\n",
            "          18       0.69      0.60      0.64       222\n",
            "          19       0.56      0.69      0.62       220\n",
            "          20       0.67      0.87      0.76       219\n",
            "          21       0.72      0.72      0.72       232\n",
            "          22       0.58      0.66      0.62       224\n",
            "          23       0.81      0.77      0.79       230\n",
            "          24       0.73      0.73      0.73       224\n",
            "          25       0.66      0.77      0.71       220\n",
            "          26       0.66      0.69      0.68       221\n",
            "          27       0.75      0.78      0.76       225\n",
            "          28       0.70      0.73      0.71       224\n",
            "          29       0.65      0.65      0.65       228\n",
            "          30       0.91      0.88      0.90       229\n",
            "          31       0.73      0.62      0.67       232\n",
            "          32       0.48      0.53      0.50       228\n",
            "          33       0.69      0.61      0.65       231\n",
            "          34       0.84      0.70      0.77       213\n",
            "          35       0.45      0.43      0.44       202\n",
            "          36       0.61      0.69      0.65       228\n",
            "          37       0.67      0.59      0.63       212\n",
            "          38       0.49      0.56      0.52       208\n",
            "          39       0.41      0.45      0.43       216\n",
            "          40       0.73      0.72      0.72       212\n",
            "          41       0.71      0.78      0.74       223\n",
            "          42       0.60      0.64      0.62       238\n",
            "          43       0.70      0.70      0.70       212\n",
            "          44       0.59      0.71      0.64       229\n",
            "          45       0.60      0.35      0.44       233\n",
            "          46       0.64      0.64      0.64       224\n",
            "          47       0.60      0.53      0.56       234\n",
            "          48       0.66      0.79      0.72       223\n",
            "          49       0.70      0.75      0.73       229\n",
            "          50       0.75      0.64      0.69       225\n",
            "          51       0.92      0.70      0.80       228\n",
            "          52       0.63      0.48      0.55       232\n",
            "          53       0.46      0.50      0.48       233\n",
            "          54       0.56      0.63      0.59       203\n",
            "          55       0.41      0.50      0.45       234\n",
            "          56       0.48      0.71      0.57       223\n",
            "          57       0.60      0.68      0.64       234\n",
            "          58       0.40      0.48      0.43       232\n",
            "          59       0.77      0.74      0.76       233\n",
            "          60       0.58      0.67      0.62       197\n",
            "          61       0.32      0.45      0.37       215\n",
            "          62       0.60      0.67      0.63       238\n",
            "          63       0.83      0.76      0.79       231\n",
            "          64       0.65      0.72      0.69       227\n",
            "          65       0.83      0.82      0.83       234\n",
            "          66       0.70      0.70      0.70       224\n",
            "          67       0.60      0.68      0.64       231\n",
            "          68       0.87      0.70      0.77       217\n",
            "          69       0.57      0.54      0.55       223\n",
            "          70       0.81      0.67      0.73       230\n",
            "          71       0.79      0.72      0.76       228\n",
            "          72       0.70      0.74      0.72       234\n",
            "          73       0.72      0.73      0.73       225\n",
            "          74       0.60      0.40      0.48       211\n",
            "          75       0.69      0.68      0.68       218\n",
            "          76       0.65      0.73      0.69       235\n",
            "          77       0.52      0.62      0.57       227\n",
            "          78       0.71      0.70      0.71       210\n",
            "          79       0.80      0.78      0.79       220\n",
            "          80       0.77      0.79      0.78       224\n",
            "          81       0.59      0.45      0.51       232\n",
            "          82       0.62      0.44      0.52       236\n",
            "          83       0.81      0.72      0.76       227\n",
            "          84       0.58      0.63      0.60       236\n",
            "          85       0.83      0.67      0.74       215\n",
            "          86       0.52      0.42      0.47       226\n",
            "          87       0.62      0.55      0.58       228\n",
            "          88       0.69      0.68      0.69       219\n",
            "          89       0.61      0.64      0.62       228\n",
            "          90       0.73      0.85      0.79       213\n",
            "          91       0.80      0.85      0.83       218\n",
            "          92       0.76      0.72      0.74       230\n",
            "          93       0.44      0.48      0.46       229\n",
            "          94       0.77      0.77      0.77       223\n",
            "          95       0.72      0.67      0.69       237\n",
            "          96       0.57      0.63      0.60       233\n",
            "          97       0.67      0.68      0.68       223\n",
            "          98       0.76      0.73      0.74       232\n",
            "          99       0.63      0.39      0.49       211\n",
            "         100       0.88      0.74      0.81       235\n",
            "\n",
            "    accuracy                           0.65     22716\n",
            "   macro avg       0.66      0.65      0.65     22716\n",
            "weighted avg       0.66      0.65      0.65     22716\n",
            "\n",
            "Confusion Matrix: \n",
            "[[131   1   3 ...   1   0   2]\n",
            " [  1 172   1 ...   0   0   0]\n",
            " [  3   0 191 ...   1   1   0]\n",
            " ...\n",
            " [  0   1   0 ... 169   0   0]\n",
            " [  1   1   1 ...   0  83   0]\n",
            " [  1   0   0 ...   1   0 175]]\n",
            "Training Loss==========================>>\n",
            "Epoch 8/20 Training Loss: 0.1626\n",
            "Accuracy: 0.6526237013558726\n",
            "Precision: 0.6594913475463743\n",
            "Recall: 0.6526237013558726\n",
            "F1-score: 0.6520718234786488\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.59      0.63       234\n",
            "           1       0.73      0.70      0.71       221\n",
            "           2       0.76      0.85      0.80       226\n",
            "           3       0.59      0.72      0.65       222\n",
            "           4       0.41      0.44      0.42       225\n",
            "           5       0.77      0.67      0.72       224\n",
            "           6       0.65      0.69      0.67       224\n",
            "           7       0.56      0.65      0.60       225\n",
            "           8       0.60      0.65      0.62       226\n",
            "           9       0.64      0.65      0.65       214\n",
            "          10       0.75      0.70      0.72       231\n",
            "          11       0.78      0.74      0.76       227\n",
            "          12       0.73      0.62      0.67       230\n",
            "          13       0.70      0.80      0.75       220\n",
            "          14       0.60      0.62      0.61       231\n",
            "          15       0.72      0.53      0.61       227\n",
            "          16       0.47      0.51      0.49       224\n",
            "          17       0.67      0.67      0.67       233\n",
            "          18       0.69      0.65      0.67       222\n",
            "          19       0.73      0.62      0.67       220\n",
            "          20       0.64      0.89      0.74       219\n",
            "          21       0.65      0.79      0.71       232\n",
            "          22       0.62      0.62      0.62       224\n",
            "          23       0.77      0.75      0.76       230\n",
            "          24       0.66      0.74      0.70       224\n",
            "          25       0.74      0.73      0.73       220\n",
            "          26       0.68      0.72      0.70       221\n",
            "          27       0.79      0.74      0.77       225\n",
            "          28       0.62      0.75      0.68       224\n",
            "          29       0.67      0.59      0.63       228\n",
            "          30       0.92      0.89      0.90       229\n",
            "          31       0.74      0.65      0.69       232\n",
            "          32       0.47      0.54      0.50       228\n",
            "          33       0.71      0.62      0.66       231\n",
            "          34       0.86      0.69      0.76       213\n",
            "          35       0.48      0.50      0.49       202\n",
            "          36       0.72      0.60      0.65       228\n",
            "          37       0.58      0.62      0.60       212\n",
            "          38       0.46      0.56      0.51       208\n",
            "          39       0.47      0.45      0.46       216\n",
            "          40       0.64      0.71      0.67       212\n",
            "          41       0.65      0.80      0.72       223\n",
            "          42       0.62      0.61      0.61       238\n",
            "          43       0.68      0.70      0.69       212\n",
            "          44       0.63      0.59      0.61       229\n",
            "          45       0.55      0.41      0.47       233\n",
            "          46       0.55      0.67      0.60       224\n",
            "          47       0.40      0.65      0.50       234\n",
            "          48       0.73      0.67      0.70       223\n",
            "          49       0.71      0.79      0.75       229\n",
            "          50       0.77      0.59      0.67       225\n",
            "          51       0.87      0.78      0.82       228\n",
            "          52       0.68      0.41      0.51       232\n",
            "          53       0.50      0.46      0.48       233\n",
            "          54       0.61      0.54      0.58       203\n",
            "          55       0.58      0.39      0.46       234\n",
            "          56       0.58      0.65      0.62       223\n",
            "          57       0.65      0.63      0.64       234\n",
            "          58       0.37      0.52      0.43       232\n",
            "          59       0.75      0.73      0.74       233\n",
            "          60       0.69      0.61      0.65       197\n",
            "          61       0.38      0.38      0.38       215\n",
            "          62       0.66      0.51      0.58       238\n",
            "          63       0.79      0.78      0.79       231\n",
            "          64       0.76      0.66      0.71       227\n",
            "          65       0.90      0.78      0.84       234\n",
            "          66       0.73      0.70      0.71       224\n",
            "          67       0.68      0.67      0.68       231\n",
            "          68       0.71      0.76      0.73       217\n",
            "          69       0.61      0.50      0.55       223\n",
            "          70       0.65      0.70      0.68       230\n",
            "          71       0.76      0.77      0.77       228\n",
            "          72       0.76      0.71      0.74       234\n",
            "          73       0.76      0.75      0.76       225\n",
            "          74       0.53      0.47      0.50       211\n",
            "          75       0.59      0.74      0.66       218\n",
            "          76       0.66      0.74      0.70       235\n",
            "          77       0.58      0.55      0.56       227\n",
            "          78       0.62      0.68      0.65       210\n",
            "          79       0.64      0.83      0.72       220\n",
            "          80       0.82      0.81      0.81       224\n",
            "          81       0.52      0.56      0.54       232\n",
            "          82       0.61      0.49      0.54       236\n",
            "          83       0.79      0.76      0.77       227\n",
            "          84       0.59      0.57      0.58       236\n",
            "          85       0.65      0.71      0.68       215\n",
            "          86       0.57      0.42      0.48       226\n",
            "          87       0.65      0.56      0.60       228\n",
            "          88       0.74      0.63      0.68       219\n",
            "          89       0.66      0.60      0.63       228\n",
            "          90       0.72      0.88      0.79       213\n",
            "          91       0.81      0.80      0.80       218\n",
            "          92       0.83      0.70      0.76       230\n",
            "          93       0.44      0.49      0.46       229\n",
            "          94       0.76      0.80      0.78       223\n",
            "          95       0.61      0.70      0.65       237\n",
            "          96       0.64      0.56      0.60       233\n",
            "          97       0.60      0.73      0.66       223\n",
            "          98       0.71      0.77      0.74       232\n",
            "          99       0.67      0.46      0.55       211\n",
            "         100       0.70      0.80      0.75       235\n",
            "\n",
            "    accuracy                           0.65     22716\n",
            "   macro avg       0.66      0.65      0.65     22716\n",
            "weighted avg       0.66      0.65      0.65     22716\n",
            "\n",
            "Confusion Matrix: \n",
            "[[138   0   4 ...   1   0   9]\n",
            " [  0 155   0 ...   1   0   0]\n",
            " [  2   0 192 ...   1   0   0]\n",
            " ...\n",
            " [  1   0   1 ... 178   0   2]\n",
            " [  0   1   1 ...   0  98   2]\n",
            " [  0   0   0 ...   0   0 189]]\n",
            "Training Loss==========================>>\n",
            "Epoch 9/20 Training Loss: 0.1092\n",
            "Accuracy: 0.6553090332805072\n",
            "Precision: 0.663144324543475\n",
            "Recall: 0.6553090332805072\n",
            "F1-score: 0.6540460989002264\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.50      0.60       234\n",
            "           1       0.78      0.66      0.71       221\n",
            "           2       0.79      0.84      0.82       226\n",
            "           3       0.65      0.72      0.68       222\n",
            "           4       0.50      0.40      0.45       225\n",
            "           5       0.70      0.75      0.73       224\n",
            "           6       0.68      0.65      0.67       224\n",
            "           7       0.67      0.62      0.64       225\n",
            "           8       0.60      0.66      0.63       226\n",
            "           9       0.72      0.61      0.66       214\n",
            "          10       0.78      0.63      0.70       231\n",
            "          11       0.67      0.81      0.73       227\n",
            "          12       0.61      0.67      0.64       230\n",
            "          13       0.70      0.80      0.75       220\n",
            "          14       0.68      0.65      0.67       231\n",
            "          15       0.47      0.67      0.55       227\n",
            "          16       0.58      0.50      0.54       224\n",
            "          17       0.63      0.66      0.65       233\n",
            "          18       0.74      0.59      0.66       222\n",
            "          19       0.73      0.59      0.65       220\n",
            "          20       0.84      0.84      0.84       219\n",
            "          21       0.70      0.75      0.72       232\n",
            "          22       0.54      0.67      0.59       224\n",
            "          23       0.80      0.74      0.77       230\n",
            "          24       0.71      0.70      0.71       224\n",
            "          25       0.68      0.80      0.73       220\n",
            "          26       0.58      0.74      0.65       221\n",
            "          27       0.67      0.76      0.72       225\n",
            "          28       0.63      0.75      0.69       224\n",
            "          29       0.66      0.63      0.64       228\n",
            "          30       0.84      0.92      0.88       229\n",
            "          31       0.63      0.67      0.65       232\n",
            "          32       0.55      0.46      0.50       228\n",
            "          33       0.60      0.63      0.61       231\n",
            "          34       0.75      0.78      0.76       213\n",
            "          35       0.52      0.45      0.48       202\n",
            "          36       0.70      0.57      0.62       228\n",
            "          37       0.60      0.68      0.64       212\n",
            "          38       0.53      0.48      0.50       208\n",
            "          39       0.41      0.47      0.44       216\n",
            "          40       0.74      0.66      0.70       212\n",
            "          41       0.76      0.78      0.77       223\n",
            "          42       0.57      0.62      0.59       238\n",
            "          43       0.61      0.73      0.67       212\n",
            "          44       0.55      0.75      0.63       229\n",
            "          45       0.49      0.60      0.54       233\n",
            "          46       0.61      0.65      0.63       224\n",
            "          47       0.56      0.59      0.57       234\n",
            "          48       0.75      0.71      0.73       223\n",
            "          49       0.80      0.74      0.77       229\n",
            "          50       0.78      0.64      0.70       225\n",
            "          51       0.71      0.80      0.76       228\n",
            "          52       0.66      0.47      0.55       232\n",
            "          53       0.60      0.38      0.46       233\n",
            "          54       0.69      0.52      0.59       203\n",
            "          55       0.64      0.39      0.48       234\n",
            "          56       0.59      0.68      0.63       223\n",
            "          57       0.74      0.56      0.64       234\n",
            "          58       0.47      0.34      0.40       232\n",
            "          59       0.69      0.81      0.75       233\n",
            "          60       0.39      0.73      0.51       197\n",
            "          61       0.39      0.38      0.38       215\n",
            "          62       0.75      0.55      0.63       238\n",
            "          63       0.83      0.75      0.79       231\n",
            "          64       0.70      0.70      0.70       227\n",
            "          65       0.85      0.83      0.84       234\n",
            "          66       0.60      0.80      0.69       224\n",
            "          67       0.73      0.64      0.68       231\n",
            "          68       0.83      0.77      0.80       217\n",
            "          69       0.53      0.55      0.54       223\n",
            "          70       0.61      0.76      0.68       230\n",
            "          71       0.86      0.66      0.75       228\n",
            "          72       0.79      0.71      0.75       234\n",
            "          73       0.60      0.81      0.69       225\n",
            "          74       0.39      0.54      0.45       211\n",
            "          75       0.70      0.68      0.69       218\n",
            "          76       0.74      0.69      0.72       235\n",
            "          77       0.63      0.45      0.52       227\n",
            "          78       0.66      0.69      0.68       210\n",
            "          79       0.76      0.82      0.79       220\n",
            "          80       0.77      0.80      0.79       224\n",
            "          81       0.53      0.54      0.53       232\n",
            "          82       0.56      0.48      0.52       236\n",
            "          83       0.79      0.76      0.77       227\n",
            "          84       0.67      0.47      0.55       236\n",
            "          85       0.79      0.65      0.71       215\n",
            "          86       0.42      0.51      0.46       226\n",
            "          87       0.62      0.57      0.59       228\n",
            "          88       0.66      0.72      0.69       219\n",
            "          89       0.58      0.57      0.58       228\n",
            "          90       0.74      0.86      0.80       213\n",
            "          91       0.86      0.82      0.84       218\n",
            "          92       0.70      0.76      0.73       230\n",
            "          93       0.55      0.41      0.47       229\n",
            "          94       0.73      0.78      0.75       223\n",
            "          95       0.74      0.68      0.71       237\n",
            "          96       0.63      0.63      0.63       233\n",
            "          97       0.73      0.69      0.71       223\n",
            "          98       0.67      0.80      0.73       232\n",
            "          99       0.59      0.55      0.57       211\n",
            "         100       0.87      0.77      0.81       235\n",
            "\n",
            "    accuracy                           0.66     22716\n",
            "   macro avg       0.66      0.66      0.65     22716\n",
            "weighted avg       0.66      0.66      0.65     22716\n",
            "\n",
            "Confusion Matrix: \n",
            "[[118   0   7 ...   3   1   4]\n",
            " [  0 145   0 ...   1   1   0]\n",
            " [  2   1 190 ...   1   2   0]\n",
            " ...\n",
            " [  0   1   0 ... 185   0   0]\n",
            " [  0   1   0 ...   0 115   0]\n",
            " [  0   0   0 ...   3   1 180]]\n",
            "Training Loss==========================>>\n",
            "Epoch 10/20 Training Loss: 0.0820\n",
            "Accuracy: 0.668031343546399\n",
            "Precision: 0.6716936990418972\n",
            "Recall: 0.668031343546399\n",
            "F1-score: 0.666814965297738\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.56      0.62       234\n",
            "           1       0.77      0.72      0.74       221\n",
            "           2       0.79      0.83      0.81       226\n",
            "           3       0.67      0.73      0.70       222\n",
            "           4       0.58      0.34      0.43       225\n",
            "           5       0.80      0.75      0.77       224\n",
            "           6       0.76      0.62      0.68       224\n",
            "           7       0.64      0.60      0.62       225\n",
            "           8       0.46      0.70      0.56       226\n",
            "           9       0.68      0.58      0.63       214\n",
            "          10       0.69      0.73      0.71       231\n",
            "          11       0.70      0.83      0.76       227\n",
            "          12       0.73      0.63      0.68       230\n",
            "          13       0.69      0.82      0.75       220\n",
            "          14       0.71      0.62      0.67       231\n",
            "          15       0.59      0.64      0.62       227\n",
            "          16       0.60      0.52      0.56       224\n",
            "          17       0.73      0.66      0.70       233\n",
            "          18       0.52      0.72      0.61       222\n",
            "          19       0.67      0.68      0.67       220\n",
            "          20       0.74      0.87      0.80       219\n",
            "          21       0.73      0.76      0.74       232\n",
            "          22       0.68      0.62      0.65       224\n",
            "          23       0.76      0.75      0.76       230\n",
            "          24       0.69      0.74      0.72       224\n",
            "          25       0.73      0.80      0.76       220\n",
            "          26       0.59      0.78      0.67       221\n",
            "          27       0.72      0.76      0.74       225\n",
            "          28       0.67      0.76      0.71       224\n",
            "          29       0.68      0.66      0.67       228\n",
            "          30       0.88      0.94      0.91       229\n",
            "          31       0.63      0.72      0.67       232\n",
            "          32       0.63      0.49      0.55       228\n",
            "          33       0.60      0.67      0.64       231\n",
            "          34       0.81      0.78      0.79       213\n",
            "          35       0.67      0.42      0.51       202\n",
            "          36       0.70      0.63      0.66       228\n",
            "          37       0.60      0.64      0.62       212\n",
            "          38       0.58      0.58      0.58       208\n",
            "          39       0.47      0.47      0.47       216\n",
            "          40       0.73      0.67      0.70       212\n",
            "          41       0.66      0.79      0.72       223\n",
            "          42       0.57      0.65      0.61       238\n",
            "          43       0.80      0.68      0.73       212\n",
            "          44       0.61      0.67      0.64       229\n",
            "          45       0.53      0.54      0.54       233\n",
            "          46       0.66      0.65      0.65       224\n",
            "          47       0.58      0.55      0.57       234\n",
            "          48       0.72      0.72      0.72       223\n",
            "          49       0.75      0.79      0.77       229\n",
            "          50       0.78      0.67      0.72       225\n",
            "          51       0.82      0.79      0.80       228\n",
            "          52       0.61      0.51      0.55       232\n",
            "          53       0.57      0.46      0.51       233\n",
            "          54       0.53      0.66      0.59       203\n",
            "          55       0.49      0.44      0.46       234\n",
            "          56       0.62      0.67      0.64       223\n",
            "          57       0.67      0.65      0.66       234\n",
            "          58       0.45      0.41      0.43       232\n",
            "          59       0.73      0.77      0.75       233\n",
            "          60       0.58      0.67      0.62       197\n",
            "          61       0.39      0.40      0.39       215\n",
            "          62       0.58      0.61      0.59       238\n",
            "          63       0.82      0.80      0.81       231\n",
            "          64       0.68      0.70      0.69       227\n",
            "          65       0.84      0.82      0.83       234\n",
            "          66       0.64      0.75      0.69       224\n",
            "          67       0.57      0.76      0.65       231\n",
            "          68       0.82      0.71      0.77       217\n",
            "          69       0.53      0.57      0.55       223\n",
            "          70       0.67      0.71      0.69       230\n",
            "          71       0.74      0.73      0.73       228\n",
            "          72       0.72      0.74      0.73       234\n",
            "          73       0.78      0.77      0.78       225\n",
            "          74       0.56      0.48      0.52       211\n",
            "          75       0.70      0.65      0.67       218\n",
            "          76       0.66      0.75      0.70       235\n",
            "          77       0.57      0.56      0.57       227\n",
            "          78       0.67      0.71      0.69       210\n",
            "          79       0.92      0.71      0.80       220\n",
            "          80       0.79      0.77      0.78       224\n",
            "          81       0.56      0.53      0.55       232\n",
            "          82       0.59      0.50      0.54       236\n",
            "          83       0.80      0.77      0.78       227\n",
            "          84       0.56      0.64      0.59       236\n",
            "          85       0.81      0.64      0.71       215\n",
            "          86       0.51      0.49      0.50       226\n",
            "          87       0.65      0.57      0.61       228\n",
            "          88       0.77      0.71      0.74       219\n",
            "          89       0.66      0.63      0.65       228\n",
            "          90       0.80      0.86      0.83       213\n",
            "          91       0.78      0.86      0.82       218\n",
            "          92       0.74      0.76      0.75       230\n",
            "          93       0.50      0.50      0.50       229\n",
            "          94       0.81      0.79      0.80       223\n",
            "          95       0.74      0.67      0.70       237\n",
            "          96       0.62      0.64      0.63       233\n",
            "          97       0.62      0.71      0.66       223\n",
            "          98       0.75      0.75      0.75       232\n",
            "          99       0.56      0.55      0.55       211\n",
            "         100       0.85      0.72      0.78       235\n",
            "\n",
            "    accuracy                           0.67     22716\n",
            "   macro avg       0.67      0.67      0.67     22716\n",
            "weighted avg       0.67      0.67      0.67     22716\n",
            "\n",
            "Confusion Matrix: \n",
            "[[130   0   6 ...   2   0   1]\n",
            " [  0 160   0 ...   1   0   0]\n",
            " [  3   0 187 ...   1   0   0]\n",
            " ...\n",
            " [  1   1   1 ... 175   0   0]\n",
            " [  0   0   0 ...   0 115   0]\n",
            " [  0   0   1 ...   1   0 170]]\n",
            "Training Loss==========================>>\n",
            "Epoch 11/20 Training Loss: 0.0577\n",
            "Accuracy: 0.6715530903328051\n",
            "Precision: 0.6759275715925285\n",
            "Recall: 0.6715530903328051\n",
            "F1-score: 0.6702908292353164\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.58      0.63       234\n",
            "           1       0.79      0.70      0.74       221\n",
            "           2       0.82      0.82      0.82       226\n",
            "           3       0.63      0.74      0.68       222\n",
            "           4       0.47      0.44      0.45       225\n",
            "           5       0.66      0.77      0.71       224\n",
            "           6       0.71      0.70      0.70       224\n",
            "           7       0.59      0.68      0.63       225\n",
            "           8       0.67      0.58      0.62       226\n",
            "           9       0.74      0.57      0.65       214\n",
            "          10       0.67      0.71      0.69       231\n",
            "          11       0.68      0.84      0.75       227\n",
            "          12       0.74      0.63      0.68       230\n",
            "          13       0.68      0.81      0.74       220\n",
            "          14       0.78      0.63      0.70       231\n",
            "          15       0.56      0.67      0.61       227\n",
            "          16       0.57      0.54      0.55       224\n",
            "          17       0.66      0.72      0.69       233\n",
            "          18       0.66      0.70      0.68       222\n",
            "          19       0.67      0.65      0.66       220\n",
            "          20       0.77      0.88      0.82       219\n",
            "          21       0.78      0.72      0.74       232\n",
            "          22       0.60      0.63      0.61       224\n",
            "          23       0.82      0.78      0.80       230\n",
            "          24       0.69      0.75      0.72       224\n",
            "          25       0.69      0.80      0.74       220\n",
            "          26       0.62      0.76      0.68       221\n",
            "          27       0.71      0.79      0.75       225\n",
            "          28       0.76      0.74      0.75       224\n",
            "          29       0.66      0.68      0.67       228\n",
            "          30       0.86      0.91      0.88       229\n",
            "          31       0.71      0.67      0.69       232\n",
            "          32       0.60      0.41      0.49       228\n",
            "          33       0.82      0.57      0.67       231\n",
            "          34       0.79      0.78      0.79       213\n",
            "          35       0.59      0.47      0.52       202\n",
            "          36       0.71      0.57      0.63       228\n",
            "          37       0.56      0.71      0.63       212\n",
            "          38       0.52      0.56      0.54       208\n",
            "          39       0.49      0.44      0.46       216\n",
            "          40       0.67      0.71      0.69       212\n",
            "          41       0.76      0.76      0.76       223\n",
            "          42       0.69      0.56      0.62       238\n",
            "          43       0.82      0.66      0.73       212\n",
            "          44       0.63      0.72      0.67       229\n",
            "          45       0.52      0.59      0.55       233\n",
            "          46       0.62      0.64      0.63       224\n",
            "          47       0.67      0.56      0.61       234\n",
            "          48       0.60      0.78      0.68       223\n",
            "          49       0.74      0.78      0.76       229\n",
            "          50       0.76      0.69      0.73       225\n",
            "          51       0.85      0.76      0.80       228\n",
            "          52       0.47      0.59      0.52       232\n",
            "          53       0.55      0.41      0.47       233\n",
            "          54       0.61      0.62      0.61       203\n",
            "          55       0.60      0.46      0.52       234\n",
            "          56       0.69      0.65      0.67       223\n",
            "          57       0.66      0.65      0.65       234\n",
            "          58       0.49      0.39      0.43       232\n",
            "          59       0.68      0.81      0.74       233\n",
            "          60       0.74      0.63      0.68       197\n",
            "          61       0.42      0.44      0.43       215\n",
            "          62       0.58      0.70      0.63       238\n",
            "          63       0.90      0.77      0.83       231\n",
            "          64       0.70      0.70      0.70       227\n",
            "          65       0.86      0.82      0.84       234\n",
            "          66       0.73      0.67      0.70       224\n",
            "          67       0.63      0.74      0.68       231\n",
            "          68       0.84      0.76      0.80       217\n",
            "          69       0.63      0.52      0.57       223\n",
            "          70       0.65      0.73      0.68       230\n",
            "          71       0.81      0.70      0.75       228\n",
            "          72       0.67      0.76      0.71       234\n",
            "          73       0.62      0.78      0.69       225\n",
            "          74       0.50      0.54      0.52       211\n",
            "          75       0.67      0.72      0.69       218\n",
            "          76       0.61      0.74      0.67       235\n",
            "          77       0.59      0.52      0.55       227\n",
            "          78       0.75      0.68      0.71       210\n",
            "          79       0.80      0.80      0.80       220\n",
            "          80       0.75      0.83      0.79       224\n",
            "          81       0.55      0.56      0.56       232\n",
            "          82       0.45      0.61      0.51       236\n",
            "          83       0.80      0.73      0.76       227\n",
            "          84       0.54      0.65      0.59       236\n",
            "          85       0.70      0.66      0.68       215\n",
            "          86       0.51      0.48      0.49       226\n",
            "          87       0.62      0.61      0.61       228\n",
            "          88       0.64      0.75      0.69       219\n",
            "          89       0.66      0.61      0.63       228\n",
            "          90       0.78      0.88      0.83       213\n",
            "          91       0.80      0.89      0.84       218\n",
            "          92       0.77      0.75      0.76       230\n",
            "          93       0.56      0.44      0.49       229\n",
            "          94       0.80      0.76      0.78       223\n",
            "          95       0.72      0.68      0.70       237\n",
            "          96       0.65      0.60      0.63       233\n",
            "          97       0.72      0.70      0.71       223\n",
            "          98       0.80      0.77      0.79       232\n",
            "          99       0.62      0.53      0.57       211\n",
            "         100       0.84      0.77      0.80       235\n",
            "\n",
            "    accuracy                           0.67     22716\n",
            "   macro avg       0.68      0.67      0.67     22716\n",
            "weighted avg       0.68      0.67      0.67     22716\n",
            "\n",
            "Confusion Matrix: \n",
            "[[135   0   2 ...   1   0   4]\n",
            " [  0 154   1 ...   1   0   0]\n",
            " [  3   0 186 ...   1   2   0]\n",
            " ...\n",
            " [  0   1   0 ... 179   0   0]\n",
            " [  0   0   0 ...   0 112   2]\n",
            " [  0   0   0 ...   2   0 180]]\n",
            "Training Loss==========================>>\n",
            "Epoch 12/20 Training Loss: 0.0425\n",
            "Accuracy: 0.676175382989963\n",
            "Precision: 0.6777021741387006\n",
            "Recall: 0.676175382989963\n",
            "F1-score: 0.6750364418407935\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.70      0.65       234\n",
            "           1       0.76      0.71      0.74       221\n",
            "           2       0.83      0.83      0.83       226\n",
            "           3       0.74      0.67      0.70       222\n",
            "           4       0.51      0.44      0.48       225\n",
            "           5       0.73      0.77      0.75       224\n",
            "           6       0.67      0.69      0.68       224\n",
            "           7       0.72      0.62      0.67       225\n",
            "           8       0.57      0.67      0.62       226\n",
            "           9       0.68      0.62      0.65       214\n",
            "          10       0.67      0.73      0.70       231\n",
            "          11       0.76      0.78      0.77       227\n",
            "          12       0.68      0.69      0.68       230\n",
            "          13       0.77      0.79      0.78       220\n",
            "          14       0.65      0.67      0.66       231\n",
            "          15       0.65      0.61      0.63       227\n",
            "          16       0.58      0.52      0.55       224\n",
            "          17       0.71      0.69      0.70       233\n",
            "          18       0.54      0.70      0.61       222\n",
            "          19       0.65      0.67      0.66       220\n",
            "          20       0.74      0.90      0.81       219\n",
            "          21       0.70      0.76      0.73       232\n",
            "          22       0.68      0.59      0.63       224\n",
            "          23       0.80      0.76      0.78       230\n",
            "          24       0.67      0.77      0.71       224\n",
            "          25       0.66      0.82      0.73       220\n",
            "          26       0.69      0.73      0.71       221\n",
            "          27       0.73      0.79      0.76       225\n",
            "          28       0.78      0.73      0.75       224\n",
            "          29       0.62      0.67      0.64       228\n",
            "          30       0.92      0.91      0.92       229\n",
            "          31       0.70      0.68      0.69       232\n",
            "          32       0.50      0.52      0.51       228\n",
            "          33       0.71      0.68      0.70       231\n",
            "          34       0.77      0.79      0.78       213\n",
            "          35       0.62      0.45      0.52       202\n",
            "          36       0.69      0.62      0.65       228\n",
            "          37       0.69      0.64      0.66       212\n",
            "          38       0.49      0.62      0.55       208\n",
            "          39       0.48      0.43      0.45       216\n",
            "          40       0.74      0.73      0.73       212\n",
            "          41       0.81      0.73      0.77       223\n",
            "          42       0.67      0.59      0.63       238\n",
            "          43       0.72      0.71      0.71       212\n",
            "          44       0.67      0.70      0.68       229\n",
            "          45       0.54      0.54      0.54       233\n",
            "          46       0.64      0.62      0.63       224\n",
            "          47       0.67      0.52      0.59       234\n",
            "          48       0.62      0.80      0.70       223\n",
            "          49       0.74      0.76      0.75       229\n",
            "          50       0.75      0.75      0.75       225\n",
            "          51       0.78      0.81      0.80       228\n",
            "          52       0.55      0.59      0.57       232\n",
            "          53       0.48      0.49      0.49       233\n",
            "          54       0.52      0.65      0.58       203\n",
            "          55       0.47      0.47      0.47       234\n",
            "          56       0.68      0.63      0.65       223\n",
            "          57       0.72      0.63      0.67       234\n",
            "          58       0.42      0.48      0.45       232\n",
            "          59       0.78      0.78      0.78       233\n",
            "          60       0.65      0.65      0.65       197\n",
            "          61       0.45      0.38      0.41       215\n",
            "          62       0.68      0.65      0.66       238\n",
            "          63       0.89      0.75      0.81       231\n",
            "          64       0.69      0.70      0.70       227\n",
            "          65       0.88      0.82      0.85       234\n",
            "          66       0.67      0.75      0.71       224\n",
            "          67       0.70      0.71      0.71       231\n",
            "          68       0.69      0.78      0.73       217\n",
            "          69       0.62      0.53      0.57       223\n",
            "          70       0.65      0.77      0.70       230\n",
            "          71       0.74      0.77      0.75       228\n",
            "          72       0.73      0.79      0.76       234\n",
            "          73       0.77      0.75      0.76       225\n",
            "          74       0.52      0.51      0.52       211\n",
            "          75       0.69      0.74      0.71       218\n",
            "          76       0.67      0.77      0.72       235\n",
            "          77       0.63      0.51      0.56       227\n",
            "          78       0.67      0.69      0.68       210\n",
            "          79       0.78      0.81      0.80       220\n",
            "          80       0.81      0.79      0.80       224\n",
            "          81       0.63      0.46      0.53       232\n",
            "          82       0.61      0.54      0.57       236\n",
            "          83       0.80      0.74      0.77       227\n",
            "          84       0.65      0.58      0.61       236\n",
            "          85       0.65      0.71      0.68       215\n",
            "          86       0.54      0.50      0.52       226\n",
            "          87       0.67      0.58      0.62       228\n",
            "          88       0.76      0.67      0.71       219\n",
            "          89       0.64      0.64      0.64       228\n",
            "          90       0.79      0.86      0.82       213\n",
            "          91       0.86      0.85      0.86       218\n",
            "          92       0.73      0.77      0.75       230\n",
            "          93       0.52      0.48      0.50       229\n",
            "          94       0.76      0.81      0.78       223\n",
            "          95       0.72      0.70      0.71       237\n",
            "          96       0.59      0.59      0.59       233\n",
            "          97       0.71      0.73      0.72       223\n",
            "          98       0.79      0.78      0.78       232\n",
            "          99       0.53      0.56      0.55       211\n",
            "         100       0.82      0.77      0.80       235\n",
            "\n",
            "    accuracy                           0.68     22716\n",
            "   macro avg       0.68      0.68      0.67     22716\n",
            "weighted avg       0.68      0.68      0.68     22716\n",
            "\n",
            "Confusion Matrix: \n",
            "[[163   0   4 ...   2   0   3]\n",
            " [  0 158   1 ...   0   0   0]\n",
            " [  6   0 188 ...   1   0   0]\n",
            " ...\n",
            " [  0   1   1 ... 180   0   0]\n",
            " [  0   1   0 ...   0 119   0]\n",
            " [  2   0   0 ...   2   0 181]]\n",
            "Training Loss==========================>>\n",
            "Epoch 13/20 Training Loss: 0.0331\n",
            "Accuracy: 0.6809737629864413\n",
            "Precision: 0.683960067218982\n",
            "Recall: 0.6809737629864413\n",
            "F1-score: 0.6800959083607575\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.61      0.65       234\n",
            "           1       0.70      0.73      0.72       221\n",
            "           2       0.75      0.84      0.79       226\n",
            "           3       0.69      0.72      0.71       222\n",
            "           4       0.57      0.45      0.50       225\n",
            "           5       0.80      0.73      0.76       224\n",
            "           6       0.60      0.72      0.66       224\n",
            "           7       0.61      0.64      0.63       225\n",
            "           8       0.54      0.70      0.61       226\n",
            "           9       0.67      0.62      0.64       214\n",
            "          10       0.69      0.73      0.71       231\n",
            "          11       0.78      0.78      0.78       227\n",
            "          12       0.79      0.61      0.69       230\n",
            "          13       0.72      0.82      0.76       220\n",
            "          14       0.67      0.63      0.65       231\n",
            "          15       0.67      0.65      0.66       227\n",
            "          16       0.64      0.55      0.59       224\n",
            "          17       0.67      0.69      0.68       233\n",
            "          18       0.69      0.66      0.67       222\n",
            "          19       0.70      0.66      0.68       220\n",
            "          20       0.76      0.88      0.81       219\n",
            "          21       0.62      0.82      0.71       232\n",
            "          22       0.57      0.65      0.61       224\n",
            "          23       0.77      0.78      0.77       230\n",
            "          24       0.70      0.72      0.71       224\n",
            "          25       0.68      0.80      0.73       220\n",
            "          26       0.70      0.75      0.72       221\n",
            "          27       0.78      0.76      0.77       225\n",
            "          28       0.73      0.74      0.73       224\n",
            "          29       0.63      0.68      0.65       228\n",
            "          30       0.89      0.92      0.91       229\n",
            "          31       0.70      0.69      0.69       232\n",
            "          32       0.52      0.48      0.50       228\n",
            "          33       0.68      0.63      0.65       231\n",
            "          34       0.82      0.77      0.80       213\n",
            "          35       0.63      0.46      0.53       202\n",
            "          36       0.72      0.64      0.68       228\n",
            "          37       0.65      0.64      0.64       212\n",
            "          38       0.58      0.58      0.58       208\n",
            "          39       0.45      0.51      0.48       216\n",
            "          40       0.72      0.73      0.73       212\n",
            "          41       0.75      0.81      0.78       223\n",
            "          42       0.66      0.61      0.63       238\n",
            "          43       0.82      0.69      0.75       212\n",
            "          44       0.56      0.76      0.65       229\n",
            "          45       0.52      0.58      0.55       233\n",
            "          46       0.69      0.65      0.67       224\n",
            "          47       0.59      0.61      0.60       234\n",
            "          48       0.70      0.77      0.73       223\n",
            "          49       0.79      0.78      0.79       229\n",
            "          50       0.76      0.71      0.73       225\n",
            "          51       0.75      0.82      0.79       228\n",
            "          52       0.53      0.58      0.55       232\n",
            "          53       0.52      0.46      0.49       233\n",
            "          54       0.63      0.59      0.61       203\n",
            "          55       0.55      0.45      0.50       234\n",
            "          56       0.70      0.62      0.66       223\n",
            "          57       0.58      0.71      0.64       234\n",
            "          58       0.46      0.44      0.45       232\n",
            "          59       0.74      0.83      0.78       233\n",
            "          60       0.66      0.61      0.64       197\n",
            "          61       0.42      0.46      0.44       215\n",
            "          62       0.66      0.68      0.67       238\n",
            "          63       0.79      0.81      0.80       231\n",
            "          64       0.69      0.72      0.70       227\n",
            "          65       0.89      0.83      0.86       234\n",
            "          66       0.66      0.77      0.71       224\n",
            "          67       0.68      0.71      0.70       231\n",
            "          68       0.86      0.75      0.80       217\n",
            "          69       0.68      0.54      0.60       223\n",
            "          70       0.72      0.71      0.72       230\n",
            "          71       0.83      0.70      0.76       228\n",
            "          72       0.75      0.75      0.75       234\n",
            "          73       0.73      0.78      0.75       225\n",
            "          74       0.63      0.49      0.55       211\n",
            "          75       0.70      0.73      0.72       218\n",
            "          76       0.66      0.74      0.69       235\n",
            "          77       0.56      0.67      0.61       227\n",
            "          78       0.69      0.70      0.69       210\n",
            "          79       0.85      0.77      0.81       220\n",
            "          80       0.81      0.82      0.81       224\n",
            "          81       0.61      0.54      0.57       232\n",
            "          82       0.65      0.50      0.56       236\n",
            "          83       0.78      0.73      0.75       227\n",
            "          84       0.56      0.66      0.61       236\n",
            "          85       0.80      0.67      0.73       215\n",
            "          86       0.59      0.47      0.52       226\n",
            "          87       0.66      0.62      0.64       228\n",
            "          88       0.72      0.73      0.72       219\n",
            "          89       0.68      0.64      0.66       228\n",
            "          90       0.85      0.85      0.85       213\n",
            "          91       0.85      0.84      0.85       218\n",
            "          92       0.79      0.72      0.76       230\n",
            "          93       0.47      0.50      0.49       229\n",
            "          94       0.72      0.81      0.77       223\n",
            "          95       0.71      0.71      0.71       237\n",
            "          96       0.69      0.57      0.62       233\n",
            "          97       0.66      0.72      0.69       223\n",
            "          98       0.72      0.79      0.75       232\n",
            "          99       0.58      0.56      0.57       211\n",
            "         100       0.84      0.76      0.80       235\n",
            "\n",
            "    accuracy                           0.68     22716\n",
            "   macro avg       0.68      0.68      0.68     22716\n",
            "weighted avg       0.68      0.68      0.68     22716\n",
            "\n",
            "Confusion Matrix: \n",
            "[[143   0   7 ...   4   0   3]\n",
            " [  0 162   0 ...   2   0   0]\n",
            " [  2   0 189 ...   1   2   0]\n",
            " ...\n",
            " [  1   0   0 ... 183   0   1]\n",
            " [  0   0   0 ...   0 119   0]\n",
            " [  1   0   0 ...   1   0 179]]\n",
            "Training Loss==========================>>\n",
            "Epoch 14/20 Training Loss: 0.0282\n",
            "Accuracy: 0.6783324528966367\n",
            "Precision: 0.6791286797686173\n",
            "Recall: 0.6783324528966367\n",
            "F1-score: 0.6764174167138357\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.65      0.65       234\n",
            "           1       0.76      0.72      0.74       221\n",
            "           2       0.80      0.85      0.82       226\n",
            "           3       0.73      0.73      0.73       222\n",
            "           4       0.52      0.43      0.47       225\n",
            "           5       0.69      0.76      0.73       224\n",
            "           6       0.65      0.71      0.68       224\n",
            "           7       0.65      0.66      0.65       225\n",
            "           8       0.58      0.68      0.62       226\n",
            "           9       0.63      0.65      0.64       214\n",
            "          10       0.72      0.72      0.72       231\n",
            "          11       0.73      0.82      0.77       227\n",
            "          12       0.71      0.65      0.68       230\n",
            "          13       0.74      0.79      0.77       220\n",
            "          14       0.68      0.66      0.67       231\n",
            "          15       0.59      0.66      0.62       227\n",
            "          16       0.57      0.55      0.56       224\n",
            "          17       0.77      0.59      0.67       233\n",
            "          18       0.60      0.69      0.64       222\n",
            "          19       0.62      0.71      0.66       220\n",
            "          20       0.68      0.89      0.77       219\n",
            "          21       0.68      0.72      0.70       232\n",
            "          22       0.49      0.71      0.58       224\n",
            "          23       0.76      0.80      0.78       230\n",
            "          24       0.69      0.75      0.72       224\n",
            "          25       0.72      0.81      0.76       220\n",
            "          26       0.69      0.72      0.71       221\n",
            "          27       0.74      0.78      0.76       225\n",
            "          28       0.74      0.76      0.75       224\n",
            "          29       0.61      0.69      0.65       228\n",
            "          30       0.91      0.93      0.92       229\n",
            "          31       0.71      0.67      0.69       232\n",
            "          32       0.56      0.51      0.53       228\n",
            "          33       0.77      0.61      0.68       231\n",
            "          34       0.81      0.76      0.78       213\n",
            "          35       0.56      0.47      0.51       202\n",
            "          36       0.64      0.65      0.64       228\n",
            "          37       0.58      0.70      0.64       212\n",
            "          38       0.56      0.62      0.58       208\n",
            "          39       0.51      0.44      0.47       216\n",
            "          40       0.71      0.73      0.72       212\n",
            "          41       0.81      0.78      0.79       223\n",
            "          42       0.66      0.61      0.63       238\n",
            "          43       0.81      0.66      0.73       212\n",
            "          44       0.63      0.71      0.67       229\n",
            "          45       0.55      0.54      0.54       233\n",
            "          46       0.66      0.62      0.64       224\n",
            "          47       0.70      0.51      0.59       234\n",
            "          48       0.72      0.75      0.73       223\n",
            "          49       0.77      0.79      0.78       229\n",
            "          50       0.73      0.72      0.72       225\n",
            "          51       0.79      0.81      0.80       228\n",
            "          52       0.62      0.53      0.57       232\n",
            "          53       0.51      0.50      0.50       233\n",
            "          54       0.48      0.69      0.57       203\n",
            "          55       0.48      0.45      0.47       234\n",
            "          56       0.70      0.62      0.66       223\n",
            "          57       0.65      0.68      0.67       234\n",
            "          58       0.47      0.43      0.45       232\n",
            "          59       0.70      0.84      0.76       233\n",
            "          60       0.68      0.68      0.68       197\n",
            "          61       0.43      0.36      0.39       215\n",
            "          62       0.71      0.64      0.68       238\n",
            "          63       0.83      0.80      0.82       231\n",
            "          64       0.72      0.68      0.70       227\n",
            "          65       0.82      0.84      0.83       234\n",
            "          66       0.68      0.75      0.71       224\n",
            "          67       0.65      0.70      0.68       231\n",
            "          68       0.78      0.77      0.78       217\n",
            "          69       0.65      0.56      0.60       223\n",
            "          70       0.74      0.73      0.73       230\n",
            "          71       0.75      0.78      0.76       228\n",
            "          72       0.76      0.73      0.74       234\n",
            "          73       0.74      0.75      0.74       225\n",
            "          74       0.53      0.53      0.53       211\n",
            "          75       0.70      0.69      0.69       218\n",
            "          76       0.66      0.74      0.70       235\n",
            "          77       0.63      0.56      0.59       227\n",
            "          78       0.68      0.70      0.69       210\n",
            "          79       0.80      0.78      0.79       220\n",
            "          80       0.79      0.81      0.80       224\n",
            "          81       0.64      0.45      0.53       232\n",
            "          82       0.66      0.50      0.57       236\n",
            "          83       0.76      0.76      0.76       227\n",
            "          84       0.65      0.61      0.63       236\n",
            "          85       0.69      0.71      0.70       215\n",
            "          86       0.54      0.45      0.49       226\n",
            "          87       0.69      0.58      0.63       228\n",
            "          88       0.70      0.70      0.70       219\n",
            "          89       0.60      0.66      0.63       228\n",
            "          90       0.75      0.89      0.81       213\n",
            "          91       0.88      0.83      0.85       218\n",
            "          92       0.79      0.73      0.76       230\n",
            "          93       0.53      0.45      0.49       229\n",
            "          94       0.76      0.82      0.79       223\n",
            "          95       0.73      0.70      0.72       237\n",
            "          96       0.61      0.62      0.61       233\n",
            "          97       0.74      0.72      0.73       223\n",
            "          98       0.79      0.75      0.77       232\n",
            "          99       0.58      0.57      0.57       211\n",
            "         100       0.79      0.83      0.81       235\n",
            "\n",
            "    accuracy                           0.68     22716\n",
            "   macro avg       0.68      0.68      0.68     22716\n",
            "weighted avg       0.68      0.68      0.68     22716\n",
            "\n",
            "Confusion Matrix: \n",
            "[[152   0   4 ...   2   0   5]\n",
            " [  0 159   0 ...   1   1   0]\n",
            " [  4   0 192 ...   1   0   0]\n",
            " ...\n",
            " [  1   1   0 ... 175   0   0]\n",
            " [  0   0   1 ...   0 120   1]\n",
            " [  0   0   0 ...   0   0 195]]\n",
            "Training Loss==========================>>\n",
            "Epoch 15/20 Training Loss: 0.0231\n",
            "Accuracy: 0.6830427892234548\n",
            "Precision: 0.6841555762804199\n",
            "Recall: 0.6830427892234548\n",
            "F1-score: 0.6811669501025435\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.65      0.66       234\n",
            "           1       0.68      0.77      0.72       221\n",
            "           2       0.81      0.86      0.83       226\n",
            "           3       0.66      0.72      0.69       222\n",
            "           4       0.47      0.47      0.47       225\n",
            "           5       0.78      0.76      0.77       224\n",
            "           6       0.65      0.70      0.67       224\n",
            "           7       0.59      0.67      0.63       225\n",
            "           8       0.59      0.64      0.61       226\n",
            "           9       0.75      0.61      0.67       214\n",
            "          10       0.60      0.78      0.68       231\n",
            "          11       0.76      0.80      0.78       227\n",
            "          12       0.73      0.65      0.69       230\n",
            "          13       0.74      0.80      0.77       220\n",
            "          14       0.74      0.61      0.67       231\n",
            "          15       0.64      0.62      0.63       227\n",
            "          16       0.59      0.54      0.56       224\n",
            "          17       0.69      0.70      0.69       233\n",
            "          18       0.66      0.70      0.68       222\n",
            "          19       0.69      0.70      0.70       220\n",
            "          20       0.81      0.86      0.83       219\n",
            "          21       0.71      0.75      0.73       232\n",
            "          22       0.63      0.68      0.65       224\n",
            "          23       0.77      0.77      0.77       230\n",
            "          24       0.70      0.72      0.71       224\n",
            "          25       0.67      0.83      0.74       220\n",
            "          26       0.66      0.74      0.70       221\n",
            "          27       0.74      0.77      0.75       225\n",
            "          28       0.79      0.73      0.76       224\n",
            "          29       0.56      0.70      0.62       228\n",
            "          30       0.91      0.93      0.92       229\n",
            "          31       0.67      0.70      0.68       232\n",
            "          32       0.52      0.49      0.50       228\n",
            "          33       0.72      0.65      0.68       231\n",
            "          34       0.75      0.78      0.77       213\n",
            "          35       0.53      0.50      0.52       202\n",
            "          36       0.67      0.64      0.66       228\n",
            "          37       0.82      0.62      0.70       212\n",
            "          38       0.53      0.65      0.58       208\n",
            "          39       0.55      0.41      0.47       216\n",
            "          40       0.76      0.72      0.74       212\n",
            "          41       0.73      0.78      0.76       223\n",
            "          42       0.69      0.58      0.63       238\n",
            "          43       0.81      0.70      0.75       212\n",
            "          44       0.63      0.73      0.68       229\n",
            "          45       0.55      0.55      0.55       233\n",
            "          46       0.74      0.57      0.64       224\n",
            "          47       0.68      0.56      0.61       234\n",
            "          48       0.75      0.77      0.76       223\n",
            "          49       0.79      0.76      0.78       229\n",
            "          50       0.80      0.70      0.75       225\n",
            "          51       0.82      0.80      0.81       228\n",
            "          52       0.56      0.56      0.56       232\n",
            "          53       0.51      0.45      0.48       233\n",
            "          54       0.59      0.63      0.61       203\n",
            "          55       0.50      0.45      0.48       234\n",
            "          56       0.73      0.62      0.67       223\n",
            "          57       0.63      0.69      0.66       234\n",
            "          58       0.52      0.41      0.46       232\n",
            "          59       0.78      0.82      0.80       233\n",
            "          60       0.66      0.66      0.66       197\n",
            "          61       0.42      0.39      0.40       215\n",
            "          62       0.63      0.68      0.66       238\n",
            "          63       0.79      0.78      0.79       231\n",
            "          64       0.69      0.75      0.72       227\n",
            "          65       0.86      0.82      0.84       234\n",
            "          66       0.69      0.77      0.73       224\n",
            "          67       0.68      0.68      0.68       231\n",
            "          68       0.83      0.76      0.79       217\n",
            "          69       0.68      0.55      0.61       223\n",
            "          70       0.73      0.75      0.74       230\n",
            "          71       0.78      0.76      0.77       228\n",
            "          72       0.69      0.77      0.73       234\n",
            "          73       0.71      0.79      0.75       225\n",
            "          74       0.54      0.49      0.51       211\n",
            "          75       0.69      0.71      0.70       218\n",
            "          76       0.59      0.80      0.68       235\n",
            "          77       0.60      0.53      0.57       227\n",
            "          78       0.62      0.72      0.67       210\n",
            "          79       0.80      0.80      0.80       220\n",
            "          80       0.71      0.84      0.77       224\n",
            "          81       0.60      0.54      0.57       232\n",
            "          82       0.69      0.48      0.57       236\n",
            "          83       0.78      0.77      0.77       227\n",
            "          84       0.67      0.61      0.64       236\n",
            "          85       0.70      0.73      0.71       215\n",
            "          86       0.56      0.45      0.50       226\n",
            "          87       0.69      0.60      0.64       228\n",
            "          88       0.79      0.70      0.74       219\n",
            "          89       0.70      0.64      0.67       228\n",
            "          90       0.80      0.86      0.83       213\n",
            "          91       0.82      0.88      0.85       218\n",
            "          92       0.80      0.73      0.76       230\n",
            "          93       0.49      0.52      0.51       229\n",
            "          94       0.79      0.80      0.79       223\n",
            "          95       0.67      0.73      0.69       237\n",
            "          96       0.62      0.63      0.63       233\n",
            "          97       0.68      0.72      0.70       223\n",
            "          98       0.71      0.83      0.76       232\n",
            "          99       0.56      0.59      0.57       211\n",
            "         100       0.83      0.80      0.82       235\n",
            "\n",
            "    accuracy                           0.68     22716\n",
            "   macro avg       0.68      0.68      0.68     22716\n",
            "weighted avg       0.68      0.68      0.68     22716\n",
            "\n",
            "Confusion Matrix: \n",
            "[[153   0   4 ...   3   0   3]\n",
            " [  0 171   0 ...   0   0   0]\n",
            " [  3   0 194 ...   1   1   0]\n",
            " ...\n",
            " [  0   1   0 ... 192   0   0]\n",
            " [  0   1   0 ...   0 124   0]\n",
            " [  1   0   1 ...   2   0 188]]\n",
            "Training Loss==========================>>\n",
            "Epoch 16/20 Training Loss: 0.0211\n",
            "Accuracy: 0.6829987673886248\n",
            "Precision: 0.6836054432397227\n",
            "Recall: 0.6829987673886248\n",
            "F1-score: 0.6811389901251318\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.65      0.64       234\n",
            "           1       0.71      0.76      0.74       221\n",
            "           2       0.78      0.87      0.82       226\n",
            "           3       0.67      0.73      0.70       222\n",
            "           4       0.50      0.43      0.46       225\n",
            "           5       0.75      0.74      0.74       224\n",
            "           6       0.71      0.67      0.69       224\n",
            "           7       0.65      0.67      0.66       225\n",
            "           8       0.56      0.70      0.63       226\n",
            "           9       0.70      0.62      0.66       214\n",
            "          10       0.68      0.76      0.72       231\n",
            "          11       0.82      0.78      0.80       227\n",
            "          12       0.77      0.63      0.69       230\n",
            "          13       0.73      0.80      0.76       220\n",
            "          14       0.72      0.68      0.70       231\n",
            "          15       0.63      0.67      0.65       227\n",
            "          16       0.59      0.56      0.57       224\n",
            "          17       0.68      0.69      0.68       233\n",
            "          18       0.65      0.68      0.67       222\n",
            "          19       0.71      0.68      0.70       220\n",
            "          20       0.76      0.88      0.82       219\n",
            "          21       0.69      0.77      0.73       232\n",
            "          22       0.61      0.64      0.62       224\n",
            "          23       0.84      0.77      0.80       230\n",
            "          24       0.71      0.74      0.72       224\n",
            "          25       0.69      0.82      0.75       220\n",
            "          26       0.69      0.72      0.71       221\n",
            "          27       0.82      0.76      0.79       225\n",
            "          28       0.70      0.72      0.71       224\n",
            "          29       0.63      0.69      0.66       228\n",
            "          30       0.90      0.91      0.90       229\n",
            "          31       0.63      0.70      0.67       232\n",
            "          32       0.56      0.49      0.52       228\n",
            "          33       0.66      0.67      0.66       231\n",
            "          34       0.81      0.77      0.79       213\n",
            "          35       0.57      0.46      0.51       202\n",
            "          36       0.62      0.65      0.64       228\n",
            "          37       0.62      0.68      0.65       212\n",
            "          38       0.56      0.61      0.58       208\n",
            "          39       0.46      0.50      0.48       216\n",
            "          40       0.74      0.71      0.73       212\n",
            "          41       0.74      0.78      0.76       223\n",
            "          42       0.63      0.63      0.63       238\n",
            "          43       0.88      0.68      0.77       212\n",
            "          44       0.54      0.76      0.63       229\n",
            "          45       0.51      0.57      0.54       233\n",
            "          46       0.66      0.63      0.65       224\n",
            "          47       0.69      0.55      0.61       234\n",
            "          48       0.75      0.76      0.75       223\n",
            "          49       0.75      0.76      0.76       229\n",
            "          50       0.74      0.72      0.73       225\n",
            "          51       0.79      0.81      0.80       228\n",
            "          52       0.62      0.54      0.58       232\n",
            "          53       0.52      0.48      0.50       233\n",
            "          54       0.61      0.63      0.62       203\n",
            "          55       0.49      0.44      0.47       234\n",
            "          56       0.69      0.65      0.67       223\n",
            "          57       0.67      0.70      0.68       234\n",
            "          58       0.46      0.43      0.45       232\n",
            "          59       0.80      0.82      0.81       233\n",
            "          60       0.68      0.66      0.67       197\n",
            "          61       0.43      0.39      0.41       215\n",
            "          62       0.68      0.66      0.67       238\n",
            "          63       0.78      0.80      0.79       231\n",
            "          64       0.68      0.76      0.72       227\n",
            "          65       0.78      0.85      0.81       234\n",
            "          66       0.68      0.77      0.72       224\n",
            "          67       0.69      0.69      0.69       231\n",
            "          68       0.80      0.77      0.78       217\n",
            "          69       0.66      0.58      0.62       223\n",
            "          70       0.66      0.75      0.70       230\n",
            "          71       0.72      0.81      0.76       228\n",
            "          72       0.67      0.76      0.71       234\n",
            "          73       0.68      0.79      0.73       225\n",
            "          74       0.54      0.49      0.52       211\n",
            "          75       0.72      0.72      0.72       218\n",
            "          76       0.66      0.75      0.70       235\n",
            "          77       0.62      0.51      0.56       227\n",
            "          78       0.63      0.71      0.67       210\n",
            "          79       0.83      0.80      0.82       220\n",
            "          80       0.79      0.80      0.80       224\n",
            "          81       0.64      0.48      0.55       232\n",
            "          82       0.75      0.51      0.61       236\n",
            "          83       0.74      0.78      0.76       227\n",
            "          84       0.69      0.57      0.62       236\n",
            "          85       0.71      0.71      0.71       215\n",
            "          86       0.62      0.45      0.52       226\n",
            "          87       0.66      0.60      0.63       228\n",
            "          88       0.70      0.74      0.72       219\n",
            "          89       0.66      0.65      0.65       228\n",
            "          90       0.83      0.87      0.85       213\n",
            "          91       0.87      0.84      0.85       218\n",
            "          92       0.77      0.76      0.76       230\n",
            "          93       0.50      0.48      0.49       229\n",
            "          94       0.74      0.83      0.78       223\n",
            "          95       0.68      0.72      0.70       237\n",
            "          96       0.67      0.59      0.63       233\n",
            "          97       0.68      0.72      0.70       223\n",
            "          98       0.80      0.78      0.79       232\n",
            "          99       0.60      0.54      0.57       211\n",
            "         100       0.88      0.77      0.82       235\n",
            "\n",
            "    accuracy                           0.68     22716\n",
            "   macro avg       0.68      0.68      0.68     22716\n",
            "weighted avg       0.68      0.68      0.68     22716\n",
            "\n",
            "Confusion Matrix: \n",
            "[[152   0   5 ...   2   0   2]\n",
            " [  0 168   0 ...   0   0   0]\n",
            " [  2   0 197 ...   1   1   0]\n",
            " ...\n",
            " [  0   0   0 ... 181   0   0]\n",
            " [  0   0   0 ...   0 114   0]\n",
            " [  2   0   0 ...   1   0 182]]\n",
            "Training Loss==========================>>\n",
            "Epoch 17/20 Training Loss: 0.0187\n",
            "Accuracy: 0.6868726888536715\n",
            "Precision: 0.6886399485008334\n",
            "Recall: 0.6868726888536715\n",
            "F1-score: 0.6857585923563577\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.65      0.65       234\n",
            "           1       0.69      0.77      0.73       221\n",
            "           2       0.82      0.85      0.84       226\n",
            "           3       0.73      0.69      0.71       222\n",
            "           4       0.50      0.48      0.49       225\n",
            "           5       0.76      0.77      0.76       224\n",
            "           6       0.71      0.71      0.71       224\n",
            "           7       0.66      0.64      0.65       225\n",
            "           8       0.57      0.67      0.61       226\n",
            "           9       0.68      0.64      0.66       214\n",
            "          10       0.73      0.73      0.73       231\n",
            "          11       0.78      0.79      0.79       227\n",
            "          12       0.72      0.63      0.68       230\n",
            "          13       0.76      0.78      0.77       220\n",
            "          14       0.68      0.68      0.68       231\n",
            "          15       0.60      0.67      0.63       227\n",
            "          16       0.61      0.56      0.58       224\n",
            "          17       0.76      0.63      0.69       233\n",
            "          18       0.64      0.68      0.66       222\n",
            "          19       0.67      0.70      0.68       220\n",
            "          20       0.74      0.88      0.81       219\n",
            "          21       0.76      0.75      0.75       232\n",
            "          22       0.63      0.61      0.62       224\n",
            "          23       0.86      0.77      0.81       230\n",
            "          24       0.67      0.76      0.71       224\n",
            "          25       0.73      0.80      0.76       220\n",
            "          26       0.66      0.77      0.71       221\n",
            "          27       0.80      0.78      0.79       225\n",
            "          28       0.74      0.73      0.74       224\n",
            "          29       0.58      0.71      0.64       228\n",
            "          30       0.90      0.90      0.90       229\n",
            "          31       0.73      0.68      0.71       232\n",
            "          32       0.58      0.52      0.55       228\n",
            "          33       0.73      0.64      0.68       231\n",
            "          34       0.80      0.79      0.79       213\n",
            "          35       0.58      0.47      0.52       202\n",
            "          36       0.68      0.64      0.66       228\n",
            "          37       0.70      0.64      0.67       212\n",
            "          38       0.56      0.62      0.59       208\n",
            "          39       0.52      0.48      0.50       216\n",
            "          40       0.78      0.72      0.75       212\n",
            "          41       0.80      0.76      0.78       223\n",
            "          42       0.72      0.59      0.65       238\n",
            "          43       0.80      0.70      0.74       212\n",
            "          44       0.57      0.76      0.65       229\n",
            "          45       0.53      0.58      0.55       233\n",
            "          46       0.70      0.63      0.66       224\n",
            "          47       0.58      0.61      0.60       234\n",
            "          48       0.69      0.78      0.73       223\n",
            "          49       0.77      0.79      0.78       229\n",
            "          50       0.72      0.74      0.73       225\n",
            "          51       0.83      0.79      0.81       228\n",
            "          52       0.56      0.56      0.56       232\n",
            "          53       0.52      0.46      0.49       233\n",
            "          54       0.51      0.65      0.57       203\n",
            "          55       0.53      0.48      0.50       234\n",
            "          56       0.72      0.68      0.70       223\n",
            "          57       0.64      0.70      0.67       234\n",
            "          58       0.48      0.45      0.47       232\n",
            "          59       0.73      0.85      0.79       233\n",
            "          60       0.66      0.65      0.66       197\n",
            "          61       0.40      0.42      0.41       215\n",
            "          62       0.64      0.70      0.67       238\n",
            "          63       0.83      0.81      0.82       231\n",
            "          64       0.80      0.64      0.71       227\n",
            "          65       0.84      0.84      0.84       234\n",
            "          66       0.67      0.78      0.72       224\n",
            "          67       0.67      0.71      0.69       231\n",
            "          68       0.81      0.73      0.77       217\n",
            "          69       0.67      0.57      0.62       223\n",
            "          70       0.68      0.77      0.72       230\n",
            "          71       0.73      0.81      0.77       228\n",
            "          72       0.71      0.77      0.74       234\n",
            "          73       0.75      0.79      0.77       225\n",
            "          74       0.59      0.53      0.56       211\n",
            "          75       0.68      0.75      0.71       218\n",
            "          76       0.62      0.80      0.70       235\n",
            "          77       0.64      0.53      0.58       227\n",
            "          78       0.65      0.71      0.68       210\n",
            "          79       0.77      0.81      0.79       220\n",
            "          80       0.76      0.82      0.79       224\n",
            "          81       0.60      0.55      0.58       232\n",
            "          82       0.68      0.49      0.57       236\n",
            "          83       0.85      0.70      0.77       227\n",
            "          84       0.62      0.62      0.62       236\n",
            "          85       0.77      0.68      0.72       215\n",
            "          86       0.53      0.51      0.52       226\n",
            "          87       0.68      0.59      0.63       228\n",
            "          88       0.71      0.72      0.71       219\n",
            "          89       0.65      0.64      0.64       228\n",
            "          90       0.80      0.86      0.83       213\n",
            "          91       0.83      0.86      0.85       218\n",
            "          92       0.82      0.75      0.78       230\n",
            "          93       0.54      0.47      0.50       229\n",
            "          94       0.77      0.83      0.80       223\n",
            "          95       0.69      0.73      0.71       237\n",
            "          96       0.63      0.61      0.62       233\n",
            "          97       0.75      0.72      0.73       223\n",
            "          98       0.76      0.79      0.78       232\n",
            "          99       0.60      0.57      0.59       211\n",
            "         100       0.84      0.81      0.83       235\n",
            "\n",
            "    accuracy                           0.69     22716\n",
            "   macro avg       0.69      0.69      0.69     22716\n",
            "weighted avg       0.69      0.69      0.69     22716\n",
            "\n",
            "Confusion Matrix: \n",
            "[[152   1   4 ...   2   0   4]\n",
            " [  0 171   0 ...   0   0   0]\n",
            " [  3   1 193 ...   1   1   0]\n",
            " ...\n",
            " [  0   1   0 ... 183   0   1]\n",
            " [  0   0   1 ...   0 121   1]\n",
            " [  1   0   0 ...   1   0 190]]\n",
            "Training Loss==========================>>\n",
            "Epoch 18/20 Training Loss: 0.0177\n",
            "Accuracy: 0.6856400774784293\n",
            "Precision: 0.6873091117912957\n",
            "Recall: 0.6856400774784293\n",
            "F1-score: 0.6841444920988309\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.66      0.64       234\n",
            "           1       0.73      0.76      0.75       221\n",
            "           2       0.76      0.88      0.82       226\n",
            "           3       0.64      0.74      0.68       222\n",
            "           4       0.52      0.46      0.49       225\n",
            "           5       0.76      0.76      0.76       224\n",
            "           6       0.74      0.66      0.70       224\n",
            "           7       0.71      0.65      0.68       225\n",
            "           8       0.56      0.69      0.62       226\n",
            "           9       0.70      0.64      0.67       214\n",
            "          10       0.72      0.73      0.73       231\n",
            "          11       0.77      0.79      0.78       227\n",
            "          12       0.70      0.66      0.68       230\n",
            "          13       0.70      0.82      0.76       220\n",
            "          14       0.66      0.70      0.68       231\n",
            "          15       0.62      0.64      0.63       227\n",
            "          16       0.58      0.56      0.57       224\n",
            "          17       0.62      0.69      0.65       233\n",
            "          18       0.69      0.67      0.68       222\n",
            "          19       0.67      0.69      0.68       220\n",
            "          20       0.79      0.85      0.82       219\n",
            "          21       0.67      0.81      0.74       232\n",
            "          22       0.62      0.62      0.62       224\n",
            "          23       0.85      0.78      0.81       230\n",
            "          24       0.77      0.73      0.75       224\n",
            "          25       0.68      0.83      0.75       220\n",
            "          26       0.63      0.75      0.68       221\n",
            "          27       0.76      0.80      0.78       225\n",
            "          28       0.76      0.73      0.74       224\n",
            "          29       0.65      0.64      0.65       228\n",
            "          30       0.90      0.93      0.91       229\n",
            "          31       0.67      0.68      0.67       232\n",
            "          32       0.64      0.47      0.54       228\n",
            "          33       0.70      0.65      0.68       231\n",
            "          34       0.83      0.81      0.82       213\n",
            "          35       0.59      0.45      0.51       202\n",
            "          36       0.65      0.64      0.65       228\n",
            "          37       0.59      0.70      0.64       212\n",
            "          38       0.60      0.60      0.60       208\n",
            "          39       0.47      0.45      0.46       216\n",
            "          40       0.75      0.75      0.75       212\n",
            "          41       0.82      0.76      0.79       223\n",
            "          42       0.72      0.60      0.65       238\n",
            "          43       0.77      0.70      0.73       212\n",
            "          44       0.55      0.79      0.64       229\n",
            "          45       0.55      0.51      0.53       233\n",
            "          46       0.69      0.66      0.67       224\n",
            "          47       0.70      0.55      0.61       234\n",
            "          48       0.68      0.78      0.73       223\n",
            "          49       0.78      0.79      0.78       229\n",
            "          50       0.81      0.72      0.76       225\n",
            "          51       0.79      0.81      0.80       228\n",
            "          52       0.59      0.56      0.58       232\n",
            "          53       0.50      0.48      0.49       233\n",
            "          54       0.61      0.63      0.62       203\n",
            "          55       0.51      0.46      0.48       234\n",
            "          56       0.73      0.64      0.68       223\n",
            "          57       0.71      0.64      0.67       234\n",
            "          58       0.40      0.49      0.44       232\n",
            "          59       0.79      0.81      0.80       233\n",
            "          60       0.66      0.68      0.67       197\n",
            "          61       0.42      0.41      0.41       215\n",
            "          62       0.72      0.66      0.68       238\n",
            "          63       0.83      0.79      0.81       231\n",
            "          64       0.73      0.70      0.71       227\n",
            "          65       0.81      0.84      0.83       234\n",
            "          66       0.66      0.77      0.71       224\n",
            "          67       0.67      0.72      0.69       231\n",
            "          68       0.79      0.75      0.77       217\n",
            "          69       0.66      0.54      0.59       223\n",
            "          70       0.74      0.75      0.74       230\n",
            "          71       0.64      0.83      0.72       228\n",
            "          72       0.72      0.75      0.73       234\n",
            "          73       0.71      0.79      0.74       225\n",
            "          74       0.57      0.50      0.53       211\n",
            "          75       0.76      0.69      0.72       218\n",
            "          76       0.63      0.77      0.69       235\n",
            "          77       0.63      0.52      0.57       227\n",
            "          78       0.68      0.72      0.70       210\n",
            "          79       0.72      0.81      0.76       220\n",
            "          80       0.75      0.83      0.79       224\n",
            "          81       0.64      0.54      0.59       232\n",
            "          82       0.64      0.55      0.59       236\n",
            "          83       0.73      0.80      0.76       227\n",
            "          84       0.63      0.61      0.62       236\n",
            "          85       0.74      0.70      0.72       215\n",
            "          86       0.61      0.46      0.53       226\n",
            "          87       0.73      0.59      0.65       228\n",
            "          88       0.75      0.71      0.73       219\n",
            "          89       0.65      0.65      0.65       228\n",
            "          90       0.83      0.87      0.85       213\n",
            "          91       0.86      0.87      0.86       218\n",
            "          92       0.83      0.72      0.77       230\n",
            "          93       0.47      0.48      0.47       229\n",
            "          94       0.79      0.78      0.79       223\n",
            "          95       0.69      0.72      0.71       237\n",
            "          96       0.57      0.64      0.61       233\n",
            "          97       0.72      0.72      0.72       223\n",
            "          98       0.81      0.78      0.79       232\n",
            "          99       0.62      0.56      0.59       211\n",
            "         100       0.84      0.81      0.82       235\n",
            "\n",
            "    accuracy                           0.69     22716\n",
            "   macro avg       0.69      0.69      0.68     22716\n",
            "weighted avg       0.69      0.69      0.68     22716\n",
            "\n",
            "Confusion Matrix: \n",
            "[[154   1   6 ...   2   0   5]\n",
            " [  0 168   0 ...   0   0   0]\n",
            " [  3   0 200 ...   0   1   0]\n",
            " ...\n",
            " [  0   0   0 ... 181   0   1]\n",
            " [  0   2   1 ...   0 119   1]\n",
            " [  0   0   0 ...   1   0 190]]\n",
            "Training Loss==========================>>\n",
            "Epoch 19/20 Training Loss: 0.0166\n",
            "Accuracy: 0.6859042084874097\n",
            "Precision: 0.6871234838864015\n",
            "Recall: 0.6859042084874097\n",
            "F1-score: 0.6841252392989425\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.63      0.64       234\n",
            "           1       0.67      0.78      0.72       221\n",
            "           2       0.82      0.87      0.84       226\n",
            "           3       0.65      0.72      0.68       222\n",
            "           4       0.49      0.43      0.45       225\n",
            "           5       0.69      0.79      0.74       224\n",
            "           6       0.66      0.72      0.69       224\n",
            "           7       0.76      0.62      0.68       225\n",
            "           8       0.59      0.65      0.62       226\n",
            "           9       0.70      0.63      0.66       214\n",
            "          10       0.69      0.74      0.72       231\n",
            "          11       0.69      0.83      0.75       227\n",
            "          12       0.80      0.63      0.70       230\n",
            "          13       0.74      0.78      0.76       220\n",
            "          14       0.75      0.64      0.69       231\n",
            "          15       0.65      0.67      0.66       227\n",
            "          16       0.57      0.56      0.56       224\n",
            "          17       0.69      0.70      0.69       233\n",
            "          18       0.66      0.67      0.67       222\n",
            "          19       0.64      0.71      0.68       220\n",
            "          20       0.74      0.86      0.79       219\n",
            "          21       0.68      0.77      0.72       232\n",
            "          22       0.66      0.61      0.63       224\n",
            "          23       0.82      0.79      0.81       230\n",
            "          24       0.70      0.75      0.72       224\n",
            "          25       0.71      0.82      0.76       220\n",
            "          26       0.60      0.75      0.67       221\n",
            "          27       0.76      0.79      0.78       225\n",
            "          28       0.78      0.70      0.74       224\n",
            "          29       0.66      0.66      0.66       228\n",
            "          30       0.92      0.93      0.92       229\n",
            "          31       0.72      0.68      0.70       232\n",
            "          32       0.64      0.46      0.53       228\n",
            "          33       0.69      0.65      0.67       231\n",
            "          34       0.84      0.76      0.80       213\n",
            "          35       0.58      0.48      0.53       202\n",
            "          36       0.67      0.62      0.65       228\n",
            "          37       0.77      0.60      0.68       212\n",
            "          38       0.57      0.60      0.58       208\n",
            "          39       0.45      0.47      0.46       216\n",
            "          40       0.68      0.75      0.71       212\n",
            "          41       0.77      0.78      0.77       223\n",
            "          42       0.69      0.61      0.65       238\n",
            "          43       0.84      0.69      0.76       212\n",
            "          44       0.66      0.72      0.69       229\n",
            "          45       0.55      0.56      0.55       233\n",
            "          46       0.63      0.70      0.66       224\n",
            "          47       0.63      0.59      0.61       234\n",
            "          48       0.76      0.74      0.75       223\n",
            "          49       0.73      0.82      0.77       229\n",
            "          50       0.74      0.74      0.74       225\n",
            "          51       0.71      0.85      0.77       228\n",
            "          52       0.60      0.53      0.56       232\n",
            "          53       0.53      0.46      0.50       233\n",
            "          54       0.59      0.63      0.61       203\n",
            "          55       0.56      0.45      0.50       234\n",
            "          56       0.66      0.70      0.68       223\n",
            "          57       0.63      0.70      0.66       234\n",
            "          58       0.48      0.39      0.43       232\n",
            "          59       0.73      0.81      0.77       233\n",
            "          60       0.68      0.66      0.67       197\n",
            "          61       0.41      0.40      0.41       215\n",
            "          62       0.66      0.66      0.66       238\n",
            "          63       0.82      0.81      0.81       231\n",
            "          64       0.70      0.71      0.70       227\n",
            "          65       0.82      0.83      0.82       234\n",
            "          66       0.74      0.71      0.72       224\n",
            "          67       0.57      0.75      0.65       231\n",
            "          68       0.79      0.76      0.78       217\n",
            "          69       0.60      0.57      0.59       223\n",
            "          70       0.70      0.74      0.72       230\n",
            "          71       0.70      0.81      0.75       228\n",
            "          72       0.76      0.75      0.76       234\n",
            "          73       0.73      0.79      0.76       225\n",
            "          74       0.60      0.49      0.54       211\n",
            "          75       0.63      0.74      0.68       218\n",
            "          76       0.62      0.80      0.70       235\n",
            "          77       0.65      0.52      0.58       227\n",
            "          78       0.79      0.67      0.73       210\n",
            "          79       0.84      0.79      0.81       220\n",
            "          80       0.76      0.84      0.80       224\n",
            "          81       0.58      0.59      0.59       232\n",
            "          82       0.66      0.53      0.59       236\n",
            "          83       0.73      0.78      0.75       227\n",
            "          84       0.63      0.64      0.63       236\n",
            "          85       0.74      0.71      0.72       215\n",
            "          86       0.60      0.46      0.52       226\n",
            "          87       0.71      0.59      0.64       228\n",
            "          88       0.75      0.75      0.75       219\n",
            "          89       0.63      0.65      0.64       228\n",
            "          90       0.77      0.89      0.83       213\n",
            "          91       0.86      0.88      0.87       218\n",
            "          92       0.81      0.77      0.79       230\n",
            "          93       0.45      0.56      0.50       229\n",
            "          94       0.77      0.78      0.77       223\n",
            "          95       0.73      0.70      0.72       237\n",
            "          96       0.65      0.59      0.62       233\n",
            "          97       0.74      0.70      0.72       223\n",
            "          98       0.82      0.78      0.80       232\n",
            "          99       0.59      0.61      0.60       211\n",
            "         100       0.88      0.80      0.84       235\n",
            "\n",
            "    accuracy                           0.69     22716\n",
            "   macro avg       0.69      0.69      0.68     22716\n",
            "weighted avg       0.69      0.69      0.68     22716\n",
            "\n",
            "Confusion Matrix: \n",
            "[[147   1   4 ...   2   0   4]\n",
            " [  0 173   0 ...   0   0   0]\n",
            " [  4   0 196 ...   0   0   0]\n",
            " ...\n",
            " [  0   1   0 ... 180   0   1]\n",
            " [  0   2   1 ...   0 128   0]\n",
            " [  1   0   0 ...   1   0 189]]\n",
            "Training Loss==========================>>\n",
            "Epoch 20/20 Training Loss: 0.0157\n",
            "Accuracy: 0.687224863532312\n",
            "Precision: 0.6870965920238944\n",
            "Recall: 0.687224863532312\n",
            "F1-score: 0.6855105810832525\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.62      0.67       234\n",
            "           1       0.73      0.75      0.74       221\n",
            "           2       0.81      0.88      0.84       226\n",
            "           3       0.63      0.74      0.68       222\n",
            "           4       0.50      0.44      0.47       225\n",
            "           5       0.78      0.76      0.77       224\n",
            "           6       0.66      0.69      0.67       224\n",
            "           7       0.70      0.64      0.67       225\n",
            "           8       0.60      0.66      0.63       226\n",
            "           9       0.76      0.62      0.68       214\n",
            "          10       0.66      0.76      0.71       231\n",
            "          11       0.79      0.79      0.79       227\n",
            "          12       0.70      0.68      0.69       230\n",
            "          13       0.75      0.81      0.78       220\n",
            "          14       0.71      0.66      0.69       231\n",
            "          15       0.58      0.65      0.61       227\n",
            "          16       0.54      0.54      0.54       224\n",
            "          17       0.66      0.72      0.69       233\n",
            "          18       0.62      0.68      0.65       222\n",
            "          19       0.70      0.70      0.70       220\n",
            "          20       0.73      0.88      0.80       219\n",
            "          21       0.66      0.79      0.72       232\n",
            "          22       0.64      0.61      0.62       224\n",
            "          23       0.78      0.80      0.79       230\n",
            "          24       0.72      0.75      0.74       224\n",
            "          25       0.73      0.80      0.76       220\n",
            "          26       0.61      0.76      0.68       221\n",
            "          27       0.80      0.78      0.79       225\n",
            "          28       0.74      0.72      0.73       224\n",
            "          29       0.64      0.67      0.66       228\n",
            "          30       0.91      0.92      0.92       229\n",
            "          31       0.67      0.72      0.70       232\n",
            "          32       0.50      0.52      0.51       228\n",
            "          33       0.72      0.66      0.69       231\n",
            "          34       0.78      0.80      0.79       213\n",
            "          35       0.48      0.49      0.48       202\n",
            "          36       0.71      0.63      0.67       228\n",
            "          37       0.65      0.68      0.66       212\n",
            "          38       0.61      0.55      0.58       208\n",
            "          39       0.45      0.46      0.46       216\n",
            "          40       0.76      0.72      0.74       212\n",
            "          41       0.80      0.77      0.78       223\n",
            "          42       0.65      0.64      0.65       238\n",
            "          43       0.85      0.70      0.77       212\n",
            "          44       0.70      0.69      0.70       229\n",
            "          45       0.52      0.56      0.54       233\n",
            "          46       0.70      0.65      0.67       224\n",
            "          47       0.66      0.57      0.61       234\n",
            "          48       0.73      0.76      0.74       223\n",
            "          49       0.73      0.80      0.77       229\n",
            "          50       0.73      0.72      0.73       225\n",
            "          51       0.84      0.82      0.83       228\n",
            "          52       0.60      0.53      0.56       232\n",
            "          53       0.54      0.48      0.51       233\n",
            "          54       0.63      0.62      0.62       203\n",
            "          55       0.51      0.45      0.48       234\n",
            "          56       0.64      0.67      0.65       223\n",
            "          57       0.65      0.69      0.67       234\n",
            "          58       0.48      0.42      0.45       232\n",
            "          59       0.76      0.85      0.80       233\n",
            "          60       0.59      0.69      0.64       197\n",
            "          61       0.46      0.39      0.42       215\n",
            "          62       0.68      0.67      0.68       238\n",
            "          63       0.80      0.80      0.80       231\n",
            "          64       0.67      0.73      0.70       227\n",
            "          65       0.83      0.83      0.83       234\n",
            "          66       0.70      0.73      0.72       224\n",
            "          67       0.63      0.71      0.67       231\n",
            "          68       0.82      0.76      0.78       217\n",
            "          69       0.70      0.56      0.62       223\n",
            "          70       0.72      0.73      0.72       230\n",
            "          71       0.75      0.82      0.78       228\n",
            "          72       0.69      0.76      0.73       234\n",
            "          73       0.69      0.78      0.73       225\n",
            "          74       0.53      0.52      0.53       211\n",
            "          75       0.68      0.75      0.71       218\n",
            "          76       0.62      0.78      0.69       235\n",
            "          77       0.65      0.55      0.59       227\n",
            "          78       0.73      0.69      0.71       210\n",
            "          79       0.81      0.80      0.80       220\n",
            "          80       0.82      0.80      0.81       224\n",
            "          81       0.64      0.51      0.57       232\n",
            "          82       0.65      0.52      0.57       236\n",
            "          83       0.74      0.79      0.77       227\n",
            "          84       0.67      0.64      0.65       236\n",
            "          85       0.79      0.71      0.75       215\n",
            "          86       0.54      0.47      0.50       226\n",
            "          87       0.69      0.57      0.63       228\n",
            "          88       0.76      0.73      0.74       219\n",
            "          89       0.65      0.66      0.65       228\n",
            "          90       0.84      0.87      0.86       213\n",
            "          91       0.85      0.89      0.87       218\n",
            "          92       0.81      0.76      0.78       230\n",
            "          93       0.55      0.48      0.51       229\n",
            "          94       0.74      0.80      0.77       223\n",
            "          95       0.72      0.71      0.72       237\n",
            "          96       0.69      0.59      0.63       233\n",
            "          97       0.70      0.72      0.71       223\n",
            "          98       0.73      0.82      0.77       232\n",
            "          99       0.61      0.57      0.59       211\n",
            "         100       0.87      0.80      0.84       235\n",
            "\n",
            "    accuracy                           0.69     22716\n",
            "   macro avg       0.69      0.69      0.69     22716\n",
            "weighted avg       0.69      0.69      0.69     22716\n",
            "\n",
            "Confusion Matrix: \n",
            "[[146   1   2 ...   2   0   4]\n",
            " [  0 165   0 ...   0   0   0]\n",
            " [  2   0 198 ...   1   0   0]\n",
            " ...\n",
            " [  0   1   0 ... 190   0   1]\n",
            " [  0   1   1 ...   0 120   0]\n",
            " [  1   0   0 ...   1   0 188]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the loss curve\n",
        "plt.plot(range(1, num_epochs+1), training_loss)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss Curve')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uBQm6WUy2Mas",
        "outputId": "aa7c5323-20ba-4fe3-ef5d-b7cfc80cea4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save Model"
      ],
      "metadata": {
        "id": "oA19HxxWkW78"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "En61iS0MncTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd /content/Model/Models-Train-07/"
      ],
      "metadata": {
        "id": "JdafxfVVneqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !cp 21_model.pth /content/drive/MyDrive/21_model.pth"
      ],
      "metadata": {
        "id": "Y_N_X_mUngns"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}