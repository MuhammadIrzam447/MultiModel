{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "C4SHn9-5RZZn",
        "kptdeoGFRdUx"
      ],
      "authorship_tag": "ABX9TyNAVAp2xDg3AFsGb7xFXeY9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MuhammadIrzam447/MultiModel/blob/master/Train_33.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Notebook for the training and evalution of ViT for Multi-Class & Multi-Label Dataset i.e MM-IMDB"
      ],
      "metadata": {
        "id": "rip0FqLeRYxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read Training Dataset"
      ],
      "metadata": {
        "id": "C4SHn9-5RZZn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJo17J8LP5N-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "image_file_paths = []\n",
        "image_genre_labels = []\n",
        "enc_file_paths = []\n",
        "enc_genre_labels = []\n",
        "\n",
        "labels_file = \"/content/Dataset(s)/mm-imdb/joint/joint_train_label.txt\"\n",
        "image_folder_add = \"/content/Dataset(s)/mm-imdb/joint/train\"\n",
        "\n",
        "with open(labels_file, 'r') as file:\n",
        "    for line in file:\n",
        "        parts = line.strip().split('|')\n",
        "        filename = parts[0].strip()\n",
        "        labels = parts[1].strip().split(', ')\n",
        "        image_path = os.path.join(image_folder_add, filename)\n",
        "\n",
        "        if filename.endswith(\"_4.png\"):\n",
        "            enc_file_paths.append(image_path)\n",
        "            enc_genre_labels.append(labels)\n",
        "\n",
        "        elif filename.endswith(\"_3.png\"):\n",
        "            image_file_paths.append(image_path)\n",
        "            image_genre_labels.append(labels)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(enc_file_paths), len(image_file_paths)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKAkggLBScIl",
        "outputId": "8648f571-07dd-404f-d603-24dae5ac7071"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15552, 15552)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "label_counts = defaultdict(int)\n",
        "\n",
        "for labels in image_genre_labels:\n",
        "    for label in labels:\n",
        "        label_counts[label] += 1\n",
        "\n",
        "label_count_list = [(label, count) for label, count in label_counts.items()]\n",
        "sorted_label_count_list = sorted(label_count_list, key=lambda x: x[1], reverse=True)\n",
        "for label, count in sorted_label_count_list:\n",
        "    print(f\"{label}: {count}\")\n",
        "\n",
        "print(\"Total Labels: \", len(label_count_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0q3zFSp_P_8X",
        "outputId": "121d0e1f-5ffe-4813-84a4-aeacd345680a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drama: 8424\n",
            "Comedy: 5108\n",
            "Romance: 3226\n",
            "Thriller: 3113\n",
            "Crime: 2293\n",
            "Action: 2155\n",
            "Adventure: 1611\n",
            "Horror: 1603\n",
            "Documentary: 1234\n",
            "Mystery: 1231\n",
            "Sci-Fi: 1212\n",
            "Fantasy: 1162\n",
            "Family: 978\n",
            "War: 806\n",
            "Biography: 788\n",
            "History: 680\n",
            "Music: 634\n",
            "Animation: 586\n",
            "Musical: 503\n",
            "Western: 423\n",
            "Sport: 379\n",
            "Short: 281\n",
            "Film-Noir: 202\n",
            "News: 39\n",
            "Talk-Show: 2\n",
            "Reality-TV: 1\n",
            "Total Labels:  26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "min_label_count = 40\n",
        "valid_labels = [label for label, count in label_counts.items() if count >= min_label_count]\n",
        "valid_labels = sorted(list(valid_labels))"
      ],
      "metadata": {
        "id": "rzm7XXqzQEEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_image_paths = []\n",
        "filtered_genre_labels = []\n",
        "\n",
        "for image_path, labels in zip(image_file_paths, image_genre_labels):\n",
        "    valid_labels_for_sample = [label for label in labels if label in valid_labels]\n",
        "\n",
        "    if valid_labels_for_sample:\n",
        "        filtered_image_paths.append(image_path)\n",
        "        filtered_genre_labels.append(valid_labels_for_sample)"
      ],
      "metadata": {
        "id": "fy69jMIWQHPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_enc_paths = []\n",
        "filtered_enc_labels = []\n",
        "\n",
        "for image_path, labels in zip(enc_file_paths, enc_genre_labels):\n",
        "    valid_labels_for_sample = [label for label in labels if label in valid_labels]\n",
        "\n",
        "    if valid_labels_for_sample:\n",
        "        filtered_enc_paths.append(image_path)\n",
        "        filtered_enc_labels.append(valid_labels_for_sample)"
      ],
      "metadata": {
        "id": "fyK5W87jQI9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_file_paths = filtered_image_paths\n",
        "image_genre_labels = filtered_genre_labels\n",
        "enc_file_paths = filtered_enc_paths\n",
        "enc_genre_labels = filtered_enc_labels"
      ],
      "metadata": {
        "id": "un6ZWIpsQYSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(image_file_paths), len(enc_file_paths)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6KpYkZvTMPY",
        "outputId": "c84ffcf1-8be0-4293-8385-8f5a405f7dbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15552, 15552)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "combined_data = list(zip(enc_file_paths, enc_genre_labels))\n",
        "sample_size = int(0.3 * len(combined_data))\n",
        "sampled_data = random.sample(combined_data, sample_size)\n",
        "\n",
        "\n",
        "sampled_file_paths, sampled_genre_labels = zip(*sampled_data)"
      ],
      "metadata": {
        "id": "z3qUB7PCUxRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(sampled_file_paths)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BrYrQnDU58y",
        "outputId": "60896468-17a7-4c0b-ae35-8cfa3a4464e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4665"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_file_paths[0:3]"
      ],
      "metadata": {
        "id": "W1B9Oj5Texx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_file_paths = list(sampled_file_paths)\n",
        "sampled_genre_labels = list(sampled_genre_labels)"
      ],
      "metadata": {
        "id": "lCoklGzRWeJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_file_paths = image_file_paths + sampled_file_paths\n",
        "train_genre_labels = image_genre_labels + sampled_genre_labels"
      ],
      "metadata": {
        "id": "UeMEMLwXQezp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read Test Dataset"
      ],
      "metadata": {
        "id": "kptdeoGFRdUx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "test_image_file_paths = []\n",
        "test_image_genre_labels = []\n",
        "test_enc_file_paths = []\n",
        "test_enc_genre_labels = []\n",
        "\n",
        "test_labels_file = \"/content/Dataset(s)/mm-imdb/joint/joint_test_label.txt\"\n",
        "image_folder_add = \"/content/Dataset(s)/mm-imdb/joint/test\"\n",
        "\n",
        "with open(test_labels_file, 'r') as file:\n",
        "    for line in file:\n",
        "        parts = line.strip().split('|')\n",
        "        filename = parts[0].strip()\n",
        "        labels = parts[1].strip().split(', ')\n",
        "        image_path = os.path.join(image_folder_add, filename)\n",
        "\n",
        "        if filename.endswith(\"_4.png\"):\n",
        "            test_enc_file_paths.append(image_path)\n",
        "            test_enc_genre_labels.append(labels)\n",
        "\n",
        "        elif filename.endswith(\"_3.png\"):\n",
        "            test_image_file_paths.append(image_path)\n",
        "            test_image_genre_labels.append(labels)"
      ],
      "metadata": {
        "id": "HZELsYKdQqoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_enc_file_paths), len(test_image_file_paths)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pet0k_APTf6L",
        "outputId": "aa08dcdb-9d72-4644-9b6e-e9fbc17fa76a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7799, 7799)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_enc_file_paths[0:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjvm1seuTogI",
        "outputId": "2c281ed5-7bdb-403e-ad83-74f58f888a55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/Dataset(s)/mm-imdb/joint/test/0078718_4.png',\n",
              " '/content/Dataset(s)/mm-imdb/joint/test/0089003_4.png']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_filtered_image_paths = []\n",
        "test_filtered_genre_labels = []\n",
        "\n",
        "for image_path, labels in zip(test_image_file_paths, test_image_genre_labels):\n",
        "    valid_labels_for_sample = [label for label in labels if label in valid_labels]\n",
        "\n",
        "    if valid_labels_for_sample:\n",
        "        test_filtered_image_paths.append(image_path)\n",
        "        test_filtered_genre_labels.append(valid_labels_for_sample)"
      ],
      "metadata": {
        "id": "Eq7rExa3QtWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_filtered_enc_paths = []\n",
        "test_filtered_enc_labels = []\n",
        "\n",
        "for image_path, labels in zip(test_enc_file_paths, test_enc_genre_labels):\n",
        "    valid_labels_for_sample = [label for label in labels if label in valid_labels]\n",
        "\n",
        "    if valid_labels_for_sample:\n",
        "        test_filtered_enc_paths.append(image_path)\n",
        "        test_filtered_enc_labels.append(valid_labels_for_sample)"
      ],
      "metadata": {
        "id": "4bb3IsB_QvTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_image_file_paths = test_filtered_image_paths\n",
        "test_image_genre_labels = test_filtered_genre_labels\n",
        "test_enc_file_paths = test_filtered_enc_paths\n",
        "test_enc_genre_labels = test_filtered_enc_labels"
      ],
      "metadata": {
        "id": "U97-8jEfQxFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_image_genre_labels), len(test_enc_genre_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vPN_duPTzUE",
        "outputId": "72e9144a-4047-4281-e5f8-e608b851f93f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7799, 7799)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "test_combined_data = list(zip(test_enc_file_paths, test_enc_genre_labels))\n",
        "test_sample_size = int(0.3 * len(test_combined_data))\n",
        "sampled_data = random.sample(test_combined_data, test_sample_size)\n",
        "\n",
        "\n",
        "test_sampled_file_paths, test_sampled_genre_labels = zip(*sampled_data)"
      ],
      "metadata": {
        "id": "DRohk8mUXBXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sampled_file_paths = list(test_sampled_file_paths)\n",
        "test_sampled_genre_labels = list(test_sampled_genre_labels)"
      ],
      "metadata": {
        "id": "-_ckjLQ8XcD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_file_paths = test_image_file_paths + test_sampled_file_paths\n",
        "test_genre_labels = test_image_genre_labels + test_sampled_genre_labels"
      ],
      "metadata": {
        "id": "4nRe7KaqXytl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ViT and Label Encoding"
      ],
      "metadata": {
        "id": "Gf2q9CXvQ-Ca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "!pip install transformers evaluate datasets\n",
        "import requests\n",
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import *\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "Tj7yGfsBQ-g5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model_name = \"google/vit-base-patch16-224\"\n",
        "image_processor = ViTImageProcessor.from_pretrained(model_name)\n",
        "model = ViTForImageClassification.from_pretrained(model_name).to(device)"
      ],
      "metadata": {
        "id": "KyJKaFI8Bpsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "mlb = MultiLabelBinarizer()\n",
        "mlb.fit(train_genre_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "PsAxg484RCW9",
        "outputId": "1713a197-6fb9-4a98-c6ae-43b8cd8ac95d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultiLabelBinarizer()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultiLabelBinarizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultiLabelBinarizer</label><div class=\"sk-toggleable__content\"><pre>MultiLabelBinarizer()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformed_train_genre_labels = mlb.transform(train_genre_labels)"
      ],
      "metadata": {
        "id": "yqL4eMrAREdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labeling_scheme = mlb.classes_\n",
        "print(len(labeling_scheme))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yFGK_j7RLyE",
        "outputId": "34694bd4-c5ac-47f5-82ff-16572f9ca169"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_to_class = {i: label for i, label in enumerate(labeling_scheme)}"
      ],
      "metadata": {
        "id": "yYXoHk15Y1H9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Labeling scheme (binary representations): \", labeling_scheme)\n",
        "print(\"Labeling scheme (class names): \", label_to_class)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49uHTzJxY3pk",
        "outputId": "e04a4ad3-7009-4039-f03f-2660b7fdab93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labeling scheme (binary representations):  ['Action' 'Adventure' 'Animation' 'Biography' 'Comedy' 'Crime'\n",
            " 'Documentary' 'Drama' 'Family' 'Fantasy' 'Film-Noir' 'History' 'Horror'\n",
            " 'Music' 'Musical' 'Mystery' 'Romance' 'Sci-Fi' 'Short' 'Sport' 'Thriller'\n",
            " 'War' 'Western']\n",
            "Labeling scheme (class names):  {0: 'Action', 1: 'Adventure', 2: 'Animation', 3: 'Biography', 4: 'Comedy', 5: 'Crime', 6: 'Documentary', 7: 'Drama', 8: 'Family', 9: 'Fantasy', 10: 'Film-Noir', 11: 'History', 12: 'Horror', 13: 'Music', 14: 'Musical', 15: 'Mystery', 16: 'Romance', 17: 'Sci-Fi', 18: 'Short', 19: 'Sport', 20: 'Thriller', 21: 'War', 22: 'Western'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformed_train_genre_labels[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpk94aX_YY9s",
        "outputId": "10edb5ca-206c-46f6-8d13-6f8f1e4a45d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "mlb1 = MultiLabelBinarizer()\n",
        "mlb1.fit(test_genre_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "DQzxXxaVRNio",
        "outputId": "708becfd-a45c-4ba2-f2d1-1d474e8576d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultiLabelBinarizer()"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultiLabelBinarizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultiLabelBinarizer</label><div class=\"sk-toggleable__content\"><pre>MultiLabelBinarizer()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformed_test_genre_labels = mlb1.transform(test_genre_labels)"
      ],
      "metadata": {
        "id": "OOa_BNY0RO_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labeling_scheme = mlb.classes_\n",
        "\n",
        "label_to_class = {i: label for i, label in enumerate(labeling_scheme)}\n",
        "\n",
        "print(\"Labeling scheme (binary representations): \", labeling_scheme)\n",
        "print(\"Labeling scheme (class names): \", label_to_class)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUYKvA_6Ygx-",
        "outputId": "5102d7d5-9ab8-402e-d70a-c84b73b30a47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labeling scheme (binary representations):  ['Action' 'Adventure' 'Animation' 'Biography' 'Comedy' 'Crime'\n",
            " 'Documentary' 'Drama' 'Family' 'Fantasy' 'Film-Noir' 'History' 'Horror'\n",
            " 'Music' 'Musical' 'Mystery' 'Romance' 'Sci-Fi' 'Short' 'Sport' 'Thriller'\n",
            " 'War' 'Western']\n",
            "Labeling scheme (class names):  {0: 'Action', 1: 'Adventure', 2: 'Animation', 3: 'Biography', 4: 'Comedy', 5: 'Crime', 6: 'Documentary', 7: 'Drama', 8: 'Family', 9: 'Fantasy', 10: 'Film-Noir', 11: 'History', 12: 'Horror', 13: 'Music', 14: 'Musical', 15: 'Mystery', 16: 'Romance', 17: 'Sci-Fi', 18: 'Short', 19: 'Sport', 20: 'Thriller', 21: 'War', 22: 'Western'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformed_train_genre_labels[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDGwK_WgjkcE",
        "outputId": "9dd94401-fad1-42aa-df79-2567ab0c21e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Dataset objects"
      ],
      "metadata": {
        "id": "TIPgPoPjRrvg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "train_data = {'image': train_file_paths, 'label': transformed_train_genre_labels}\n",
        "ds_train = Dataset.from_dict(train_data)"
      ],
      "metadata": {
        "id": "xRxEQ8F5Rp4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_data = {'image': test_file_paths, 'label': transformed_test_genre_labels}\n",
        "ds_val = Dataset.from_dict(val_data)"
      ],
      "metadata": {
        "id": "gQKlOYLbRyV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import PIL.Image as pil\n",
        "\n",
        "def transform(examples):\n",
        "  inputs = image_processor([pil.open(img).convert(\"RGB\") for img in examples[\"image\"]], return_tensors=\"pt\")\n",
        "  inputs[\"labels\"] = examples[\"label\"]\n",
        "  return inputs"
      ],
      "metadata": {
        "id": "Rk1KfF0WR18e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = ds_train.with_transform(transform)\n",
        "val_dataset = ds_val.with_transform(transform)"
      ],
      "metadata": {
        "id": "IJtq42AQR2_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for item in val_dataset:\n",
        "  print(item['pixel_values'].shape)\n",
        "  print(item[\"labels\"])\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4l5qRPpR3u5",
        "outputId": "98ee9b9e-5714-4005-d944-3f07c75b5c48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 224, 224])\n",
            "[0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def collate_fn(batch):\n",
        "  return {\n",
        "      \"pixel_values\": torch.stack([x[\"pixel_values\"] for x in batch]),\n",
        "      \"labels\": torch.tensor([x[\"labels\"] for x in batch]),\n",
        "      # \"labels\": torch.stack([x[\"labels\"] for x in batch], dim=0),\n",
        "  }"
      ],
      "metadata": {
        "id": "j8reQSS3R6UP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the ViT model\n",
        "model = ViTForImageClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels= len(labeling_scheme),\n",
        "    label2id = {label: str(i) for i, label in enumerate(labeling_scheme)},\n",
        "    id2label = {str(i): label for i, label in enumerate(labeling_scheme)},\n",
        "    problem_type = \"multi_label_classification\",\n",
        "    ignore_mismatched_sizes=True,\n",
        ")"
      ],
      "metadata": {
        "id": "ogUDkRUiZPp3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3869b3c-3039-4610-f5f4-e753234eb14e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--vit-base-patch16-224/snapshots/3f49326eb077187dfe1c2a2bb15fbd74e6ab91e3/config.json\n",
            "Model config ViTConfig {\n",
            "  \"_name_or_path\": \"google/vit-base-patch16-224\",\n",
            "  \"architectures\": [\n",
            "    \"ViTForImageClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.0,\n",
            "  \"encoder_stride\": 16,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.0,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"Action\",\n",
            "    \"1\": \"Adventure\",\n",
            "    \"10\": \"Film-Noir\",\n",
            "    \"11\": \"History\",\n",
            "    \"12\": \"Horror\",\n",
            "    \"13\": \"Music\",\n",
            "    \"14\": \"Musical\",\n",
            "    \"15\": \"Mystery\",\n",
            "    \"16\": \"Romance\",\n",
            "    \"17\": \"Sci-Fi\",\n",
            "    \"18\": \"Short\",\n",
            "    \"19\": \"Sport\",\n",
            "    \"2\": \"Animation\",\n",
            "    \"20\": \"Thriller\",\n",
            "    \"21\": \"War\",\n",
            "    \"22\": \"Western\",\n",
            "    \"3\": \"Biography\",\n",
            "    \"4\": \"Comedy\",\n",
            "    \"5\": \"Crime\",\n",
            "    \"6\": \"Documentary\",\n",
            "    \"7\": \"Drama\",\n",
            "    \"8\": \"Family\",\n",
            "    \"9\": \"Fantasy\"\n",
            "  },\n",
            "  \"image_size\": 224,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"Action\": \"0\",\n",
            "    \"Adventure\": \"1\",\n",
            "    \"Animation\": \"2\",\n",
            "    \"Biography\": \"3\",\n",
            "    \"Comedy\": \"4\",\n",
            "    \"Crime\": \"5\",\n",
            "    \"Documentary\": \"6\",\n",
            "    \"Drama\": \"7\",\n",
            "    \"Family\": \"8\",\n",
            "    \"Fantasy\": \"9\",\n",
            "    \"Film-Noir\": \"10\",\n",
            "    \"History\": \"11\",\n",
            "    \"Horror\": \"12\",\n",
            "    \"Music\": \"13\",\n",
            "    \"Musical\": \"14\",\n",
            "    \"Mystery\": \"15\",\n",
            "    \"Romance\": \"16\",\n",
            "    \"Sci-Fi\": \"17\",\n",
            "    \"Short\": \"18\",\n",
            "    \"Sport\": \"19\",\n",
            "    \"Thriller\": \"20\",\n",
            "    \"War\": \"21\",\n",
            "    \"Western\": \"22\"\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"model_type\": \"vit\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_channels\": 3,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"patch_size\": 16,\n",
            "  \"problem_type\": \"multi_label_classification\",\n",
            "  \"qkv_bias\": true,\n",
            "  \"transformers_version\": \"4.32.1\"\n",
            "}\n",
            "\n",
            "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--google--vit-base-patch16-224/snapshots/3f49326eb077187dfe1c2a2bb15fbd74e6ab91e3/model.safetensors\n",
            "All model checkpoint weights were used when initializing ViTForImageClassification.\n",
            "\n",
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
            "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([23]) in the model instantiated\n",
            "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([23, 768]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# from evaluate import load\n",
        "# from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# accuracy = load(\"accuracy\")\n",
        "# f1 = load(\"f1\")\n",
        "\n",
        "# def compute_metrics(eval_pred):\n",
        "#     raw_logits = torch.tensor(eval_pred.predictions)\n",
        "#     sigmoid_predictions = torch.sigmoid(raw_logits).numpy()\n",
        "\n",
        "#     thresholded_predictions = (sigmoid_predictions > 0.5).astype(int)\n",
        "\n",
        "#     label_ids = eval_pred.label_ids\n",
        "\n",
        "#     accuracy_score = accuracy.compute(predictions=thresholded_predictions, references=label_ids)\n",
        "#     f1_score = f1.compute(predictions=thresholded_predictions, references=label_ids, average=\"macro\")\n",
        "\n",
        "#     return {**accuracy_score, **f1_score}\n"
      ],
      "metadata": {
        "id": "P0ELmE5qZsnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import TrainingArguments\n",
        "\n",
        "# training_args = TrainingArguments(\n",
        "#   output_dir=\"/content/Model/Models-Train-32\", # output directory\n",
        "#   per_device_train_batch_size=32, # batch size per device during training\n",
        "#   evaluation_strategy=\"steps\",    # evaluation strategy to adopt during training\n",
        "#   num_train_epochs=20,             # total number of training epochs\n",
        "#   # fp16=True,                    # use mixed precision\n",
        "#   save_steps=10,                # number of update steps before saving checkpoint\n",
        "#   eval_steps=10,                # number of update steps before evaluating\n",
        "#   logging_steps=10,             # number of update steps before logging\n",
        "#   # save_steps=50,\n",
        "#   # eval_steps=50,\n",
        "#   # logging_steps=50,\n",
        "#   save_total_limit=2,             # limit the total amount of checkpoints on disk\n",
        "#   remove_unused_columns=False,    # remove unused columns from the dataset\n",
        "#   push_to_hub=False,              # do not push the model to the hub\n",
        "#   report_to='tensorboard',        # report metrics to tensorboard\n",
        "#   load_best_model_at_end=True,    # load the best model at the end of training\n",
        "# )"
      ],
      "metadata": {
        "id": "JUlbHzr9fTul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import Trainer\n",
        "\n",
        "# trainer = Trainer(\n",
        "#     model=model,                        # the instantiated 🤗 Transformers model to be trained\n",
        "#     args=training_args,                 # training arguments, defined above\n",
        "#     data_collator=collate_fn,           # the data collator that will be used for batching\n",
        "#     compute_metrics=compute_metrics,    # the metrics function that will be used for evaluation\n",
        "#     train_dataset=train_dataset,        # training dataset\n",
        "#     eval_dataset=val_dataset,           # evaluation dataset\n",
        "#     tokenizer=image_processor,          # the processor that will be used for preprocessing the images\n",
        "# )"
      ],
      "metadata": {
        "id": "SBg-Z2cifdwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trainer.train()"
      ],
      "metadata": {
        "id": "j2IeEOw9fh-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7w8qdhOylYq4",
        "outputId": "71e90d25-ab5e-45c9-bc35-bd44926dc579"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['image', 'label'],\n",
              "    num_rows: 20217\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBhl4folA1mY",
        "outputId": "0e7c0326-954a-4bb5-fc3c-80d1eac54536"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['image', 'label'],\n",
              "    num_rows: 10138\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample = train_dataset[0]"
      ],
      "metadata": {
        "id": "qukNA0USA24i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9_SoSkvA4v7",
        "outputId": "613782ac-9a7e-49bb-b198-3e89938546c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'pixel_values': tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          ...,\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
              " \n",
              "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          ...,\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
              " \n",
              "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          ...,\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.]]]),\n",
              " 'labels': [0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0]}"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting Hyperparameters"
      ],
      "metadata": {
        "id": "kCt2qpZCA_HF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "7tG1R2KNA501"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_loader = DataLoader(train_dataset, collate_fn=collate_fn, batch_size=batch_size, shuffle=True)\n",
        "valid_dataset_loader = DataLoader(val_dataset, collate_fn=collate_fn, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "03kubwrZBCQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n"
      ],
      "metadata": {
        "id": "d7ktYOQjBEff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_dir = \"/content/Model/Models-Train-33/runs\"\n",
        "summary_writer = SummaryWriter(log_dir=log_dir)"
      ],
      "metadata": {
        "id": "ETmUWzaPBF9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 30\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "pZd-lMvXBLg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_train_steps = num_epochs * len(train_dataset_loader)\n",
        "n_valid_steps = len(valid_dataset_loader)\n",
        "current_step = 0"
      ],
      "metadata": {
        "id": "2sVhNwOqBNCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_train_steps , n_valid_steps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHKRXzBhBOPM",
        "outputId": "e42f3685-1f96-44d6-dbaa-bc5ee43158d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(18960, 317)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# logging, eval & save steps\n",
        "save_steps = 650 #3000"
      ],
      "metadata": {
        "id": "J92Upx38BOrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "import numpy as np\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    # progress_bar = tqdm(range(current_step, n_train_steps), \"Training\", dynamic_ncols=True, ncols=80)\n",
        "\n",
        "    for batch in train_dataset_loader:\n",
        "      if (current_step+1) % save_steps == 0:\n",
        "        print()\n",
        "        print(f\"Validation at step {current_step}...\")\n",
        "        print()\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        predictions, labels = [], []\n",
        "        valid_loss = 0\n",
        "\n",
        "        for batch in valid_dataset_loader:\n",
        "            pixel_values = batch[\"pixel_values\"].to(device)\n",
        "            label_ids = batch[\"labels\"].to(device).float()\n",
        "\n",
        "            outputs = model(pixel_values=pixel_values, labels=label_ids)\n",
        "\n",
        "            loss = outputs.loss\n",
        "            valid_loss += loss.item()\n",
        "\n",
        "            logits = outputs.logits.detach().cpu()\n",
        "\n",
        "            # predictions.extend((logits > 0.5).int().cpu().numpy())\n",
        "            predictions.extend(F.sigmoid(logits).cpu().numpy())\n",
        "            labels.extend(label_ids.int().cpu().numpy())\n",
        "\n",
        "        # eval_prediction = EvalPrediction(predictions=predictions, label_ids=labels)\n",
        "        # metrics = compute_metrics(eval_prediction)\n",
        "        print()\n",
        "        print(f\"Epoch: {epoch}, Step: {current_step}, Train Loss: {train_loss / save_steps:.4f}, \" +\n",
        "              f\"Valid Loss: {valid_loss / n_valid_steps:.4f}\") # , Accuracy: {metrics['accuracy']}, \" + f\"F1 Score: {metrics['f1']}\")\n",
        "        print()\n",
        "\n",
        "        # summary_writer.add_scalar(\"valid_loss\", valid_loss / n_valid_steps, global_step=current_step)\n",
        "        # summary_writer.add_scalar(\"accuracy\", metrics[\"accuracy\"], global_step=current_step)\n",
        "        # summary_writer.add_scalar(\"f1\", metrics[\"f1\"], global_step=current_step)\n",
        "\n",
        "        model.save_pretrained(f\"/content/Model/Models-Train-33/checkpoint-{current_step}\")\n",
        "        image_processor.save_pretrained(f\"/content/Model/Models-Train-33/checkpoint-{current_step}\")\n",
        "\n",
        "        predictions = np.array(predictions)\n",
        "        threshold = 0.5\n",
        "        predictions = (predictions > threshold).astype(int)\n",
        "\n",
        "        accuracy = accuracy_score(labels, predictions)\n",
        "        precision = precision_score(labels, predictions, average='macro')\n",
        "        recall = recall_score(labels, predictions, average='macro')\n",
        "        f1 = f1_score(labels, predictions, average='macro')\n",
        "\n",
        "        print(f\"Accuracy: {accuracy}\")\n",
        "        print(f\"Precision: {precision}\")\n",
        "        print(f\"Recall: {recall}\")\n",
        "        print(f\"F1-Score: {f1}\")\n",
        "        # print(classification_report(labels, predictions))\n",
        "\n",
        "        model.train()\n",
        "        train_loss, valid_loss = 0, 0\n",
        "\n",
        "      pixel_values = batch[\"pixel_values\"].to(device)\n",
        "      labels = batch[\"labels\"].to(device).float()\n",
        "\n",
        "      outputs = model(pixel_values=pixel_values, labels=labels)\n",
        "\n",
        "      loss = outputs.loss\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      loss_v = loss.item()\n",
        "      train_loss += loss_v\n",
        "\n",
        "      current_step += 1\n",
        "      # progress_bar.update(1)\n",
        "      summary_writer.add_scalar(\"train_loss\", loss_v, global_step=current_step)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jWiD2nAnBUD3",
        "outputId": "c18d6868-639a-4151-fc39-cf0d925056cf"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation at step 649...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Configuration saved in /content/Model/Models-Train-33/checkpoint-649/config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1, Step: 649, Train Loss: 0.0071, Valid Loss: 0.2652\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Model weights saved in /content/Model/Models-Train-33/checkpoint-649/pytorch_model.bin\n",
            "Image processor saved in /content/Model/Models-Train-33/checkpoint-649/preprocessor_config.json\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.09380548431643322\n",
            "Precision: 0.2756601541175136\n",
            "Recall: 0.05104722749691235\n",
            "F1-Score: 0.05305317270440901\n",
            "\n",
            "Validation at step 1299...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Configuration saved in /content/Model/Models-Train-33/checkpoint-1299/config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 2, Step: 1299, Train Loss: 0.0131, Valid Loss: 0.2466\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Model weights saved in /content/Model/Models-Train-33/checkpoint-1299/pytorch_model.bin\n",
            "Image processor saved in /content/Model/Models-Train-33/checkpoint-1299/preprocessor_config.json\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.11106727165121326\n",
            "Precision: 0.39394896374197363\n",
            "Recall: 0.12207853033083946\n",
            "F1-Score: 0.1627297421290352\n",
            "\n",
            "Validation at step 1949...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Configuration saved in /content/Model/Models-Train-33/checkpoint-1949/config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 3, Step: 1949, Train Loss: 0.0183, Valid Loss: 0.2386\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Model weights saved in /content/Model/Models-Train-33/checkpoint-1949/pytorch_model.bin\n",
            "Image processor saved in /content/Model/Models-Train-33/checkpoint-1949/preprocessor_config.json\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.11955020714144801\n",
            "Precision: 0.4789365087847527\n",
            "Recall: 0.14897936328592878\n",
            "F1-Score: 0.19375788781127917\n",
            "\n",
            "Validation at step 2599...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Configuration saved in /content/Model/Models-Train-33/checkpoint-2599/config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 4, Step: 2599, Train Loss: 0.0238, Valid Loss: 0.2321\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Model weights saved in /content/Model/Models-Train-33/checkpoint-2599/pytorch_model.bin\n",
            "Image processor saved in /content/Model/Models-Train-33/checkpoint-2599/preprocessor_config.json\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.12260800946932333\n",
            "Precision: 0.4705158983691629\n",
            "Recall: 0.18566820807652945\n",
            "F1-Score: 0.2431995538063381\n",
            "\n",
            "Validation at step 3249...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Configuration saved in /content/Model/Models-Train-33/checkpoint-3249/config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 5, Step: 3249, Train Loss: 0.0282, Valid Loss: 0.2279\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Model weights saved in /content/Model/Models-Train-33/checkpoint-3249/pytorch_model.bin\n",
            "Image processor saved in /content/Model/Models-Train-33/checkpoint-3249/preprocessor_config.json\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.12704675478398106\n",
            "Precision: 0.57449001964397\n",
            "Recall: 0.2094131012930578\n",
            "F1-Score: 0.27120765577267275\n",
            "\n",
            "Validation at step 3899...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Configuration saved in /content/Model/Models-Train-33/checkpoint-3899/config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 6, Step: 3899, Train Loss: 0.0317, Valid Loss: 0.2289\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Model weights saved in /content/Model/Models-Train-33/checkpoint-3899/pytorch_model.bin\n",
            "Image processor saved in /content/Model/Models-Train-33/checkpoint-3899/preprocessor_config.json\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.12606036693627934\n",
            "Precision: 0.5531723707057783\n",
            "Recall: 0.22605162900060155\n",
            "F1-Score: 0.2831965588110486\n",
            "\n",
            "Validation at step 4549...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Configuration saved in /content/Model/Models-Train-33/checkpoint-4549/config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 7, Step: 4549, Train Loss: 0.0352, Valid Loss: 0.2287\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Model weights saved in /content/Model/Models-Train-33/checkpoint-4549/pytorch_model.bin\n",
            "Image processor saved in /content/Model/Models-Train-33/checkpoint-4549/preprocessor_config.json\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.12823042020122313\n",
            "Precision: 0.5548600329540695\n",
            "Recall: 0.2339788487327246\n",
            "F1-Score: 0.2980587044532595\n",
            "\n",
            "Validation at step 5199...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Configuration saved in /content/Model/Models-Train-33/checkpoint-5199/config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 8, Step: 5199, Train Loss: 0.0378, Valid Loss: 0.2263\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Model weights saved in /content/Model/Models-Train-33/checkpoint-5199/pytorch_model.bin\n",
            "Image processor saved in /content/Model/Models-Train-33/checkpoint-5199/preprocessor_config.json\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.13503649635036497\n",
            "Precision: 0.5287228519241994\n",
            "Recall: 0.24214153016927484\n",
            "F1-Score: 0.30578473283489577\n",
            "\n",
            "Validation at step 5849...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Configuration saved in /content/Model/Models-Train-33/checkpoint-5849/config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 9, Step: 5849, Train Loss: 0.0394, Valid Loss: 0.2288\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Model weights saved in /content/Model/Models-Train-33/checkpoint-5849/pytorch_model.bin\n",
            "Image processor saved in /content/Model/Models-Train-33/checkpoint-5849/preprocessor_config.json\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.12971000197277568\n",
            "Precision: 0.5555278550230737\n",
            "Recall: 0.24390739479802873\n",
            "F1-Score: 0.3117047522620143\n",
            "\n",
            "Validation at step 6499...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Configuration saved in /content/Model/Models-Train-33/checkpoint-6499/config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 10, Step: 6499, Train Loss: 0.0402, Valid Loss: 0.2325\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Model weights saved in /content/Model/Models-Train-33/checkpoint-6499/pytorch_model.bin\n",
            "Image processor saved in /content/Model/Models-Train-33/checkpoint-6499/preprocessor_config.json\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.13237324916157034\n",
            "Precision: 0.5750737030740254\n",
            "Recall: 0.24572844789027146\n",
            "F1-Score: 0.318157115065142\n",
            "\n",
            "Validation at step 7149...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Configuration saved in /content/Model/Models-Train-33/checkpoint-7149/config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 11, Step: 7149, Train Loss: 0.0398, Valid Loss: 0.2357\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Model weights saved in /content/Model/Models-Train-33/checkpoint-7149/pytorch_model.bin\n",
            "Image processor saved in /content/Model/Models-Train-33/checkpoint-7149/preprocessor_config.json\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.12773722627737227\n",
            "Precision: 0.5406516191190365\n",
            "Recall: 0.27402029343368234\n",
            "F1-Score: 0.3368458881499321\n",
            "\n",
            "Validation at step 7799...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Configuration saved in /content/Model/Models-Train-33/checkpoint-7799/config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 12, Step: 7799, Train Loss: 0.0386, Valid Loss: 0.2450\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Model weights saved in /content/Model/Models-Train-33/checkpoint-7799/pytorch_model.bin\n",
            "Image processor saved in /content/Model/Models-Train-33/checkpoint-7799/preprocessor_config.json\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.1262576445058197\n",
            "Precision: 0.560374582918868\n",
            "Recall: 0.24061398045843024\n",
            "F1-Score: 0.3065193385013935\n",
            "\n",
            "Validation at step 8449...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Configuration saved in /content/Model/Models-Train-33/checkpoint-8449/config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 13, Step: 8449, Train Loss: 0.0363, Valid Loss: 0.2512\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Model weights saved in /content/Model/Models-Train-33/checkpoint-8449/pytorch_model.bin\n",
            "Image processor saved in /content/Model/Models-Train-33/checkpoint-8449/preprocessor_config.json\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.11984612349575853\n",
            "Precision: 0.5380087867837452\n",
            "Recall: 0.2596484666337372\n",
            "F1-Score: 0.32348918712334673\n",
            "\n",
            "Validation at step 9099...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Configuration saved in /content/Model/Models-Train-33/checkpoint-9099/config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 14, Step: 9099, Train Loss: 0.0335, Valid Loss: 0.2605\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Model weights saved in /content/Model/Models-Train-33/checkpoint-9099/pytorch_model.bin\n",
            "Image processor saved in /content/Model/Models-Train-33/checkpoint-9099/preprocessor_config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.12408759124087591\n",
            "Precision: 0.5535424022425558\n",
            "Recall: 0.26273099551617884\n",
            "F1-Score: 0.33077008290936305\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-55e1d36d83fb>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m       \u001b[0mloss_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m       \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_v\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}