{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNN6Z9/NrXE73DZbKmxFdu8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MuhammadIrzam447/MultiModel/blob/master/Valid_52.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !gdown https://drive.google.com/uc?id=1kl1NQvbcre7ktBmaxvdZ5fN_EmQQicBL"
      ],
      "metadata": {
        "id": "0av_aZNG0pVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !gdown https://drive.google.com/uc?id=1luEtABDl5_Y1OMVRcuUYXerwoFeiVrjD"
      ],
      "metadata": {
        "id": "ps8bsZi_0nhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip \"/content/mulitmodal_joint_224*224.zip\""
      ],
      "metadata": {
        "id": "Vj0M1c-403JD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kTdeTrGeujZ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31850bde-49f0-4117-852f-50726ab6d4c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.32.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "import torch\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.transforms import transforms\n",
        "from transformers import ViTForImageClassification, ViTFeatureExtractor, AdamW\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix, roc_auc_score, ConfusionMatrixDisplay\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnWhKCGhumd3",
        "outputId": "e6aaa3c7-542a-4e9e-d806-f7f2fd8aa5c8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Validation Dataset\n"
      ],
      "metadata": {
        "id": "iUhE22cMvHHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ViTImageProcessor\n",
        "\n",
        "processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
        "image_mean, image_std = processor.image_mean, processor.image_std\n",
        "size = processor.size[\"height\"]\n",
        "\n",
        "# Define transformations for the input images\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=image_mean, std=image_std)\n",
        "])\n"
      ],
      "metadata": {
        "id": "epGVIaiFvJGG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ValidationDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_dir, val_transform):\n",
        "        self.data_dir = data_dir\n",
        "        self.dataset = datasets.ImageFolder(data_dir)\n",
        "        self.classes = self.dataset.classes\n",
        "        self.val_transform = val_transform\n",
        "\n",
        "        self.selected_indices = []\n",
        "        for class_idx in range(len(self.classes)):\n",
        "            indices = [idx for idx, (_, label) in enumerate(self.dataset.samples) if label == class_idx]\n",
        "            indices_3 = [idx for idx in indices if self.dataset.samples[idx][0].endswith(\"_3.png\")]    # _3.png are the encoded_text images\n",
        "            indices_4 = [idx for idx in indices if self.dataset.samples[idx][0].endswith(\"_4.png\")]    # _4.png are the actual images\n",
        "\n",
        "            self.selected_indices.extend(indices_4)\n",
        "            # self.selected_indices.extend(indices_3)\n",
        "        print(\"Selected Indices:\", len(self.selected_indices))\n",
        "        # for idx in self.selected_indices:\n",
        "          # print(self.dataset.samples[idx][0])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # print(\"Entered get_item\")\n",
        "        img, label = self.dataset[self.selected_indices[index]]\n",
        "        filename = self.dataset.samples[self.selected_indices[index]][0]\n",
        "        image_3 = None\n",
        "        image_4 = None\n",
        "        if filename.endswith(\"_4.png\"):\n",
        "                image_4 = img\n",
        "                image_4_path = filename\n",
        "                # print(\"image_4_path: \", str(filename))\n",
        "                image_3_path = filename.replace('_4.png', '_3.png')\n",
        "                # print(\"image_3_path: \", str(image_3_path))\n",
        "                image_3 = self._load_image(image_3_path)\n",
        "        else:\n",
        "                image_3 = img\n",
        "                image_3_path = filename\n",
        "                # print(\"image_3_path: \", str(filename))\n",
        "                image_4_path = filename.replace('_3.png', '_4.png')\n",
        "                # print(\"image_4_path: \", str(image_4_path))\n",
        "                image_4 = self._load_image(image_4_path)\n",
        "\n",
        "        # print(\"Exit get_item\")\n",
        "        return image_3, image_4, label, image_3_path, image_4_path\n",
        "\n",
        "    def _load_image(self, path):\n",
        "        image = Image.open(path)\n",
        "        return image\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.selected_indices)"
      ],
      "metadata": {
        "id": "UOmFi2bfvJ74"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valPath = \"/content/mulitmodal/test\"\n",
        "val_dataset = ValidationDataset(valPath,transform)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XO7sTkmLvclI",
        "outputId": "decbabc6-383a-4f7d-e1b4-1be1509cd388"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected Indices: 22716\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label2id = {class_name: idx for class_name, idx in val_dataset.dataset.class_to_idx.items()}\n",
        "id2label = {idx: class_name for class_name, idx in val_dataset.dataset.class_to_idx.items()}"
      ],
      "metadata": {
        "id": "XRjPrSC3vpy6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label2id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQRJBXj_1su8",
        "outputId": "f75af875-3b21-4feb-98ec-7b504a31a079"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'apple_pie': 0,\n",
              " 'baby_back_ribs': 1,\n",
              " 'baklava': 2,\n",
              " 'beef_carpaccio': 3,\n",
              " 'beef_tartare': 4,\n",
              " 'beet_salad': 5,\n",
              " 'beignets': 6,\n",
              " 'bibimbap': 7,\n",
              " 'bread_pudding': 8,\n",
              " 'breakfast_burrito': 9,\n",
              " 'bruschetta': 10,\n",
              " 'caesar_salad': 11,\n",
              " 'cannoli': 12,\n",
              " 'caprese_salad': 13,\n",
              " 'carrot_cake': 14,\n",
              " 'ceviche': 15,\n",
              " 'cheese_plate': 16,\n",
              " 'cheesecake': 17,\n",
              " 'chicken_curry': 18,\n",
              " 'chicken_quesadilla': 19,\n",
              " 'chicken_wings': 20,\n",
              " 'chocolate_cake': 21,\n",
              " 'chocolate_mousse': 22,\n",
              " 'churros': 23,\n",
              " 'clam_chowder': 24,\n",
              " 'club_sandwich': 25,\n",
              " 'crab_cakes': 26,\n",
              " 'creme_brulee': 27,\n",
              " 'croque_madame': 28,\n",
              " 'cup_cakes': 29,\n",
              " 'deviled_eggs': 30,\n",
              " 'donuts': 31,\n",
              " 'dumplings': 32,\n",
              " 'edamame': 33,\n",
              " 'eggs_benedict': 34,\n",
              " 'escargots': 35,\n",
              " 'falafel': 36,\n",
              " 'filet_mignon': 37,\n",
              " 'fish_and_chips': 38,\n",
              " 'foie_gras': 39,\n",
              " 'french_fries': 40,\n",
              " 'french_onion_soup': 41,\n",
              " 'french_toast': 42,\n",
              " 'fried_calamari': 43,\n",
              " 'fried_rice': 44,\n",
              " 'frozen_yogurt': 45,\n",
              " 'garlic_bread': 46,\n",
              " 'gnocchi': 47,\n",
              " 'greek_salad': 48,\n",
              " 'grilled_cheese_sandwich': 49,\n",
              " 'grilled_salmon': 50,\n",
              " 'guacamole': 51,\n",
              " 'gyoza': 52,\n",
              " 'hamburger': 53,\n",
              " 'hot_and_sour_soup': 54,\n",
              " 'hot_dog': 55,\n",
              " 'huevos_rancheros': 56,\n",
              " 'hummus': 57,\n",
              " 'ice_cream': 58,\n",
              " 'lasagna': 59,\n",
              " 'lobster_bisque': 60,\n",
              " 'lobster_roll_sandwich': 61,\n",
              " 'macaroni_and_cheese': 62,\n",
              " 'macarons': 63,\n",
              " 'miso_soup': 64,\n",
              " 'mussels': 65,\n",
              " 'nachos': 66,\n",
              " 'omelette': 67,\n",
              " 'onion_rings': 68,\n",
              " 'oysters': 69,\n",
              " 'pad_thai': 70,\n",
              " 'paella': 71,\n",
              " 'pancakes': 72,\n",
              " 'panna_cotta': 73,\n",
              " 'peking_duck': 74,\n",
              " 'pho': 75,\n",
              " 'pizza': 76,\n",
              " 'pork_chop': 77,\n",
              " 'poutine': 78,\n",
              " 'prime_rib': 79,\n",
              " 'pulled_pork_sandwich': 80,\n",
              " 'ramen': 81,\n",
              " 'ravioli': 82,\n",
              " 'red_velvet_cake': 83,\n",
              " 'risotto': 84,\n",
              " 'samosa': 85,\n",
              " 'sashimi': 86,\n",
              " 'scallops': 87,\n",
              " 'seaweed_salad': 88,\n",
              " 'shrimp_and_grits': 89,\n",
              " 'spaghetti_bolognese': 90,\n",
              " 'spaghetti_carbonara': 91,\n",
              " 'spring_rolls': 92,\n",
              " 'steak': 93,\n",
              " 'strawberry_shortcake': 94,\n",
              " 'sushi': 95,\n",
              " 'tacos': 96,\n",
              " 'takoyaki': 97,\n",
              " 'tiramisu': 98,\n",
              " 'tuna_tartare': 99,\n",
              " 'waffles': 100}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "id2label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cucoooer1upH",
        "outputId": "35153512-d0a0-4ba3-81de-5c0152f720db"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'apple_pie',\n",
              " 1: 'baby_back_ribs',\n",
              " 2: 'baklava',\n",
              " 3: 'beef_carpaccio',\n",
              " 4: 'beef_tartare',\n",
              " 5: 'beet_salad',\n",
              " 6: 'beignets',\n",
              " 7: 'bibimbap',\n",
              " 8: 'bread_pudding',\n",
              " 9: 'breakfast_burrito',\n",
              " 10: 'bruschetta',\n",
              " 11: 'caesar_salad',\n",
              " 12: 'cannoli',\n",
              " 13: 'caprese_salad',\n",
              " 14: 'carrot_cake',\n",
              " 15: 'ceviche',\n",
              " 16: 'cheese_plate',\n",
              " 17: 'cheesecake',\n",
              " 18: 'chicken_curry',\n",
              " 19: 'chicken_quesadilla',\n",
              " 20: 'chicken_wings',\n",
              " 21: 'chocolate_cake',\n",
              " 22: 'chocolate_mousse',\n",
              " 23: 'churros',\n",
              " 24: 'clam_chowder',\n",
              " 25: 'club_sandwich',\n",
              " 26: 'crab_cakes',\n",
              " 27: 'creme_brulee',\n",
              " 28: 'croque_madame',\n",
              " 29: 'cup_cakes',\n",
              " 30: 'deviled_eggs',\n",
              " 31: 'donuts',\n",
              " 32: 'dumplings',\n",
              " 33: 'edamame',\n",
              " 34: 'eggs_benedict',\n",
              " 35: 'escargots',\n",
              " 36: 'falafel',\n",
              " 37: 'filet_mignon',\n",
              " 38: 'fish_and_chips',\n",
              " 39: 'foie_gras',\n",
              " 40: 'french_fries',\n",
              " 41: 'french_onion_soup',\n",
              " 42: 'french_toast',\n",
              " 43: 'fried_calamari',\n",
              " 44: 'fried_rice',\n",
              " 45: 'frozen_yogurt',\n",
              " 46: 'garlic_bread',\n",
              " 47: 'gnocchi',\n",
              " 48: 'greek_salad',\n",
              " 49: 'grilled_cheese_sandwich',\n",
              " 50: 'grilled_salmon',\n",
              " 51: 'guacamole',\n",
              " 52: 'gyoza',\n",
              " 53: 'hamburger',\n",
              " 54: 'hot_and_sour_soup',\n",
              " 55: 'hot_dog',\n",
              " 56: 'huevos_rancheros',\n",
              " 57: 'hummus',\n",
              " 58: 'ice_cream',\n",
              " 59: 'lasagna',\n",
              " 60: 'lobster_bisque',\n",
              " 61: 'lobster_roll_sandwich',\n",
              " 62: 'macaroni_and_cheese',\n",
              " 63: 'macarons',\n",
              " 64: 'miso_soup',\n",
              " 65: 'mussels',\n",
              " 66: 'nachos',\n",
              " 67: 'omelette',\n",
              " 68: 'onion_rings',\n",
              " 69: 'oysters',\n",
              " 70: 'pad_thai',\n",
              " 71: 'paella',\n",
              " 72: 'pancakes',\n",
              " 73: 'panna_cotta',\n",
              " 74: 'peking_duck',\n",
              " 75: 'pho',\n",
              " 76: 'pizza',\n",
              " 77: 'pork_chop',\n",
              " 78: 'poutine',\n",
              " 79: 'prime_rib',\n",
              " 80: 'pulled_pork_sandwich',\n",
              " 81: 'ramen',\n",
              " 82: 'ravioli',\n",
              " 83: 'red_velvet_cake',\n",
              " 84: 'risotto',\n",
              " 85: 'samosa',\n",
              " 86: 'sashimi',\n",
              " 87: 'scallops',\n",
              " 88: 'seaweed_salad',\n",
              " 89: 'shrimp_and_grits',\n",
              " 90: 'spaghetti_bolognese',\n",
              " 91: 'spaghetti_carbonara',\n",
              " 92: 'spring_rolls',\n",
              " 93: 'steak',\n",
              " 94: 'strawberry_shortcake',\n",
              " 95: 'sushi',\n",
              " 96: 'tacos',\n",
              " 97: 'takoyaki',\n",
              " 98: 'tiramisu',\n",
              " 99: 'tuna_tartare',\n",
              " 100: 'waffles'}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_collate(batch):\n",
        "    to_tensor = transforms.ToTensor()\n",
        "    # Define transformations for the input images\n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.Resize(size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=image_mean, std=image_std)\n",
        "    ])\n",
        "\n",
        "    images_3 = [val_transform(item[0]) for item in batch]\n",
        "    images_4 = [val_transform(item[1]) for item in batch]\n",
        "    labels = [torch.tensor(item[2]) for item in batch]\n",
        "\n",
        "    images_3 = torch.stack(images_3)\n",
        "    images_4 = torch.stack(images_4)\n",
        "\n",
        "    # Print the filenames in each list\n",
        "    # print(\"Filenames in images_3 list:\")\n",
        "    # for item in batch:\n",
        "    #     if item[0] is not None:\n",
        "    #         print(item[3])\n",
        "\n",
        "    # print(\"Filenames in images_4 list:\")\n",
        "    # for item in batch:\n",
        "    #     if item[1] is not None:\n",
        "    #         print(item[4])\n",
        "\n",
        "    return images_3, images_4, labels"
      ],
      "metadata": {
        "id": "jGwgjNM0wGa9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "num_workers = 1\n",
        "\n",
        "validation_data_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, collate_fn=custom_collate)"
      ],
      "metadata": {
        "id": "u4Ui6M_wv0Cz"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Model"
      ],
      "metadata": {
        "id": "wopXDv9WvIGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = len(val_dataset.classes)\n",
        "print(num_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FF9QV6fWvCb6",
        "outputId": "066578c5-c317-4b68-c00f-60c180ca5048"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_dir = '/content/'\n",
        "load_path = os.path.join(save_dir, 'fused_model.pth')\n",
        "\n",
        "vit = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224\", num_labels=num_classes, id2label=id2label, label2id=label2id, ignore_mismatched_sizes=True)\n",
        "vit.load_state_dict(torch.load(load_path))\n",
        "\n",
        "vit.to(device)\n",
        "print(vit)"
      ],
      "metadata": {
        "id": "LOCfX7jGuyVT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36a88cb9-18b3-482a-b36c-08deaa72aa23"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
            "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([101, 768]) in the model instantiated\n",
            "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([101]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ViTForImageClassification(\n",
            "  (vit): ViTModel(\n",
            "    (embeddings): ViTEmbeddings(\n",
            "      (patch_embeddings): ViTPatchEmbeddings(\n",
            "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
            "      )\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (encoder): ViTEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0-11): 12 x ViTLayer(\n",
            "          (attention): ViTAttention(\n",
            "            (attention): ViTSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (output): ViTSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): ViTIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): ViTOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "  )\n",
            "  (classifier): Linear(in_features=768, out_features=101, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Model"
      ],
      "metadata": {
        "id": "_mCt4PEEyWDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of samples:\", len(val_dataset))\n",
        "print(\"Number of classes:\", len(val_dataset.classes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2AjomtNyWfQ",
        "outputId": "21c9a985-ca34-4c4f-c3c8-0e091b7d76a4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples: 22716\n",
            "Number of classes: 101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_batches = len(validation_data_loader)\n",
        "print(\"Number of batches:\", num_batches)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9S8qI33yyfbE",
        "outputId": "32c94a28-0c4c-46c1-9155-f41240e347e8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of batches: 1420\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_batches = len(validation_data_loader)\n",
        "num_batches_with_logits_3 = int(num_batches * 0.5)  # __% of batches that you want to include (30 for now)\n",
        "print(\"Total Batches: \", num_batches)\n",
        "print(\"Missing Modaility Batches: \", num_batches_with_logits_3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yl606Ykz5C0m",
        "outputId": "75dd92ef-bf2f-4f31-a88d-7d4ad38d8bc4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Batches:  1420\n",
            "Missing Modaility Batches:  710\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\"\"\n",
        "1. 100% Text Missing (Image-only Validation)\n",
        "2. 100% Image Missing (Text-only Validation)\n",
        "3. Missing Modality Check\"\"\")\n",
        "method = str(input(\"Choose method: \"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vk6zEUs85Ics",
        "outputId": "fbe7523f-235f-42dc-d185-3e677eafadb3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "1. 100% Text Missing (Image-only Validation)\n",
            "2. 100% Image Missing (Text-only Validation)\n",
            "3. Missing Modality Check\n",
            "Choose method: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vit.eval()\n",
        "\n",
        "# Initialize lists to store the average probabilities and true labels\n",
        "predicted_labels = []\n",
        "true_labels = []\n",
        "\n",
        "\n",
        "batch_idx = 0\n",
        "for images_3, images_4, labels in validation_data_loader:\n",
        "\n",
        "    if method == \"1\":\n",
        "      # print(\"M1\")\n",
        "      images_4 = images_4.to(device)\n",
        "      logits_4 = vit(images_4).logits\n",
        "      logits_3 = torch.zeros_like(logits_4)\n",
        "\n",
        "    elif method == \"2\":\n",
        "      # print(\"M2\")\n",
        "      images_3 = images_3.to(device)\n",
        "      logits_3 = vit(images_3).logits\n",
        "      logits_4 = torch.zeros_like(logits_3)\n",
        "\n",
        "    elif method == \"3\":\n",
        "      # print(\"M3\")\n",
        "      images_4 = images_4.to(device)\n",
        "      logits_4 = vit(images_4).logits\n",
        "      if batch_idx < num_batches_with_logits_3:\n",
        "          images_3 = images_3.to(device)\n",
        "          logits_3 = vit(images_3).logits\n",
        "      else:\n",
        "          logits_3 = torch.zeros_like(logits_4)\n",
        "    else:\n",
        "      print(\"Invalid Choice!\")\n",
        "      break\n",
        "\n",
        "    probabilities_3 = torch.softmax(logits_3, dim=1)\n",
        "    probabilities_4 = torch.softmax(logits_4, dim=1)\n",
        "\n",
        "    avg_probabilities_batch = (probabilities_3 + probabilities_4) / 2\n",
        "\n",
        "    predicted_labels.extend(avg_probabilities_batch.cpu().tolist())\n",
        "    true_labels.extend(labels)\n",
        "    batch_idx += 1\n",
        "\n",
        "print(batch_idx)"
      ],
      "metadata": {
        "id": "vdjakhiUykzy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7d5343f-3baf-492a-d184-73e0ddcf572c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1420\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_classes = torch.argmax(torch.tensor(predicted_labels), dim=1)\n",
        "actual_labels = torch.tensor(true_labels)"
      ],
      "metadata": {
        "id": "TNHHECsGzJJd"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(actual_labels, predicted_classes)\n",
        "precision = precision_score(actual_labels, predicted_classes, average='weighted')\n",
        "recall = recall_score(actual_labels, predicted_classes, average='weighted')\n",
        "f1 = f1_score(actual_labels, predicted_classes, average='weighted')\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1v8UK6JUzPPW",
        "outputId": "67cd822e-cfa0-418a-98fb-ac12a75f7fb6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7072107765451664\n",
            "Precision: 0.7398034204517053\n",
            "Recall: 0.7072107765451664\n",
            "F1-score: 0.7093603081906448\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(actual_labels, predicted_classes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kpqf3kudzRjJ",
        "outputId": "7cc4bc21-68b4-4717-9147-2f03ca022b01"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.59      0.70       234\n",
            "           1       0.82      0.74      0.78       221\n",
            "           2       0.84      0.86      0.85       226\n",
            "           3       0.72      0.71      0.72       222\n",
            "           4       0.64      0.51      0.56       225\n",
            "           5       0.73      0.85      0.78       224\n",
            "           6       0.74      0.74      0.74       224\n",
            "           7       0.86      0.63      0.73       225\n",
            "           8       0.86      0.47      0.61       226\n",
            "           9       0.72      0.69      0.70       214\n",
            "          10       0.71      0.75      0.73       231\n",
            "          11       0.85      0.86      0.85       227\n",
            "          12       0.90      0.70      0.78       230\n",
            "          13       0.87      0.83      0.85       220\n",
            "          14       0.69      0.65      0.67       231\n",
            "          15       0.85      0.65      0.74       227\n",
            "          16       0.60      0.62      0.61       224\n",
            "          17       0.71      0.62      0.67       233\n",
            "          18       0.67      0.83      0.74       222\n",
            "          19       0.53      0.75      0.62       220\n",
            "          20       0.88      0.90      0.89       219\n",
            "          21       0.70      0.84      0.76       232\n",
            "          22       0.79      0.62      0.70       224\n",
            "          23       0.91      0.67      0.77       230\n",
            "          24       0.86      0.69      0.76       224\n",
            "          25       0.87      0.71      0.78       220\n",
            "          26       0.62      0.82      0.70       221\n",
            "          27       0.39      0.92      0.54       225\n",
            "          28       0.82      0.83      0.82       224\n",
            "          29       0.70      0.62      0.66       228\n",
            "          30       0.95      0.96      0.95       229\n",
            "          31       0.85      0.69      0.76       232\n",
            "          32       0.67      0.43      0.53       228\n",
            "          33       0.58      0.82      0.68       231\n",
            "          34       0.91      0.83      0.86       213\n",
            "          35       0.68      0.60      0.64       202\n",
            "          36       0.85      0.71      0.77       228\n",
            "          37       0.52      0.76      0.62       212\n",
            "          38       0.59      0.70      0.64       208\n",
            "          39       0.76      0.48      0.59       216\n",
            "          40       0.80      0.72      0.76       212\n",
            "          41       0.60      0.89      0.72       223\n",
            "          42       0.64      0.75      0.69       238\n",
            "          43       0.79      0.71      0.75       212\n",
            "          44       0.70      0.78      0.74       229\n",
            "          45       0.73      0.48      0.58       233\n",
            "          46       0.48      0.83      0.61       224\n",
            "          47       0.73      0.59      0.65       234\n",
            "          48       0.81      0.85      0.83       223\n",
            "          49       0.75      0.84      0.79       229\n",
            "          50       0.80      0.77      0.79       225\n",
            "          51       0.87      0.84      0.85       228\n",
            "          52       0.71      0.57      0.63       232\n",
            "          53       0.67      0.45      0.54       233\n",
            "          54       0.73      0.54      0.62       203\n",
            "          55       0.45      0.10      0.16       234\n",
            "          56       0.88      0.54      0.67       223\n",
            "          57       0.81      0.73      0.77       234\n",
            "          58       0.58      0.49      0.53       232\n",
            "          59       0.85      0.82      0.83       233\n",
            "          60       0.59      0.66      0.63       197\n",
            "          61       0.30      0.60      0.40       215\n",
            "          62       0.70      0.71      0.71       238\n",
            "          63       0.91      0.82      0.86       231\n",
            "          64       0.73      0.72      0.73       227\n",
            "          65       0.85      0.90      0.87       234\n",
            "          66       0.86      0.80      0.83       224\n",
            "          67       0.82      0.78      0.80       231\n",
            "          68       0.85      0.79      0.82       217\n",
            "          69       0.56      0.66      0.61       223\n",
            "          70       0.78      0.78      0.78       230\n",
            "          71       0.77      0.87      0.82       228\n",
            "          72       0.74      0.80      0.77       234\n",
            "          73       0.82      0.76      0.79       225\n",
            "          74       0.35      0.66      0.46       211\n",
            "          75       0.72      0.80      0.76       218\n",
            "          76       0.78      0.77      0.78       235\n",
            "          77       0.61      0.64      0.62       227\n",
            "          78       0.70      0.75      0.73       210\n",
            "          79       0.95      0.65      0.77       220\n",
            "          80       0.90      0.76      0.83       224\n",
            "          81       0.59      0.60      0.60       232\n",
            "          82       0.76      0.60      0.67       236\n",
            "          83       0.83      0.76      0.80       227\n",
            "          84       0.78      0.69      0.73       236\n",
            "          85       0.80      0.67      0.73       215\n",
            "          86       0.25      0.65      0.36       226\n",
            "          87       0.62      0.73      0.67       228\n",
            "          88       0.72      0.79      0.75       219\n",
            "          89       0.94      0.58      0.72       228\n",
            "          90       0.85      0.89      0.87       213\n",
            "          91       0.87      0.83      0.85       218\n",
            "          92       0.73      0.87      0.79       230\n",
            "          93       0.80      0.15      0.26       229\n",
            "          94       0.71      0.87      0.78       223\n",
            "          95       0.85      0.54      0.66       237\n",
            "          96       0.87      0.62      0.73       233\n",
            "          97       0.61      0.77      0.68       223\n",
            "          98       0.89      0.78      0.83       232\n",
            "          99       0.49      0.66      0.56       211\n",
            "         100       0.90      0.80      0.84       235\n",
            "\n",
            "    accuracy                           0.71     22716\n",
            "   macro avg       0.74      0.71      0.71     22716\n",
            "weighted avg       0.74      0.71      0.71     22716\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# auroc = roc_auc_score(actual_labels, predicted_classes)\n",
        "# print(\"AUROC:\", auroc)"
      ],
      "metadata": {
        "id": "S8YkNNV2zWAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cm = confusion_matrix(actual_labels, predicted_classes)\n",
        "# print(cm)"
      ],
      "metadata": {
        "id": "dSdc2-dXziFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = ['Accuracy', 'Precision', 'Recall', 'F1-score']\n",
        "values = [accuracy, precision, recall, f1]\n",
        "\n",
        "x = np.arange(len(labels))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(5, 4))\n",
        "bars = ax.bar(x, values)\n",
        "\n",
        "# labels, title, and legend\n",
        "ax.set_xlabel('Metrics')\n",
        "ax.set_ylabel('Score')\n",
        "ax.set_title('Model Performance Metrics')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(labels)\n",
        "\n",
        "# scores on top of each bar\n",
        "for i, bar in enumerate(bars):\n",
        "    score = values[i]\n",
        "    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), f'{score:.2f}', ha='center', va='bottom')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8nkq1Wekzlfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn import metrics\n",
        "# fpr, tpr, _ = metrics.roc_curve(actual_labels,  predicted_classes)\n",
        "# plt.plot(fpr,tpr)\n",
        "# plt.ylabel('True Positive Rate')\n",
        "# plt.xlabel('False Positive Rate')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "iKfaELj5zX3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u2QM2s2d4RIX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}