{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "p2-Au_0xa3mq"
      ],
      "authorship_tag": "ABX9TyN+ZdScI14cJCrGneQZNZTK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MuhammadIrzam447/MultiModel/blob/master/Train_30.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "p2-Au_0xa3mq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from datasets import Dataset\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
      ],
      "metadata": {
        "id": "03AUO6dj9ssM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HfQ6Yxb6JyQ"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"imagefolder\", data_dir=\"/content/Dataset(s)/jointHF-train+test_unseen/train\", split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "def filter_funtion(example):\n",
        "    img = example[\"image\"]\n",
        "    filename = os.path.basename(img.filename)\n",
        "    return filename.endswith(\"_4.png\")\n",
        "\n",
        "def filter_funtion1(example):\n",
        "    img = example[\"image\"]\n",
        "    filename = os.path.basename(img.filename)\n",
        "    return filename.endswith(\"_3.png\")"
      ],
      "metadata": {
        "id": "TyS41Rux7ZsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_imgs = ds.filter(filter_funtion)"
      ],
      "metadata": {
        "id": "OniagCm57eca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_ds = ds.filter(filter_funtion1)"
      ],
      "metadata": {
        "id": "G_ZA7GzM7i_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_samples = len(filtered_ds)\n",
        "desired_samples = int(total_samples * 0.3)\n",
        "\n",
        "ds_encoded = filtered_ds.shuffle(seed=42).select([i for i in range(desired_samples)])"
      ],
      "metadata": {
        "id": "HeMN2h3lDSYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_encoded"
      ],
      "metadata": {
        "id": "tkRy7WCgDZJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import concatenate_datasets\n",
        "ds_train = concatenate_datasets([ds_imgs, ds_encoded])"
      ],
      "metadata": {
        "id": "qtp0yTIl_5NB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_train"
      ],
      "metadata": {
        "id": "PLeItkHbD_wH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\n",
        "def apply_transform(example):\n",
        "    image = Image.open(example[\"image\"].filename).convert(\"RGB\")  # Ensure the image is in RGB mode\n",
        "    image = transform(image)\n",
        "    return {\"image\": image, \"label\": example[\"label\"]}"
      ],
      "metadata": {
        "id": "XOtglMPzIH5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_transformed = ds_train.map(apply_transform)"
      ],
      "metadata": {
        "id": "agOj9LVaIP0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "train_loader = DataLoader(ds_train, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "ZqL7ztOhEAj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = ds_train.features[\"label\"].names\n",
        "labels"
      ],
      "metadata": {
        "id": "m70yhqToF9EJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = len(labels)"
      ],
      "metadata": {
        "id": "QyldaP7-GUre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "\n",
        "resnet = models.resnet101(pretrained=True)\n",
        "num_features = resnet.fc.in_features\n",
        "resnet.fc = nn.Linear(num_features, num_classes)  # num_classes is the number of classes in your dataset\n",
        "resnet.to(device)\n",
        "print(resnet)\n"
      ],
      "metadata": {
        "id": "HKTn48hsGX1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(resnet.parameters(), lr=0.001, momentum=0.9)\n",
        "num_epochs = 20"
      ],
      "metadata": {
        "id": "7xuQPnogGxql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_loss = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Set the model to training mode\n",
        "    resnet.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    # Iterate over the data loader\n",
        "    for images, labels in train_loader:\n",
        "        # Move the images and labels to the GPU if available\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Clear the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = resnet(images)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the running loss\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "    # Calculate the average loss for the epoch\n",
        "    epoch_loss = running_loss / len(data_loader.dataset)\n",
        "    training_loss.append(epoch_loss)\n",
        "    # Print the epoch loss\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} Training Loss: {epoch_loss:.4f}\")"
      ],
      "metadata": {
        "id": "VsZXSx9-HBB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EuHPsa3FHFBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "jNTJQ9CCMOuR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "\n",
        "# # Define the path to your dataset folder\n",
        "# dataset_folder = '/content/Dataset(s)/jointHF-train+test_unseen/test'\n",
        "\n",
        "# # Define the path for the output text file\n",
        "# output_file = 'test_dataset(2).txt'\n",
        "\n",
        "# # Open the output file for writing\n",
        "# with open(output_file, 'w') as f:\n",
        "#     # Iterate through the subdirectories in the dataset folder\n",
        "#     for label in os.listdir(dataset_folder):\n",
        "#         label_path = os.path.join(dataset_folder, label)\n",
        "\n",
        "#         # Check if it's a directory (class folder)\n",
        "#         if os.path.isdir(label_path):\n",
        "#             # Get the label (class name) from the folder name\n",
        "#             class_name = label\n",
        "\n",
        "#             # Iterate through the images in the class folder\n",
        "#             for image_file in os.listdir(label_path):\n",
        "#                 image_path = os.path.join(label_path, image_file)\n",
        "\n",
        "#                 # Write the image path and label to the output file\n",
        "#                 f.write(f'{image_path}, {class_name}\\n')\n"
      ],
      "metadata": {
        "id": "3ScHa74gfAXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Dataset"
      ],
      "metadata": {
        "id": "XNma768QgFfU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "image_file_paths = []\n",
        "image_genre_labels = []\n",
        "enc_file_paths = []\n",
        "enc_genre_labels = []\n",
        "\n",
        "labels_file = \"/content/Dataset(s)/jointHF-train+test_unseen/train_dataset(2).txt\"\n",
        "\n",
        "with open(labels_file, 'r') as file:\n",
        "    for line in file:\n",
        "        parts = line.strip().split(', ')\n",
        "        filename = parts[0].strip()\n",
        "        labels = parts[1].strip()\n",
        "\n",
        "        if filename.endswith(\"_3.png\"):\n",
        "            enc_file_paths.append(filename)\n",
        "            enc_genre_labels.append(labels)\n",
        "\n",
        "        elif filename.endswith(\"_4.png\"):\n",
        "            image_file_paths.append(filename)\n",
        "            image_genre_labels.append(labels)"
      ],
      "metadata": {
        "id": "OEjnk0W-Pc8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(image_file_paths), len(enc_file_paths)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmYmWXRJRaXy",
        "outputId": "0c9bcfba-860f-4068-e6cc-d745156b01fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9500, 9500)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_file_paths[0:5] , image_genre_labels[0:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwLJAYq6dhLs",
        "outputId": "2ce1c9e0-0c47-4535-cb1b-d589d3b732db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['/content/Dataset(s)/jointHF-train+test_unseen/train/1/10547.jpg_4.png',\n",
              "  '/content/Dataset(s)/jointHF-train+test_unseen/train/1/29380.jpg_4.png',\n",
              "  '/content/Dataset(s)/jointHF-train+test_unseen/train/1/53927.jpg_4.png',\n",
              "  '/content/Dataset(s)/jointHF-train+test_unseen/train/1/23485.jpg_4.png',\n",
              "  '/content/Dataset(s)/jointHF-train+test_unseen/train/1/26495.jpg_4.png'],\n",
              " ['1', '1', '1', '1', '1'])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enc_file_paths[0:5] , enc_genre_labels[0:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxClniG0fpwN",
        "outputId": "e59ecd2d-b9d9-4de9-dbad-231dcfe8563d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['/content/Dataset(s)/jointHF-train+test_unseen/train/1/70623.jpg_3.png',\n",
              "  '/content/Dataset(s)/jointHF-train+test_unseen/train/1/25314.jpg_3.png',\n",
              "  '/content/Dataset(s)/jointHF-train+test_unseen/train/1/08637.jpg_3.png',\n",
              "  '/content/Dataset(s)/jointHF-train+test_unseen/train/1/72509.jpg_3.png',\n",
              "  '/content/Dataset(s)/jointHF-train+test_unseen/train/1/45802.jpg_3.png'],\n",
              " ['1', '1', '1', '1', '1'])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_labels = set(enc_genre_labels)\n",
        "unique_labels_list = list(unique_labels)\n",
        "\n",
        "print(\"Unique Labels:\", unique_labels_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slLNEQBQS-i-",
        "outputId": "b693e52c-01ba-4d0f-a85c-e40fb8ecbfe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique Labels: ['1', '0']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = len(unique_labels_list)"
      ],
      "metadata": {
        "id": "HX2Qa1xQZu6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "\n",
        "percentage = 0.3\n",
        "\n",
        "label_counts = {label: 0 for label in unique_labels_list}\n",
        "\n",
        "num_samples_to_include_per_label = {}\n",
        "for label in unique_labels_list:\n",
        "    label_count = enc_genre_labels.count(label)\n",
        "    num_samples_to_include = int(label_count * percentage)\n",
        "    num_samples_to_include_per_label[label] = num_samples_to_include\n",
        "\n",
        "\n",
        "filtered_enc_file_paths = []\n",
        "filtered_enc_genre_labels = []\n",
        "\n",
        "\n",
        "selected_samples_per_label = {label: 0 for label in unique_labels_list}\n",
        "for path, label in zip(enc_file_paths, enc_genre_labels):\n",
        "    if selected_samples_per_label[label] < num_samples_to_include_per_label[label]:\n",
        "        filtered_enc_file_paths.append(path)\n",
        "        filtered_enc_genre_labels.append(label)\n",
        "        selected_samples_per_label[label] += 1"
      ],
      "metadata": {
        "id": "Q9xQzx8MV3_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(filtered_enc_file_paths)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JgyuBkqf45r",
        "outputId": "16e96951-cc5f-42fe-9382-f311f9fd60f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2850"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new lists to combine data from both lists\n",
        "train_file_paths = enc_file_paths + filtered_enc_file_paths\n",
        "train_genre_labels = enc_genre_labels + filtered_enc_genre_labels"
      ],
      "metadata": {
        "id": "2WTpdvsOXRgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_file_paths)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72g7sZu8gAXW",
        "outputId": "9d169923-431d-4be3-de29-3fa4b1336cdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12350"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_hot_labels = []\n",
        "\n",
        "for labels in train_genre_labels:\n",
        "    uni_hot = [1 if label in labels else 0 for label in unique_labels_list]\n",
        "    train_hot_labels.append(uni_hot)"
      ],
      "metadata": {
        "id": "I8XxxUIQNc1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_hot_labels)"
      ],
      "metadata": {
        "id": "2fwrmOWqN16E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_hot_labels[0:5]"
      ],
      "metadata": {
        "id": "X4VrRPB0O4ZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Dataset"
      ],
      "metadata": {
        "id": "7D26BNoJbJ3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "test_image_file_paths = []\n",
        "test_image_genre_labels = []\n",
        "test_enc_file_paths = []\n",
        "test_enc_genre_labels = []\n",
        "\n",
        "test_labels_file = \"/content/Dataset(s)/jointHF-train+test_unseen/test_dataset(2).txt\"\n",
        "\n",
        "with open(test_labels_file, 'r') as file:\n",
        "    for line in file:\n",
        "        parts = line.strip().split(', ')\n",
        "        filename = parts[0].strip()\n",
        "        labels = parts[1].strip()\n",
        "\n",
        "        if filename.endswith(\"_3.png\"):\n",
        "            test_enc_file_paths.append(filename)\n",
        "            test_enc_genre_labels.append(labels)\n",
        "\n",
        "        elif filename.endswith(\"_4.png\"):\n",
        "            test_image_file_paths.append(filename)\n",
        "            test_image_genre_labels.append(labels)"
      ],
      "metadata": {
        "id": "LnGxlWzqbLIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_image_file_paths), len(test_enc_file_paths)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yd9_b3EfbLdS",
        "outputId": "78c6d6be-3607-459e-db89-770572c560ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 2000)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_image_file_paths[0:5] , test_image_genre_labels[0:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kj5sNm7ngOhz",
        "outputId": "07fedd8e-299d-423c-98e8-1c9d04ce16fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['/content/Dataset(s)/jointHF-train+test_unseen/test/1/95876.jpg_4.png',\n",
              "  '/content/Dataset(s)/jointHF-train+test_unseen/test/1/65803.jpg_4.png',\n",
              "  '/content/Dataset(s)/jointHF-train+test_unseen/test/1/02564.jpg_4.png',\n",
              "  '/content/Dataset(s)/jointHF-train+test_unseen/test/1/81329.jpg_4.png',\n",
              "  '/content/Dataset(s)/jointHF-train+test_unseen/test/1/59814.jpg_4.png'],\n",
              " ['1', '1', '1', '1', '1'])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "label_counts = {label: 0 for label in unique_labels_list}\n",
        "\n",
        "test_num_samples_to_include_per_label = {}\n",
        "for label in unique_labels_list:\n",
        "    label_count = test_enc_genre_labels.count(label)\n",
        "    num_samples_to_include = int(label_count * percentage)\n",
        "    test_num_samples_to_include_per_label[label] = num_samples_to_include\n",
        "\n",
        "\n",
        "test_filtered_enc_file_paths = []\n",
        "test_filtered_enc_genre_labels = []\n",
        "\n",
        "\n",
        "test_selected_samples_per_label = {label: 0 for label in unique_labels_list}\n",
        "for path, label in zip(test_enc_file_paths, test_enc_genre_labels):\n",
        "    if test_selected_samples_per_label[label] < test_num_samples_to_include_per_label[label]:\n",
        "        test_filtered_enc_file_paths.append(path)\n",
        "        test_filtered_enc_genre_labels.append(label)\n",
        "        test_selected_samples_per_label[label] += 1"
      ],
      "metadata": {
        "id": "rHhvWppqbLxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_filtered_enc_file_paths)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZplC_8mxgfng",
        "outputId": "c9dc9ccf-f70b-4628-f070-49c14d1a2520"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "600"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new lists to combine data from both lists\n",
        "test_file_paths = test_enc_file_paths + test_filtered_enc_file_paths\n",
        "test_genre_labels = test_enc_genre_labels + test_filtered_enc_genre_labels"
      ],
      "metadata": {
        "id": "C6sjf4Ptbcdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_file_paths)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0OH11Z2OJ_k",
        "outputId": "41ff33f9-8d40-45e3-bedd-a6e7f7ac38aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2600"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_hot_labels = []\n",
        "\n",
        "for labels in test_genre_labels:\n",
        "    uni_hot = [1 if label in labels else 0 for label in unique_labels_list]\n",
        "    test_hot_labels.append(uni_hot)"
      ],
      "metadata": {
        "id": "bzCnp6wKN8tY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_hot_labels)"
      ],
      "metadata": {
        "id": "c13D5quZOCxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_hot_labels[0:5]"
      ],
      "metadata": {
        "id": "JBrg2RIdPy0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use Dataset"
      ],
      "metadata": {
        "id": "aE_wxyh0bKpc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, file_paths, labels, transform=None):\n",
        "        self.file_paths = file_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.file_paths[idx]\n",
        "        image = Image.open(img_path)\n",
        "        label = int(self.labels[idx])\n",
        "        # label = torch.tensor(label,dtype=torch.float32)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "3Y7VXqUBYf_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CustomDataset(train_file_paths, train_genre_labels, transform=transform)\n",
        "test_dataset = CustomDataset(test_file_paths, test_genre_labels, transform=transform)"
      ],
      "metadata": {
        "id": "CjVU94XRZOXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 32  # Adjust as needed\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "niKEq0PUZVan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "resnet = models.resnet101(pretrained=True)\n",
        "num_features = resnet.fc.in_features\n",
        "resnet.fc = nn.Linear(num_features, num_classes)  # num_classes is the number of classes in your dataset\n",
        "resnet.to(device)\n",
        "print(resnet)\n"
      ],
      "metadata": {
        "id": "WkU78f5UZXqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.SGD(resnet.parameters(), lr=0.001, momentum=0.9)\n",
        "optimizer = optim.Adam(resnet.parameters(), lr=0.001)\n",
        "num_epochs = 20"
      ],
      "metadata": {
        "id": "kQnCFcJCZx5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, roc_auc_score\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "6Q79jKkChops"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_loss = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    resnet.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = resnet(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    training_loss.append(epoch_loss)\n",
        "\n",
        "    print(\"Training Loss==========================>>\")\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} Training Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    resnet.eval()\n",
        "    predicted_classes = []\n",
        "    actual_labels = []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = resnet(images)\n",
        "\n",
        "            _, predicted_label = torch.max(outputs, 1)\n",
        "\n",
        "            predicted_classes.extend(predicted_label.cpu().tolist())\n",
        "            actual_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "    accuracy = accuracy_score(actual_labels, predicted_classes)\n",
        "    # precision = precision_score(actual_labels, predicted_classes, average='weighted')\n",
        "    # recall = recall_score(actual_labels, predicted_classes, average='weighted')\n",
        "    # f1 = f1_score(actual_labels, predicted_classes, average='weighted')\n",
        "\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    # print(\"Precision:\", precision)\n",
        "    # print(\"Recall:\", recall)\n",
        "    # print(\"F1-score:\", f1)\n",
        "\n",
        "    print(classification_report(actual_labels, predicted_classes))\n",
        "    # print(\"Confusion Matrix: \")\n",
        "    # cm = confusion_matrix(actual_labels, predicted_classes)\n",
        "    # print(cm)\n",
        "    predicted_classes = np.array(predicted_classes)\n",
        "    actual_labels = np.array(actual_labels)\n",
        "    print(roc_auc_score(actual_labels, predicted_classes))\n",
        "\n",
        "\n",
        "    save_dir = \"/content/Model/Models-Train-27/\"\n",
        "    os.makedirs(save_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
        "\n",
        "    model_name = str(epoch+1) + \"_model.pth\"\n",
        "    save_path = os.path.join(save_dir, model_name)  # Specify the complete path to the model file\n",
        "    torch.save(resnet.state_dict(), save_path)"
      ],
      "metadata": {
        "id": "X70EPWQbZ_id",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ba5175d-db9b-4ccb-ffc1-497a5466064d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss==========================>>\n",
            "Epoch 1/20 Training Loss: 0.6645\n",
            "Accuracy: 0.6115384615384616\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.86      0.73      1625\n",
            "           1       0.46      0.20      0.28       975\n",
            "\n",
            "    accuracy                           0.61      2600\n",
            "   macro avg       0.55      0.53      0.51      2600\n",
            "weighted avg       0.57      0.61      0.56      2600\n",
            "\n",
            "0.5298461538461539\n",
            "Training Loss==========================>>\n",
            "Epoch 2/20 Training Loss: 0.6467\n",
            "Accuracy: 0.6126923076923076\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.90      0.74      1625\n",
            "           1       0.45      0.14      0.21       975\n",
            "\n",
            "    accuracy                           0.61      2600\n",
            "   macro avg       0.54      0.52      0.48      2600\n",
            "weighted avg       0.56      0.61      0.54      2600\n",
            "\n",
            "0.5178461538461538\n",
            "Training Loss==========================>>\n",
            "Epoch 3/20 Training Loss: 0.6421\n",
            "Accuracy: 0.5957692307692307\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.75      0.70      1625\n",
            "           1       0.45      0.34      0.38       975\n",
            "\n",
            "    accuracy                           0.60      2600\n",
            "   macro avg       0.55      0.54      0.54      2600\n",
            "weighted avg       0.58      0.60      0.58      2600\n",
            "\n",
            "0.5438974358974358\n",
            "Training Loss==========================>>\n",
            "Epoch 4/20 Training Loss: 0.6355\n",
            "Accuracy: 0.43576923076923074\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.18      0.28      1625\n",
            "           1       0.39      0.86      0.53       975\n",
            "\n",
            "    accuracy                           0.44      2600\n",
            "   macro avg       0.54      0.52      0.41      2600\n",
            "weighted avg       0.57      0.44      0.38      2600\n",
            "\n",
            "0.5215384615384615\n",
            "Training Loss==========================>>\n",
            "Epoch 5/20 Training Loss: 0.6317\n",
            "Accuracy: 0.6257692307692307\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.99      0.77      1625\n",
            "           1       0.54      0.02      0.03       975\n",
            "\n",
            "    accuracy                           0.63      2600\n",
            "   macro avg       0.58      0.50      0.40      2600\n",
            "weighted avg       0.59      0.63      0.49      2600\n",
            "\n",
            "0.5036923076923077\n",
            "Training Loss==========================>>\n",
            "Epoch 6/20 Training Loss: 0.6273\n",
            "Accuracy: 0.5773076923076923\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.65      0.66      1625\n",
            "           1       0.44      0.46      0.45       975\n",
            "\n",
            "    accuracy                           0.58      2600\n",
            "   macro avg       0.55      0.55      0.55      2600\n",
            "weighted avg       0.58      0.58      0.58      2600\n",
            "\n",
            "0.5531282051282052\n",
            "Training Loss==========================>>\n",
            "Epoch 7/20 Training Loss: 0.6246\n",
            "Accuracy: 0.6284615384615385\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.97      0.77      1625\n",
            "           1       0.54      0.06      0.11       975\n",
            "\n",
            "    accuracy                           0.63      2600\n",
            "   macro avg       0.59      0.51      0.44      2600\n",
            "weighted avg       0.60      0.63      0.52      2600\n",
            "\n",
            "0.5144615384615385\n",
            "Training Loss==========================>>\n",
            "Epoch 8/20 Training Loss: 0.6207\n",
            "Accuracy: 0.6230769230769231\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.98      0.76      1625\n",
            "           1       0.46      0.03      0.06       975\n",
            "\n",
            "    accuracy                           0.62      2600\n",
            "   macro avg       0.54      0.50      0.41      2600\n",
            "weighted avg       0.57      0.62      0.50      2600\n",
            "\n",
            "0.5046153846153846\n",
            "Training Loss==========================>>\n",
            "Epoch 9/20 Training Loss: 0.6182\n",
            "Accuracy: 0.6219230769230769\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.99      0.77      1625\n",
            "           1       0.39      0.01      0.03       975\n",
            "\n",
            "    accuracy                           0.62      2600\n",
            "   macro avg       0.51      0.50      0.40      2600\n",
            "weighted avg       0.54      0.62      0.49      2600\n",
            "\n",
            "0.5004102564102564\n",
            "Training Loss==========================>>\n",
            "Epoch 10/20 Training Loss: 0.6135\n",
            "Accuracy: 0.6176923076923077\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.81      0.73      1625\n",
            "           1       0.48      0.30      0.37       975\n",
            "\n",
            "    accuracy                           0.62      2600\n",
            "   macro avg       0.57      0.55      0.55      2600\n",
            "weighted avg       0.59      0.62      0.59      2600\n",
            "\n",
            "0.5532307692307692\n",
            "Training Loss==========================>>\n",
            "Epoch 11/20 Training Loss: 0.6104\n",
            "Accuracy: 0.6288461538461538\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.88      0.75      1625\n",
            "           1       0.51      0.20      0.29       975\n",
            "\n",
            "    accuracy                           0.63      2600\n",
            "   macro avg       0.58      0.54      0.52      2600\n",
            "weighted avg       0.60      0.63      0.58      2600\n",
            "\n",
            "0.5436923076923078\n",
            "Training Loss==========================>>\n",
            "Epoch 12/20 Training Loss: 0.6045\n",
            "Accuracy: 0.6323076923076923\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.98      0.77      1625\n",
            "           1       0.61      0.05      0.10       975\n",
            "\n",
            "    accuracy                           0.63      2600\n",
            "   macro avg       0.62      0.52      0.43      2600\n",
            "weighted avg       0.63      0.63      0.52      2600\n",
            "\n",
            "0.5163076923076924\n",
            "Training Loss==========================>>\n",
            "Epoch 13/20 Training Loss: 0.6006\n",
            "Accuracy: 0.6138461538461538\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.83      0.73      1625\n",
            "           1       0.47      0.26      0.33       975\n",
            "\n",
            "    accuracy                           0.61      2600\n",
            "   macro avg       0.56      0.54      0.53      2600\n",
            "weighted avg       0.58      0.61      0.58      2600\n",
            "\n",
            "0.5421538461538461\n",
            "Training Loss==========================>>\n",
            "Epoch 14/20 Training Loss: 0.5943\n",
            "Accuracy: 0.6269230769230769\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.88      0.75      1625\n",
            "           1       0.51      0.21      0.30       975\n",
            "\n",
            "    accuracy                           0.63      2600\n",
            "   macro avg       0.58      0.54      0.52      2600\n",
            "weighted avg       0.60      0.63      0.58      2600\n",
            "\n",
            "0.5431794871794873\n",
            "Training Loss==========================>>\n",
            "Epoch 15/20 Training Loss: 0.5903\n",
            "Accuracy: 0.5853846153846154\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.69      0.68      1625\n",
            "           1       0.44      0.40      0.42       975\n",
            "\n",
            "    accuracy                           0.59      2600\n",
            "   macro avg       0.55      0.55      0.55      2600\n",
            "weighted avg       0.58      0.59      0.58      2600\n",
            "\n",
            "0.5491282051282051\n",
            "Training Loss==========================>>\n",
            "Epoch 16/20 Training Loss: 0.5884\n",
            "Accuracy: 0.583076923076923\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.64      0.66      1625\n",
            "           1       0.45      0.49      0.47       975\n",
            "\n",
            "    accuracy                           0.58      2600\n",
            "   macro avg       0.56      0.56      0.56      2600\n",
            "weighted avg       0.59      0.58      0.59      2600\n",
            "\n",
            "0.5634871794871795\n",
            "Training Loss==========================>>\n",
            "Epoch 17/20 Training Loss: 0.5822\n",
            "Accuracy: 0.625\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.88      0.75      1625\n",
            "           1       0.50      0.20      0.28       975\n",
            "\n",
            "    accuracy                           0.62      2600\n",
            "   macro avg       0.57      0.54      0.52      2600\n",
            "weighted avg       0.59      0.62      0.57      2600\n",
            "\n",
            "0.5397948717948717\n",
            "Training Loss==========================>>\n",
            "Epoch 18/20 Training Loss: 0.5765\n",
            "Accuracy: 0.6026923076923076\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.74      0.70      1625\n",
            "           1       0.46      0.38      0.42       975\n",
            "\n",
            "    accuracy                           0.60      2600\n",
            "   macro avg       0.56      0.56      0.56      2600\n",
            "weighted avg       0.59      0.60      0.59      2600\n",
            "\n",
            "0.5574358974358975\n",
            "Training Loss==========================>>\n",
            "Epoch 19/20 Training Loss: 0.5700\n",
            "Accuracy: 0.6169230769230769\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.81      0.73      1625\n",
            "           1       0.48      0.29      0.36       975\n",
            "\n",
            "    accuracy                           0.62      2600\n",
            "   macro avg       0.57      0.55      0.54      2600\n",
            "weighted avg       0.59      0.62      0.59      2600\n",
            "\n",
            "0.5513846153846154\n",
            "Training Loss==========================>>\n",
            "Epoch 20/20 Training Loss: 0.5653\n",
            "Accuracy: 0.6334615384615384\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.90      0.75      1625\n",
            "           1       0.53      0.20      0.29       975\n",
            "\n",
            "    accuracy                           0.63      2600\n",
            "   macro avg       0.59      0.55      0.52      2600\n",
            "weighted avg       0.61      0.63      0.58      2600\n",
            "\n",
            "0.545948717948718\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oymAIt7phwz_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}