{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MuhammadIrzam447/MultiModel/blob/master/Train_26.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Created this notebook for the ViT fine tuning for multi-label classification"
      ],
      "metadata": {
        "id": "GYQB7Sm9MzPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1Yva4FLcRiSbcf3SAENVQFPWkfNG-GUmt"
      ],
      "metadata": {
        "id": "P7yV6u-6QnUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1X4cmMYRjxXFomCJ1adMhPNMYtd4WeDHP"
      ],
      "metadata": {
        "id": "UXht7T3nQndV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1dxd2pySfCIDJYG7qMtuJre8ph068xc1X"
      ],
      "metadata": {
        "id": "Tm7dq6SVQnnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1sYR9EgHkM0oiGRQVlFQCyHO8kMRJ4ibQ"
      ],
      "metadata": {
        "id": "SiCrEPxAQnzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/fused_test.zip"
      ],
      "metadata": {
        "id": "ZQnMfrIuQtoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/fused_train.zip"
      ],
      "metadata": {
        "id": "b3GTftBQQuC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LhpxluXAy0g"
      },
      "outputs": [],
      "source": [
        "!pip install transformers evaluate datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBq4mm7cA4Cp"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import *\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlLN5LTVA5eJ"
      },
      "outputs": [],
      "source": [
        "model_name = \"google/vit-base-patch16-224\"\n",
        "image_processor = ViTImageProcessor.from_pretrained(model_name)\n",
        "model = ViTForImageClassification.from_pretrained(model_name).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading a Custom Dataset"
      ],
      "metadata": {
        "id": "u7rNHqmxDMeV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Dataset"
      ],
      "metadata": {
        "id": "YmwV3dmJEoQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "image_file_paths = []\n",
        "genre_labels = []\n",
        "\n",
        "image_folder_add = \"/content/Dataset(s)/mm-imdb/fused/train\"\n",
        "labels_file = \"/content/Dataset(s)/mm-imdb/fused/train_label.txt\"\n",
        "\n",
        "with open(labels_file, 'r') as file:\n",
        "    for line in file:\n",
        "        parts = line.strip().split('|')\n",
        "        filename = parts[0].strip()\n",
        "        labels = parts[1].strip().split(', ')  # Split labels by comma and remove leading/trailing spaces\n",
        "        image_path = os.path.join(image_folder_add, filename)\n",
        "        image_file_paths.append(image_path)\n",
        "        genre_labels.append(labels)"
      ],
      "metadata": {
        "id": "-kE4fd2hEngM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "label_counts = defaultdict(int)\n",
        "\n",
        "for labels in genre_labels:\n",
        "    for label in labels:\n",
        "        label_counts[label] += 1\n",
        "\n",
        "\n",
        "label_count_list = [(label, count) for label, count in label_counts.items()]\n",
        "sorted_label_count_list = sorted(label_count_list, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "for label, count in sorted_label_count_list:\n",
        "    print(f\"{label}: {count}\")\n",
        "\n",
        "print(\"Total Labels: \", len(label_count_list))"
      ],
      "metadata": {
        "id": "DwfqtkhZE8dH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16c71f87-054a-4231-ddf8-4d57634d5390"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drama: 25272\n",
            "Comedy: 15324\n",
            "Romance: 9678\n",
            "Thriller: 9339\n",
            "Crime: 6879\n",
            "Action: 6465\n",
            "Adventure: 4833\n",
            "Horror: 4809\n",
            "Documentary: 3702\n",
            "Mystery: 3693\n",
            "Sci-Fi: 3636\n",
            "Fantasy: 3486\n",
            "Family: 2934\n",
            "War: 2418\n",
            "Biography: 2364\n",
            "History: 2040\n",
            "Music: 1902\n",
            "Animation: 1758\n",
            "Musical: 1509\n",
            "Western: 1269\n",
            "Sport: 1137\n",
            "Short: 843\n",
            "Film-Noir: 606\n",
            "News: 117\n",
            "Talk-Show: 6\n",
            "Reality-TV: 3\n",
            "Total Labels:  26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "min_label_count = 500\n",
        "valid_labels = [label for label, count in label_counts.items() if count >= min_label_count]\n",
        "valid_labels = sorted(list(valid_labels))"
      ],
      "metadata": {
        "id": "UK-HdXUOFBlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_labels, len(valid_labels)"
      ],
      "metadata": {
        "id": "a9T3vO2VFCWD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b56cabe7-67a0-4241-805b-2a3c139648d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['Action',\n",
              "  'Adventure',\n",
              "  'Animation',\n",
              "  'Biography',\n",
              "  'Comedy',\n",
              "  'Crime',\n",
              "  'Documentary',\n",
              "  'Drama',\n",
              "  'Family',\n",
              "  'Fantasy',\n",
              "  'Film-Noir',\n",
              "  'History',\n",
              "  'Horror',\n",
              "  'Music',\n",
              "  'Musical',\n",
              "  'Mystery',\n",
              "  'Romance',\n",
              "  'Sci-Fi',\n",
              "  'Short',\n",
              "  'Sport',\n",
              "  'Thriller',\n",
              "  'War',\n",
              "  'Western'],\n",
              " 23)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "multi_hot_labels = []\n",
        "\n",
        "for labels in genre_labels:\n",
        "    multi_hot = [1. if label in labels else 0 for label in valid_labels]\n",
        "    multi_hot_labels.append(multi_hot)"
      ],
      "metadata": {
        "id": "otg6ewBEFFIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "train_data = {'image': image_file_paths, 'label': multi_hot_labels}\n",
        "ds_train = Dataset.from_dict(train_data)"
      ],
      "metadata": {
        "id": "kF6DANefFLmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validation Dataset"
      ],
      "metadata": {
        "id": "cohpwOI7EKOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "test_image_file_paths = []\n",
        "test_genre_labels = []\n",
        "\n",
        "image_folder_add = \"/content/Dataset(s)/mm-imdb/fused/test\"\n",
        "labels_file = \"/content/Dataset(s)/mm-imdb/fused/test_label.txt\"\n",
        "\n",
        "with open(labels_file, 'r') as file:\n",
        "    for line in file:\n",
        "        parts = line.strip().split('|')\n",
        "        filename = parts[0].strip()\n",
        "        labels = parts[1].strip().split(', ')  # Split labels by comma and remove leading/trailing spaces\n",
        "\n",
        "        if not (filename.endswith(\"_1.png\") or filename.endswith(\"_2.png\")):\n",
        "            image_path = os.path.join(image_folder_add, filename)\n",
        "            test_image_file_paths.append(image_path)\n",
        "            test_genre_labels.append(labels)"
      ],
      "metadata": {
        "id": "ngBPu3QtDM9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_image_file_paths)"
      ],
      "metadata": {
        "id": "56oYAvLrDWkv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "829ef46b-7bd6-4c5e-e6ee-6a9253d02453"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7799"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_image_file_paths[0:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opNehG84SNhl",
        "outputId": "c2ecc681-cedc-4b22-a3d7-102974756917"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/Dataset(s)/mm-imdb/fused/test/0078718.png',\n",
              " '/content/Dataset(s)/mm-imdb/fused/test/0089003.png',\n",
              " '/content/Dataset(s)/mm-imdb/fused/test/0098136.png',\n",
              " '/content/Dataset(s)/mm-imdb/fused/test/0057693.png',\n",
              " '/content/Dataset(s)/mm-imdb/fused/test/0385330.png',\n",
              " '/content/Dataset(s)/mm-imdb/fused/test/0096487.png',\n",
              " '/content/Dataset(s)/mm-imdb/fused/test/1220553.png',\n",
              " '/content/Dataset(s)/mm-imdb/fused/test/1341764.png',\n",
              " '/content/Dataset(s)/mm-imdb/fused/test/0882969.png',\n",
              " '/content/Dataset(s)/mm-imdb/fused/test/0119918.png']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_multi_hot_labels = []\n",
        "\n",
        "for labels in test_genre_labels:\n",
        "    multi_hot = [1. if label in labels else 0 for label in valid_labels]\n",
        "    test_multi_hot_labels.append(multi_hot)"
      ],
      "metadata": {
        "id": "79PDi6zRDlGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_data = {'image': test_image_file_paths, 'label': test_multi_hot_labels}\n",
        "ds_val = Dataset.from_dict(val_data)"
      ],
      "metadata": {
        "id": "OhIZ93w5DsC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_val"
      ],
      "metadata": {
        "id": "8AZfNqNFDsrE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c5e4a91-18c2-41ea-813b-76dca8fbe409"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['image', 'label'],\n",
              "    num_rows: 7799\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Just Image Training"
      ],
      "metadata": {
        "id": "aLyKHM-Oj0bW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use _3.png for Filtering out Encoded Images\n",
        "# Use _4.png for Filtering out Just Images\n",
        "\n",
        "import os\n",
        "def filter_funtion(example):\n",
        "    img = example[\"image\"]\n",
        "    filename = os.path.basename(img.filename)\n",
        "\n",
        "    return filename.endswith(\"_4.png\")"
      ],
      "metadata": {
        "id": "yEflYIqPj01W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_train = ds_train.filter(filter_funtion)"
      ],
      "metadata": {
        "id": "ofF8M3Ajj2vP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_val = ds_val.filter(filter_funtion)"
      ],
      "metadata": {
        "id": "aiMn7wvWj8o_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring the Data"
      ],
      "metadata": {
        "id": "QVjb_p9kDr5Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgxqXVVrITqO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db74b4dd-9131-4e1a-8606-49d649d2b3b0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Action',\n",
              " 'Adventure',\n",
              " 'Animation',\n",
              " 'Biography',\n",
              " 'Comedy',\n",
              " 'Crime',\n",
              " 'Documentary',\n",
              " 'Drama',\n",
              " 'Family',\n",
              " 'Fantasy',\n",
              " 'Film-Noir',\n",
              " 'History',\n",
              " 'Horror',\n",
              " 'Music',\n",
              " 'Musical',\n",
              " 'Mystery',\n",
              " 'Romance',\n",
              " 'Sci-Fi',\n",
              " 'Short',\n",
              " 'Sport',\n",
              " 'Thriller',\n",
              " 'War',\n",
              " 'Western']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# labels = ds_train.features[\"label\"]\n",
        "# labels\n",
        "labels = valid_labels\n",
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozNlqVN3IgnR"
      },
      "outputs": [],
      "source": [
        "# labels.int2str(ds_train[532][\"label\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing the Data"
      ],
      "metadata": {
        "id": "W8jdhJCQKCOA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9x5QuhV8IwXB"
      },
      "outputs": [],
      "source": [
        "import PIL.Image as pil\n",
        "\n",
        "def transform(examples):\n",
        "  # inputs = image_processor([img.convert(\"RGB\") for img in examples[\"image\"]], return_tensors=\"pt\")\n",
        "  inputs = image_processor([pil.open(img).convert(\"RGB\") for img in examples[\"image\"]], return_tensors=\"pt\")\n",
        "  inputs[\"labels\"] = examples[\"label\"]\n",
        "  return inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vY7oXLDZJBaX"
      },
      "outputs": [],
      "source": [
        "# use the with_transform() method to apply the transform to the dataset on the fly during training\n",
        "train_dataset = ds_train.with_transform(transform)\n",
        "val_dataset = ds_val.with_transform(transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbWrF63YQdsE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48cc4813-03fb-4696-db1e-0841016aec11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 224, 224])\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]\n"
          ]
        }
      ],
      "source": [
        "for item in train_dataset:\n",
        "  print(item[\"pixel_values\"].shape)\n",
        "  print(item[\"labels\"])\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnrUwcqwKTOV"
      },
      "outputs": [],
      "source": [
        "# # extract the labels for our dataset\n",
        "# labels = ds_train.features[\"label\"].names\n",
        "# labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUukexdrJGob"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def collate_fn(batch):\n",
        "  return {\n",
        "      \"pixel_values\": torch.stack([x[\"pixel_values\"] for x in batch]),\n",
        "      \"labels\": torch.tensor([x[\"labels\"] for x in batch]),\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "id": "ZNg1HnXTteYK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c7b5b24-e8d4-46a0-9dc4-cfeafa80b49c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['image', 'label'],\n",
              "    num_rows: 46656\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset"
      ],
      "metadata": {
        "id": "Bbnt3V9ntfhJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65f7aa3f-f49a-4880-b45d-72dbb9d5f4e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['image', 'label'],\n",
              "    num_rows: 7799\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining the Metrics"
      ],
      "metadata": {
        "id": "mNT_iBYyKGAE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crXIbHCeJYFs"
      },
      "outputs": [],
      "source": [
        "# from evaluate import load\n",
        "# import numpy as np\n",
        "# from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# # load the accuracy and f1 metrics from the evaluate module\n",
        "# accuracy = load(\"accuracy\")\n",
        "# f1 = load(\"f1\")\n",
        "\n",
        "# def compute_metrics(eval_pred):\n",
        "#   # compute the accuracy and f1 scores & return them\n",
        "#   accuracy_score = accuracy.compute(predictions=np.argmax(eval_pred.predictions, axis=1), references=eval_pred.label_ids)\n",
        "#   f1_score = f1.compute(predictions=np.argmax(eval_pred.predictions, axis=1), references=eval_pred.label_ids, average=\"macro\")\n",
        "\n",
        "#   # auroc_score = roc_auc_score(eval_pred.label_ids, np.argmax(eval_pred.predictions, axis=1))\n",
        "#   # print(f\"AUROC Score: {auroc_score:.4f}\")\n",
        "\n",
        "#   return {**accuracy_score, **f1_score}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits = eval_pred.predictions\n",
        "    label_ids = eval_pred.label_ids\n",
        "\n",
        "    # sigmoid_predictions = torch.sigmoid(logits)\n",
        "    logits_tensor = torch.tensor(logits)\n",
        "    sigmoid_predictions = torch.sigmoid(logits_tensor)\n",
        "\n",
        "    threshold = 0.5\n",
        "    thresholded_predictions = (sigmoid_predictions > threshold).cpu().numpy().astype(int)\n",
        "\n",
        "\n",
        "    accuracy = accuracy_score(label_ids, thresholded_predictions)\n",
        "    f1 = f1_score(label_ids, thresholded_predictions, average=\"macro\")\n",
        "\n",
        "    return {\"accuracy\": accuracy, \"f1\": f1}\n"
      ],
      "metadata": {
        "id": "hqT6AwlqP-Yj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the Model"
      ],
      "metadata": {
        "id": "2WNJaZeYKJZH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1MmFNVLKw_2"
      },
      "outputs": [],
      "source": [
        "# load the ViT model\n",
        "model = ViTForImageClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels= len(valid_labels),\n",
        "    label2id = {label: str(i) for i, label in enumerate(valid_labels)},\n",
        "    id2label = {str(i): label for i, label in enumerate(valid_labels)},\n",
        "    problem_type = \"multi_label_classification\",\n",
        "    ignore_mismatched_sizes=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install accelerate -U"
      ],
      "metadata": {
        "id": "ojq7BsY6cbCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install transformers[torch]"
      ],
      "metadata": {
        "id": "gIOh_AB-cmS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNnG1GUUK7nj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66b09ea7-a6bf-4739-e8db-055d358416d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n",
            "PyTorch: setting up devices\n"
          ]
        }
      ],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "  output_dir=\"/content/Model/Models-Train-26\", # output directory\n",
        "  per_device_train_batch_size=32, # batch size per device during training\n",
        "  evaluation_strategy=\"steps\",    # evaluation strategy to adopt during training\n",
        "  num_train_epochs=25,             # total number of training epochs\n",
        "  # fp16=True,                    # use mixed precision\n",
        "  save_steps=4000,                # number of update steps before saving checkpoint\n",
        "  eval_steps=4000,                # number of update steps before evaluating\n",
        "  logging_steps=4000,             # number of update steps before logging\n",
        "  # save_steps=50,\n",
        "  # eval_steps=50,\n",
        "  # logging_steps=50,\n",
        "  save_total_limit=4,             # limit the total amount of checkpoints on disk\n",
        "  remove_unused_columns=False,    # remove unused columns from the dataset\n",
        "  push_to_hub=False,              # do not push the model to the hub\n",
        "  report_to='tensorboard',        # report metrics to tensorboard\n",
        "  load_best_model_at_end=True,    # load the best model at the end of training\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "id": "ASOtjmcHdc2R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b7f7aa4-03f9-41c9-8d03-8ef663296272"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['image', 'label'],\n",
              "    num_rows: 46656\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset"
      ],
      "metadata": {
        "id": "y3M13NlzsyNT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b367bbe-21db-4f4b-d049-5f85afa9d1b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['image', 'label'],\n",
              "    num_rows: 7799\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3n8Cv48_QKzr"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                        # the instantiated ðŸ¤— Transformers model to be trained\n",
        "    args=training_args,                 # training arguments, defined above\n",
        "    data_collator=collate_fn,           # the data collator that will be used for batching\n",
        "    compute_metrics=compute_metrics,    # the metrics function that will be used for evaluation\n",
        "    train_dataset=train_dataset,        # training dataset\n",
        "    eval_dataset=val_dataset,           # evaluation dataset\n",
        "    tokenizer=image_processor,          # the processor that will be used for preprocessing the images\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUUoQyh-QPED",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "outputId": "418a56b3-eaf6-4568-fb00-a04b55bd158e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 46,656\n",
            "  Num Epochs = 25\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 36,450\n",
            "  Number of trainable parameters = 85,816,343\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10' max='36450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [   10/36450 00:19 < 25:13:26, 0.40 it/s, Epoch 0.01/25]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# start training\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akZ0-H5YQSuJ"
      },
      "outputs": [],
      "source": [
        "# trainer.evaluate(dataset[\"test\"])\n",
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "2xd0XJYKZU5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import shutil, os\n",
        "\n",
        "# # Define the source folder path (in Colab)\n",
        "# source_folder_path = '/content/output/checkpoint-7000'\n",
        "\n",
        "# # Define the destination folder path (in Google Drive)\n",
        "# destination_folder_path = \"/content/drive/MyDrive/Colab Notebooks/Hateful-Memes/Vit/checkpoint-7000\"\n",
        "\n",
        "# # Remove the existing destination folder (if it exists)\n",
        "# if os.path.exists(destination_folder_path):\n",
        "#     shutil.rmtree(destination_folder_path)\n",
        "\n",
        "# # Copy the folder\n",
        "# shutil.copytree(source_folder_path, destination_folder_path)"
      ],
      "metadata": {
        "id": "pY4jH2jyVc7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAZFCk5Gd1p0"
      },
      "outputs": [],
      "source": [
        "# # start tensorboard\n",
        "# # %load_ext tensorboard\n",
        "# %reload_ext tensorboard\n",
        "# %tensorboard --logdir /content/Model/Models-Train-15/runs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_SsuMpFafPe"
      },
      "source": [
        "## Alternatively: Training using PyTorch Loop\n",
        "Run the two below cells to fine-tune using a regular PyTorch loop if you want."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C29idUGDd2yW"
      },
      "outputs": [],
      "source": [
        "# Training loop\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_dataset_loader = DataLoader(dataset[\"train\"], collate_fn=collate_fn, batch_size=batch_size, shuffle=True)\n",
        "valid_dataset_loader = DataLoader(dataset[\"validation\"], collate_fn=collate_fn, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# define the optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "log_dir = \"./image-classification/tensorboard\"\n",
        "summary_writer = SummaryWriter(log_dir=log_dir)\n",
        "\n",
        "num_epochs = 3\n",
        "model = model.to(device)\n",
        "# print some statistics before training\n",
        "# number of training steps\n",
        "n_train_steps = num_epochs * len(train_dataset_loader)\n",
        "# number of validation steps\n",
        "n_valid_steps = len(valid_dataset_loader)\n",
        "# current training step\n",
        "current_step = 0\n",
        "# logging, eval & save steps\n",
        "save_steps = 1000\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "  accuracy_score = accuracy.compute(predictions=eval_pred.predictions, references=eval_pred.label_ids)\n",
        "  f1_score = f1.compute(predictions=eval_pred.predictions, references=eval_pred.label_ids, average=\"macro\")\n",
        "  return {**accuracy_score, **f1_score}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2v6dNtUcd7-G"
      },
      "outputs": [],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    # set the model to training mode\n",
        "    model.train()\n",
        "    # initialize the training loss\n",
        "    train_loss = 0\n",
        "    # initialize the progress bar\n",
        "    progress_bar = tqdm(range(current_step, n_train_steps), \"Training\", dynamic_ncols=True, ncols=80)\n",
        "    for batch in train_dataset_loader:\n",
        "      if (current_step+1) % save_steps == 0:\n",
        "        ### evaluation code ###\n",
        "        # evaluate on the validation set\n",
        "        # if the current step is a multiple of the save steps\n",
        "        print()\n",
        "        print(f\"Validation at step {current_step}...\")\n",
        "        print()\n",
        "        # set the model to evaluation mode\n",
        "        model.eval()\n",
        "        # initialize our lists that store the predictions and the labels\n",
        "        predictions, labels = [], []\n",
        "        # initialize the validation loss\n",
        "        valid_loss = 0\n",
        "        for batch in valid_dataset_loader:\n",
        "            # get the batch\n",
        "            pixel_values = batch[\"pixel_values\"].to(device)\n",
        "            label_ids = batch[\"labels\"].to(device)\n",
        "            # forward pass\n",
        "            outputs = model(pixel_values=pixel_values, labels=label_ids)\n",
        "            # get the loss\n",
        "            loss = outputs.loss\n",
        "            valid_loss += loss.item()\n",
        "            # free the GPU memory\n",
        "            logits = outputs.logits.detach().cpu()\n",
        "            # add the predictions to the list\n",
        "            predictions.extend(logits.argmax(dim=-1).tolist())\n",
        "            # add the labels to the list\n",
        "            labels.extend(label_ids.tolist())\n",
        "        # make the EvalPrediction object that the compute_metrics function expects\n",
        "        eval_prediction = EvalPrediction(predictions=predictions, label_ids=labels)\n",
        "        # compute the metrics\n",
        "        metrics = compute_metrics(eval_prediction)\n",
        "        # print the stats\n",
        "        print()\n",
        "        print(f\"Epoch: {epoch}, Step: {current_step}, Train Loss: {train_loss / save_steps:.4f}, \" +\n",
        "              f\"Valid Loss: {valid_loss / n_valid_steps:.4f}, Accuracy: {metrics['accuracy']}, \" +\n",
        "              f\"F1 Score: {metrics['f1']}\")\n",
        "        print()\n",
        "        # log the metrics\n",
        "        summary_writer.add_scalar(\"valid_loss\", valid_loss / n_valid_steps, global_step=current_step)\n",
        "        summary_writer.add_scalar(\"accuracy\", metrics[\"accuracy\"], global_step=current_step)\n",
        "        summary_writer.add_scalar(\"f1\", metrics[\"f1\"], global_step=current_step)\n",
        "        # save the model\n",
        "        model.save_pretrained(f\"./vit-base-food/checkpoint-{current_step}\")\n",
        "        image_processor.save_pretrained(f\"./vit-base-food/checkpoint-{current_step}\")\n",
        "        # get the model back to train mode\n",
        "        model.train()\n",
        "        # reset the train and valid loss\n",
        "        train_loss, valid_loss = 0, 0\n",
        "      ### training code below ###\n",
        "      # get the batch & convert to tensor\n",
        "      pixel_values = batch[\"pixel_values\"].to(device)\n",
        "      labels = batch[\"labels\"].to(device)\n",
        "      # forward pass\n",
        "      outputs = model(pixel_values=pixel_values, labels=labels)\n",
        "      # get the loss\n",
        "      loss = outputs.loss\n",
        "      # backward pass\n",
        "      loss.backward()\n",
        "      # update the weights\n",
        "      optimizer.step()\n",
        "      # zero the gradients\n",
        "      optimizer.zero_grad()\n",
        "      # log the loss\n",
        "      loss_v = loss.item()\n",
        "      train_loss += loss_v\n",
        "      # increment the step\n",
        "      current_step += 1\n",
        "      progress_bar.update(1)\n",
        "      # log the training loss\n",
        "      summary_writer.add_scalar(\"train_loss\", loss_v, global_step=current_step)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "aLyKHM-Oj0bW",
        "H_SsuMpFafPe"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}