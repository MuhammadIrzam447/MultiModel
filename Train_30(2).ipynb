{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "NzBfAyc7apgv"
      ],
      "authorship_tag": "ABX9TyM16tcM5XX7MUnCQnLUW6s8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MuhammadIrzam447/MultiModel/blob/master/Train_30(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data\n"
      ],
      "metadata": {
        "id": "jNTJQ9CCMOuR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "\n",
        "# # Define the path to your dataset folder\n",
        "# dataset_folder = '/content/Dataset(s)/jointHF-train+test_unseen/test'\n",
        "\n",
        "# # Define the path for the output text file\n",
        "# output_file = 'test_dataset(2).txt'\n",
        "\n",
        "# # Open the output file for writing\n",
        "# with open(output_file, 'w') as f:\n",
        "#     # Iterate through the subdirectories in the dataset folder\n",
        "#     for label in os.listdir(dataset_folder):\n",
        "#         label_path = os.path.join(dataset_folder, label)\n",
        "\n",
        "#         # Check if it's a directory (class folder)\n",
        "#         if os.path.isdir(label_path):\n",
        "#             # Get the label (class name) from the folder name\n",
        "#             class_name = label\n",
        "\n",
        "#             # Iterate through the images in the class folder\n",
        "#             for image_file in os.listdir(label_path):\n",
        "#                 image_path = os.path.join(label_path, image_file)\n",
        "\n",
        "#                 # Write the image path and label to the output file\n",
        "#                 f.write(f'{image_path}, {class_name}\\n')\n"
      ],
      "metadata": {
        "id": "3ScHa74gfAXq"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Dataset"
      ],
      "metadata": {
        "id": "XNma768QgFfU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "image_file_paths = []\n",
        "image_genre_labels = []\n",
        "enc_file_paths = []\n",
        "enc_genre_labels = []\n",
        "\n",
        "labels_file = \"/content/Dataset(s)/jointHF-train+test_unseen/train_dataset(2).txt\"\n",
        "\n",
        "with open(labels_file, 'r') as file:\n",
        "    for line in file:\n",
        "        parts = line.strip().split(', ')\n",
        "        filename = parts[0].strip()\n",
        "        labels = parts[1].strip()\n",
        "\n",
        "        if filename.endswith(\"_3.png\"):\n",
        "            enc_file_paths.append(filename)\n",
        "            enc_genre_labels.append(labels)\n",
        "\n",
        "        elif filename.endswith(\"_4.png\"):\n",
        "            image_file_paths.append(filename)\n",
        "            image_genre_labels.append(labels)"
      ],
      "metadata": {
        "id": "OEjnk0W-Pc8Y"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(image_file_paths), len(enc_file_paths)"
      ],
      "metadata": {
        "id": "MmYmWXRJRaXy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c3da5ef-e987-4469-b640-565cd40d7520"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9500, 9500)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_file_paths[0:5] , image_genre_labels[0:5]"
      ],
      "metadata": {
        "id": "UwLJAYq6dhLs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3af2a8a-53e0-4e15-9a97-245f70893947"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['/content/Dataset(s)/jointHF-train+test_unseen/train/1/10547.jpg_4.png',\n",
              "  '/content/Dataset(s)/jointHF-train+test_unseen/train/1/29380.jpg_4.png',\n",
              "  '/content/Dataset(s)/jointHF-train+test_unseen/train/1/53927.jpg_4.png',\n",
              "  '/content/Dataset(s)/jointHF-train+test_unseen/train/1/23485.jpg_4.png',\n",
              "  '/content/Dataset(s)/jointHF-train+test_unseen/train/1/26495.jpg_4.png'],\n",
              " ['1', '1', '1', '1', '1'])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enc_file_paths[0:5] , enc_genre_labels[0:5]"
      ],
      "metadata": {
        "id": "WxClniG0fpwN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb20abc3-1fbf-4360-9e88-aca113c07460"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['/content/Dataset(s)/jointHF-train+test_unseen/train/1/70623.jpg_3.png',\n",
              "  '/content/Dataset(s)/jointHF-train+test_unseen/train/1/25314.jpg_3.png',\n",
              "  '/content/Dataset(s)/jointHF-train+test_unseen/train/1/08637.jpg_3.png',\n",
              "  '/content/Dataset(s)/jointHF-train+test_unseen/train/1/72509.jpg_3.png',\n",
              "  '/content/Dataset(s)/jointHF-train+test_unseen/train/1/45802.jpg_3.png'],\n",
              " ['1', '1', '1', '1', '1'])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_labels = set(enc_genre_labels)\n",
        "unique_labels_list = list(unique_labels)\n",
        "\n",
        "print(\"Unique Labels:\", unique_labels_list)"
      ],
      "metadata": {
        "id": "slLNEQBQS-i-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "772f9009-73ea-42e9-88c6-74b7468b1c69"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique Labels: ['0', '1']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = len(unique_labels_list)"
      ],
      "metadata": {
        "id": "HX2Qa1xQZu6Y"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "\n",
        "percentage = 0.3\n",
        "\n",
        "label_counts = {label: 0 for label in unique_labels_list}\n",
        "\n",
        "num_samples_to_include_per_label = {}\n",
        "for label in unique_labels_list:\n",
        "    label_count = enc_genre_labels.count(label)\n",
        "    num_samples_to_include = int(label_count * percentage)\n",
        "    num_samples_to_include_per_label[label] = num_samples_to_include\n",
        "\n",
        "\n",
        "filtered_enc_file_paths = []\n",
        "filtered_enc_genre_labels = []\n",
        "\n",
        "\n",
        "selected_samples_per_label = {label: 0 for label in unique_labels_list}\n",
        "for path, label in zip(enc_file_paths, enc_genre_labels):\n",
        "    if selected_samples_per_label[label] < num_samples_to_include_per_label[label]:\n",
        "        filtered_enc_file_paths.append(path)\n",
        "        filtered_enc_genre_labels.append(label)\n",
        "        selected_samples_per_label[label] += 1"
      ],
      "metadata": {
        "id": "Q9xQzx8MV3_U"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(filtered_enc_file_paths)"
      ],
      "metadata": {
        "id": "_JgyuBkqf45r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72fd5b2c-d1c3-4abe-dabd-d888e5d969ac"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2850"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new lists to combine data from both lists\n",
        "train_file_paths = image_file_paths + filtered_enc_file_paths\n",
        "train_genre_labels = image_genre_labels + filtered_enc_genre_labels"
      ],
      "metadata": {
        "id": "2WTpdvsOXRgD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_file_paths)"
      ],
      "metadata": {
        "id": "72g7sZu8gAXW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "273a949e-c92a-4f8f-91a2-7b5a1ae662f8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12350"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Dataset"
      ],
      "metadata": {
        "id": "7D26BNoJbJ3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "test_image_file_paths = []\n",
        "test_image_genre_labels = []\n",
        "test_enc_file_paths = []\n",
        "test_enc_genre_labels = []\n",
        "\n",
        "test_labels_file = \"/content/Dataset(s)/jointHF-train+test_unseen/test_dataset(2).txt\"\n",
        "\n",
        "with open(test_labels_file, 'r') as file:\n",
        "    for line in file:\n",
        "        parts = line.strip().split(', ')\n",
        "        filename = parts[0].strip()\n",
        "        labels = parts[1].strip()\n",
        "\n",
        "        if filename.endswith(\"_3.png\"):\n",
        "            test_enc_file_paths.append(filename)\n",
        "            test_enc_genre_labels.append(labels)\n",
        "\n",
        "        elif filename.endswith(\"_4.png\"):\n",
        "            test_image_file_paths.append(filename)\n",
        "            test_image_genre_labels.append(labels)"
      ],
      "metadata": {
        "id": "LnGxlWzqbLIj"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_image_file_paths), len(test_enc_file_paths)"
      ],
      "metadata": {
        "id": "yd9_b3EfbLdS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "420e4ef9-97cf-4a63-c2da-e91beff20504"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 2000)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_image_file_paths[0:5] , test_image_genre_labels[0:5]"
      ],
      "metadata": {
        "id": "Kj5sNm7ngOhz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fda6ca5-9f44-4314-bbca-2926309c642d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['/content/Dataset(s)/jointHF-train+test_unseen/test/1/95876.jpg_4.png',\n",
              "  '/content/Dataset(s)/jointHF-train+test_unseen/test/1/65803.jpg_4.png',\n",
              "  '/content/Dataset(s)/jointHF-train+test_unseen/test/1/02564.jpg_4.png',\n",
              "  '/content/Dataset(s)/jointHF-train+test_unseen/test/1/81329.jpg_4.png',\n",
              "  '/content/Dataset(s)/jointHF-train+test_unseen/test/1/59814.jpg_4.png'],\n",
              " ['1', '1', '1', '1', '1'])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "label_counts = {label: 0 for label in unique_labels_list}\n",
        "\n",
        "test_num_samples_to_include_per_label = {}\n",
        "for label in unique_labels_list:\n",
        "    label_count = test_enc_genre_labels.count(label)\n",
        "    num_samples_to_include = int(label_count * percentage)\n",
        "    test_num_samples_to_include_per_label[label] = num_samples_to_include\n",
        "\n",
        "\n",
        "test_filtered_enc_file_paths = []\n",
        "test_filtered_enc_genre_labels = []\n",
        "\n",
        "\n",
        "test_selected_samples_per_label = {label: 0 for label in unique_labels_list}\n",
        "for path, label in zip(test_enc_file_paths, test_enc_genre_labels):\n",
        "    if test_selected_samples_per_label[label] < test_num_samples_to_include_per_label[label]:\n",
        "        test_filtered_enc_file_paths.append(path)\n",
        "        test_filtered_enc_genre_labels.append(label)\n",
        "        test_selected_samples_per_label[label] += 1"
      ],
      "metadata": {
        "id": "rHhvWppqbLxi"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_filtered_enc_file_paths)"
      ],
      "metadata": {
        "id": "ZplC_8mxgfng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b1418b2-0730-4812-8338-65e52155e363"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "600"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new lists to combine data from both lists\n",
        "test_file_paths = test_image_file_paths + test_filtered_enc_file_paths                # check again\n",
        "test_genre_labels = test_image_genre_labels + test_filtered_enc_genre_labels"
      ],
      "metadata": {
        "id": "C6sjf4Ptbcdf"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_file_paths)"
      ],
      "metadata": {
        "id": "T0OH11Z2OJ_k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b81efb5-8454-4b8b-ccc5-fe6c4489a4f0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2600"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ResNet"
      ],
      "metadata": {
        "id": "NzBfAyc7apgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, file_paths, labels, transform=None):\n",
        "        self.file_paths = file_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.file_paths[idx]\n",
        "        image = Image.open(img_path)\n",
        "        label = int(self.labels[idx])\n",
        "        # label = torch.tensor(label,dtype=torch.float32)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "3Y7VXqUBYf_B"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CustomDataset(train_file_paths, train_genre_labels, transform=transform)\n",
        "test_dataset = CustomDataset(test_file_paths, test_genre_labels, transform=transform)"
      ],
      "metadata": {
        "id": "CjVU94XRZOXR"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 32  # Adjust as needed\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "niKEq0PUZVan"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "resnet = models.resnet101(pretrained=True)\n",
        "num_features = resnet.fc.in_features\n",
        "resnet.fc = nn.Linear(num_features, num_classes)  # num_classes is the number of classes in your dataset\n",
        "resnet.to(device)\n",
        "print(resnet)\n"
      ],
      "metadata": {
        "id": "WkU78f5UZXqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.SGD(resnet.parameters(), lr=0.001, momentum=0.9)\n",
        "optimizer = optim.Adam(resnet.parameters(), lr=0.001)\n",
        "num_epochs = 30"
      ],
      "metadata": {
        "id": "kQnCFcJCZx5m"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, roc_auc_score\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "6Q79jKkChops"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_loss = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    resnet.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = resnet(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    training_loss.append(epoch_loss)\n",
        "\n",
        "    print(\"Training Loss==========================>>\")\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} Training Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    resnet.eval()\n",
        "    predicted_classes = []\n",
        "    actual_labels = []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = resnet(images)\n",
        "\n",
        "            _, predicted_label = torch.max(outputs, 1)\n",
        "\n",
        "            predicted_classes.extend(predicted_label.cpu().tolist())\n",
        "            actual_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "    accuracy = accuracy_score(actual_labels, predicted_classes)\n",
        "    # precision = precision_score(actual_labels, predicted_classes, average='weighted')\n",
        "    # recall = recall_score(actual_labels, predicted_classes, average='weighted')\n",
        "    # f1 = f1_score(actual_labels, predicted_classes, average='weighted')\n",
        "\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    # print(\"Precision:\", precision)\n",
        "    # print(\"Recall:\", recall)\n",
        "    # print(\"F1-score:\", f1)\n",
        "\n",
        "    print(classification_report(actual_labels, predicted_classes))\n",
        "    # print(\"Confusion Matrix: \")\n",
        "    # cm = confusion_matrix(actual_labels, predicted_classes)\n",
        "    # print(cm)\n",
        "    predicted_classes = np.array(predicted_classes)\n",
        "    actual_labels = np.array(actual_labels)\n",
        "    print(roc_auc_score(actual_labels, predicted_classes))\n",
        "\n",
        "\n",
        "    save_dir = \"/content/Model/Models-Train-30/\"\n",
        "    os.makedirs(save_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
        "\n",
        "    model_name = str(epoch+1) + \"_model.pth\"\n",
        "    save_path = os.path.join(save_dir, model_name)  # Specify the complete path to the model file\n",
        "    torch.save(resnet.state_dict(), save_path)"
      ],
      "metadata": {
        "id": "X70EPWQbZ_id",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe1f22ee-e698-40a8-e485-cd0d784e856a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss==========================>>\n",
            "Epoch 1/30 Training Loss: 0.6829\n",
            "Accuracy: 0.6238461538461538\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.95      0.76      1625\n",
            "           1       0.49      0.08      0.14       975\n",
            "\n",
            "    accuracy                           0.62      2600\n",
            "   macro avg       0.56      0.52      0.45      2600\n",
            "weighted avg       0.58      0.62      0.53      2600\n",
            "\n",
            "0.5154871794871795\n",
            "Training Loss==========================>>\n",
            "Epoch 2/30 Training Loss: 0.6456\n",
            "Accuracy: 0.6219230769230769\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.96      0.76      1625\n",
            "           1       0.46      0.05      0.10       975\n",
            "\n",
            "    accuracy                           0.62      2600\n",
            "   macro avg       0.55      0.51      0.43      2600\n",
            "weighted avg       0.57      0.62      0.51      2600\n",
            "\n",
            "0.5082051282051281\n",
            "Training Loss==========================>>\n",
            "Epoch 3/30 Training Loss: 0.6239\n",
            "Accuracy: 0.6219230769230769\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.94      0.76      1625\n",
            "           1       0.48      0.09      0.15       975\n",
            "\n",
            "    accuracy                           0.62      2600\n",
            "   macro avg       0.56      0.52      0.45      2600\n",
            "weighted avg       0.57      0.62      0.53      2600\n",
            "\n",
            "0.5151794871794871\n",
            "Training Loss==========================>>\n",
            "Epoch 4/30 Training Loss: 0.6194\n",
            "Accuracy: 0.6080769230769231\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.89      0.74      1625\n",
            "           1       0.43      0.14      0.21       975\n",
            "\n",
            "    accuracy                           0.61      2600\n",
            "   macro avg       0.53      0.51      0.47      2600\n",
            "weighted avg       0.56      0.61      0.54      2600\n",
            "\n",
            "0.5135384615384615\n",
            "Training Loss==========================>>\n",
            "Epoch 5/30 Training Loss: 0.6170\n",
            "Accuracy: 0.6115384615384616\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.90      0.74      1625\n",
            "           1       0.44      0.14      0.21       975\n",
            "\n",
            "    accuracy                           0.61      2600\n",
            "   macro avg       0.54      0.52      0.48      2600\n",
            "weighted avg       0.56      0.61      0.54      2600\n",
            "\n",
            "0.5167179487179486\n",
            "Training Loss==========================>>\n",
            "Epoch 6/30 Training Loss: 0.6144\n",
            "Accuracy: 0.6103846153846154\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.88      0.74      1625\n",
            "           1       0.45      0.17      0.24       975\n",
            "\n",
            "    accuracy                           0.61      2600\n",
            "   macro avg       0.54      0.52      0.49      2600\n",
            "weighted avg       0.57      0.61      0.55      2600\n",
            "\n",
            "0.5217435897435898\n",
            "Training Loss==========================>>\n",
            "Epoch 7/30 Training Loss: 0.6117\n",
            "Accuracy: 0.6115384615384616\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.88      0.74      1625\n",
            "           1       0.45      0.16      0.23       975\n",
            "\n",
            "    accuracy                           0.61      2600\n",
            "   macro avg       0.54      0.52      0.49      2600\n",
            "weighted avg       0.57      0.61      0.55      2600\n",
            "\n",
            "0.5206153846153847\n",
            "Training Loss==========================>>\n",
            "Epoch 8/30 Training Loss: 0.6091\n",
            "Accuracy: 0.6165384615384616\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.90      0.75      1625\n",
            "           1       0.46      0.14      0.22       975\n",
            "\n",
            "    accuracy                           0.62      2600\n",
            "   macro avg       0.55      0.52      0.48      2600\n",
            "weighted avg       0.57      0.62      0.55      2600\n",
            "\n",
            "0.5215384615384615\n",
            "Training Loss==========================>>\n",
            "Epoch 9/30 Training Loss: 0.6089\n",
            "Accuracy: 0.525\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.63      0.62      1625\n",
            "           1       0.36      0.35      0.36       975\n",
            "\n",
            "    accuracy                           0.53      2600\n",
            "   macro avg       0.49      0.49      0.49      2600\n",
            "weighted avg       0.52      0.53      0.52      2600\n",
            "\n",
            "0.49056410256410254\n",
            "Training Loss==========================>>\n",
            "Epoch 10/30 Training Loss: 0.6073\n",
            "Accuracy: 0.4846153846153846\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.48      0.54      1625\n",
            "           1       0.36      0.49      0.41       975\n",
            "\n",
            "    accuracy                           0.48      2600\n",
            "   macro avg       0.49      0.48      0.48      2600\n",
            "weighted avg       0.52      0.48      0.49      2600\n",
            "\n",
            "0.4849230769230769\n",
            "Training Loss==========================>>\n",
            "Epoch 11/30 Training Loss: 0.6034\n",
            "Accuracy: 0.5788461538461539\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.76      0.69      1625\n",
            "           1       0.41      0.28      0.33       975\n",
            "\n",
            "    accuracy                           0.58      2600\n",
            "   macro avg       0.52      0.52      0.51      2600\n",
            "weighted avg       0.55      0.58      0.56      2600\n",
            "\n",
            "0.5194871794871795\n",
            "Training Loss==========================>>\n",
            "Epoch 12/30 Training Loss: 0.5984\n",
            "Accuracy: 0.5553846153846154\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.65      0.65      1625\n",
            "           1       0.40      0.39      0.40       975\n",
            "\n",
            "    accuracy                           0.56      2600\n",
            "   macro avg       0.52      0.52      0.52      2600\n",
            "weighted avg       0.55      0.56      0.55      2600\n",
            "\n",
            "0.5228717948717949\n",
            "Training Loss==========================>>\n",
            "Epoch 13/30 Training Loss: 0.5978\n",
            "Accuracy: 0.5269230769230769\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.65      0.63      1625\n",
            "           1       0.36      0.33      0.34       975\n",
            "\n",
            "    accuracy                           0.53      2600\n",
            "   macro avg       0.49      0.49      0.49      2600\n",
            "weighted avg       0.52      0.53      0.52      2600\n",
            "\n",
            "0.4869743589743589\n",
            "Training Loss==========================>>\n",
            "Epoch 14/30 Training Loss: 0.6019\n",
            "Accuracy: 0.6261538461538462\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      1.00      0.77      1625\n",
            "           1       0.67      0.01      0.01       975\n",
            "\n",
            "    accuracy                           0.63      2600\n",
            "   macro avg       0.65      0.50      0.39      2600\n",
            "weighted avg       0.64      0.63      0.49      2600\n",
            "\n",
            "0.5021538461538462\n",
            "Training Loss==========================>>\n",
            "Epoch 15/30 Training Loss: 0.5980\n",
            "Accuracy: 0.6326923076923077\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.96      0.76      1625\n",
            "           1       0.56      0.10      0.16       975\n",
            "\n",
            "    accuracy                           0.63      2600\n",
            "   macro avg       0.60      0.53      0.46      2600\n",
            "weighted avg       0.61      0.63      0.54      2600\n",
            "\n",
            "0.5252307692307692\n",
            "Training Loss==========================>>\n",
            "Epoch 16/30 Training Loss: 0.6009\n",
            "Accuracy: 0.5276923076923077\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.61      0.62      1625\n",
            "           1       0.37      0.39      0.38       975\n",
            "\n",
            "    accuracy                           0.53      2600\n",
            "   macro avg       0.50      0.50      0.50      2600\n",
            "weighted avg       0.53      0.53      0.53      2600\n",
            "\n",
            "0.4992820512820513\n",
            "Training Loss==========================>>\n",
            "Epoch 17/30 Training Loss: 0.5940\n",
            "Accuracy: 0.5973076923076923\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.76      0.70      1625\n",
            "           1       0.45      0.32      0.37       975\n",
            "\n",
            "    accuracy                           0.60      2600\n",
            "   macro avg       0.55      0.54      0.54      2600\n",
            "weighted avg       0.58      0.60      0.58      2600\n",
            "\n",
            "0.5422564102564104\n",
            "Training Loss==========================>>\n",
            "Epoch 18/30 Training Loss: 0.5892\n",
            "Accuracy: 0.5657692307692308\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.74      0.68      1625\n",
            "           1       0.39      0.28      0.33       975\n",
            "\n",
            "    accuracy                           0.57      2600\n",
            "   macro avg       0.51      0.51      0.50      2600\n",
            "weighted avg       0.54      0.57      0.55      2600\n",
            "\n",
            "0.5084102564102564\n",
            "Training Loss==========================>>\n",
            "Epoch 19/30 Training Loss: 0.5878\n",
            "Accuracy: 0.5738461538461539\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.71      0.67      1625\n",
            "           1       0.42      0.35      0.38       975\n",
            "\n",
            "    accuracy                           0.57      2600\n",
            "   macro avg       0.53      0.53      0.53      2600\n",
            "weighted avg       0.56      0.57      0.57      2600\n",
            "\n",
            "0.5296410256410257\n",
            "Training Loss==========================>>\n",
            "Epoch 20/30 Training Loss: 0.5834\n",
            "Accuracy: 0.5826923076923077\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.78      0.70      1625\n",
            "           1       0.41      0.26      0.32       975\n",
            "\n",
            "    accuracy                           0.58      2600\n",
            "   macro avg       0.52      0.52      0.51      2600\n",
            "weighted avg       0.55      0.58      0.56      2600\n",
            "\n",
            "0.5184615384615384\n",
            "Training Loss==========================>>\n",
            "Epoch 21/30 Training Loss: 0.5837\n",
            "Accuracy: 0.5957692307692307\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.83      0.72      1625\n",
            "           1       0.42      0.21      0.28       975\n",
            "\n",
            "    accuracy                           0.60      2600\n",
            "   macro avg       0.53      0.52      0.50      2600\n",
            "weighted avg       0.56      0.60      0.56      2600\n",
            "\n",
            "0.5192820512820513\n",
            "Training Loss==========================>>\n",
            "Epoch 22/30 Training Loss: 0.5888\n",
            "Accuracy: 0.5738461538461539\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.73      0.68      1625\n",
            "           1       0.41      0.32      0.36       975\n",
            "\n",
            "    accuracy                           0.57      2600\n",
            "   macro avg       0.53      0.52      0.52      2600\n",
            "weighted avg       0.55      0.57      0.56      2600\n",
            "\n",
            "0.5224615384615384\n",
            "Training Loss==========================>>\n",
            "Epoch 23/30 Training Loss: 0.5809\n",
            "Accuracy: 0.5676923076923077\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.72      0.68      1625\n",
            "           1       0.40      0.32      0.35       975\n",
            "\n",
            "    accuracy                           0.57      2600\n",
            "   macro avg       0.52      0.52      0.51      2600\n",
            "weighted avg       0.55      0.57      0.55      2600\n",
            "\n",
            "0.5173333333333334\n",
            "Training Loss==========================>>\n",
            "Epoch 24/30 Training Loss: 0.5761\n",
            "Accuracy: 0.5611538461538461\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.71      0.67      1625\n",
            "           1       0.39      0.31      0.35       975\n",
            "\n",
            "    accuracy                           0.56      2600\n",
            "   macro avg       0.51      0.51      0.51      2600\n",
            "weighted avg       0.54      0.56      0.55      2600\n",
            "\n",
            "0.5112820512820513\n",
            "Training Loss==========================>>\n",
            "Epoch 25/30 Training Loss: 0.5824\n",
            "Accuracy: 0.6276923076923077\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.89      0.75      1625\n",
            "           1       0.51      0.20      0.28       975\n",
            "\n",
            "    accuracy                           0.63      2600\n",
            "   macro avg       0.58      0.54      0.52      2600\n",
            "weighted avg       0.60      0.63      0.57      2600\n",
            "\n",
            "0.5413333333333333\n",
            "Training Loss==========================>>\n",
            "Epoch 26/30 Training Loss: 0.5934\n",
            "Accuracy: 0.6273076923076923\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.88      0.75      1625\n",
            "           1       0.51      0.21      0.30       975\n",
            "\n",
            "    accuracy                           0.63      2600\n",
            "   macro avg       0.58      0.54      0.52      2600\n",
            "weighted avg       0.60      0.63      0.58      2600\n",
            "\n",
            "0.5438974358974359\n",
            "Training Loss==========================>>\n",
            "Epoch 27/30 Training Loss: 0.5666\n",
            "Accuracy: 0.5484615384615384\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.63      0.63      1625\n",
            "           1       0.40      0.42      0.41       975\n",
            "\n",
            "    accuracy                           0.55      2600\n",
            "   macro avg       0.52      0.52      0.52      2600\n",
            "weighted avg       0.55      0.55      0.55      2600\n",
            "\n",
            "0.5224615384615384\n",
            "Training Loss==========================>>\n",
            "Epoch 28/30 Training Loss: 0.5556\n",
            "Accuracy: 0.5946153846153847\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.78      0.71      1625\n",
            "           1       0.44      0.29      0.35       975\n",
            "\n",
            "    accuracy                           0.59      2600\n",
            "   macro avg       0.54      0.53      0.53      2600\n",
            "weighted avg       0.57      0.59      0.57      2600\n",
            "\n",
            "0.5335384615384615\n",
            "Training Loss==========================>>\n",
            "Epoch 29/30 Training Loss: 0.5521\n",
            "Accuracy: 0.5892307692307692\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.74      0.69      1625\n",
            "           1       0.44      0.34      0.39       975\n",
            "\n",
            "    accuracy                           0.59      2600\n",
            "   macro avg       0.55      0.54      0.54      2600\n",
            "weighted avg       0.57      0.59      0.58      2600\n",
            "\n",
            "0.5403076923076923\n",
            "Training Loss==========================>>\n",
            "Epoch 30/30 Training Loss: 0.5349\n",
            "Accuracy: 0.5923076923076923\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.72      0.69      1625\n",
            "           1       0.45      0.37      0.41       975\n",
            "\n",
            "    accuracy                           0.59      2600\n",
            "   macro avg       0.55      0.55      0.55      2600\n",
            "weighted avg       0.58      0.59      0.58      2600\n",
            "\n",
            "0.5483076923076923\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ViT"
      ],
      "metadata": {
        "id": "VmLtbV7uZiUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "!pip install transformers evaluate datasets\n",
        "import requests\n",
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import *\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "uO3ZsSpNeu9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model_name = \"google/vit-base-patch16-224\"\n",
        "image_processor = ViTImageProcessor.from_pretrained(model_name)\n",
        "model = ViTForImageClassification.from_pretrained(model_name).to(device)"
      ],
      "metadata": {
        "id": "5kwp9qmMZjkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Label Encoding"
      ],
      "metadata": {
        "id": "UGs6LJI9guRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "transformed_train_genre_labels = label_encoder.fit_transform(train_genre_labels)"
      ],
      "metadata": {
        "id": "x5qr0pvBcKi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder1 = LabelEncoder()\n",
        "transformed_test_genre_labels = label_encoder1.fit_transform(test_genre_labels)"
      ],
      "metadata": {
        "id": "VJ-m2Q5rgsrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_genre_labels), len(transformed_train_genre_labels)"
      ],
      "metadata": {
        "id": "Q8iTY22HfXsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_genre_labels), len(transformed_test_genre_labels)"
      ],
      "metadata": {
        "id": "ClRZgMkgg4Sw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Creating"
      ],
      "metadata": {
        "id": "e297rXtbgwdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "train_data = {'image': train_file_paths, 'label': transformed_train_genre_labels}\n",
        "ds_train = Dataset.from_dict(train_data)"
      ],
      "metadata": {
        "id": "6XKv_SEBa6Sz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_data = {'image': test_file_paths, 'label': transformed_test_genre_labels}\n",
        "ds_val = Dataset.from_dict(val_data)"
      ],
      "metadata": {
        "id": "djhbA0nCaDgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = unique_labels_list\n",
        "labels"
      ],
      "metadata": {
        "id": "5TGQqMVrbMW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import PIL.Image as pil\n",
        "\n",
        "def transform(examples):\n",
        "  inputs = image_processor([pil.open(img).convert(\"RGB\") for img in examples[\"image\"]], return_tensors=\"pt\")\n",
        "  inputs[\"labels\"] = examples[\"label\"]\n",
        "  return inputs"
      ],
      "metadata": {
        "id": "9dEy0tTcbTfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = ds_train.with_transform(transform)\n",
        "val_dataset = ds_val.with_transform(transform)"
      ],
      "metadata": {
        "id": "owz6NIGTbVA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "id": "pet0z7BJdDZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset"
      ],
      "metadata": {
        "id": "00nnqJFvdHIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for item in val_dataset:\n",
        "  print(item['pixel_values'].shape)\n",
        "  print(item[\"labels\"])\n",
        "  break"
      ],
      "metadata": {
        "id": "sPsIroeXbjlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def collate_fn(batch):\n",
        "  return {\n",
        "      \"pixel_values\": torch.stack([x[\"pixel_values\"] for x in batch]),\n",
        "      \"labels\": torch.tensor([x[\"labels\"] for x in batch]),\n",
        "  }"
      ],
      "metadata": {
        "id": "22a48uX4bnZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the ViT model\n",
        "model = ViTForImageClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=len(unique_labels_list),\n",
        "    id2label={str(i): c for i, c in enumerate(unique_labels_list)},\n",
        "    label2id={c: str(i) for i, c in enumerate(unique_labels_list)},\n",
        "    ignore_mismatched_sizes=True,\n",
        ")"
      ],
      "metadata": {
        "id": "sIQenfhHbz_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation Metric"
      ],
      "metadata": {
        "id": "KLVUJAqgcCRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from evaluate import load\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# load the accuracy and f1 metrics from the evaluate module\n",
        "accuracy = load(\"accuracy\")\n",
        "f1 = load(\"f1\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "  # compute the accuracy and f1 scores & return them\n",
        "  accuracy_score = accuracy.compute(predictions=np.argmax(eval_pred.predictions, axis=1), references=eval_pred.label_ids)\n",
        "  f1_score = f1.compute(predictions=np.argmax(eval_pred.predictions, axis=1), references=eval_pred.label_ids, average=\"macro\")\n",
        "\n",
        "  auroc_score = roc_auc_score(eval_pred.label_ids, np.argmax(eval_pred.predictions, axis=1))\n",
        "  print(f\"AUROC Score: {auroc_score:.4f}\")\n",
        "\n",
        "  return {**accuracy_score, **f1_score}"
      ],
      "metadata": {
        "id": "23DkFdTNcCtE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "FuDfvAopcHUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install accelerate -U\n",
        "# !pip install transformers[torch]"
      ],
      "metadata": {
        "id": "665OJ9i4cudJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "  output_dir=\"/content/Model/Models-Train-27\", # output directory\n",
        "  per_device_train_batch_size=32, # batch size per device during training\n",
        "  evaluation_strategy=\"steps\",    # evaluation strategy to adopt during training\n",
        "  num_train_epochs=20,             # total number of training epochs\n",
        "  # fp16=True,                    # use mixed precision\n",
        "  save_steps=1500,                # number of update steps before saving checkpoint\n",
        "  eval_steps=1500,                # number of update steps before evaluating\n",
        "  logging_steps=10,             # number of update steps before logging\n",
        "  # save_steps=50,\n",
        "  # eval_steps=50,\n",
        "  # logging_steps=50,\n",
        "  save_total_limit=6,             # limit the total amount of checkpoints on disk\n",
        "  remove_unused_columns=False,    # remove unused columns from the dataset\n",
        "  push_to_hub=False,              # do not push the model to the hub\n",
        "  report_to='tensorboard',        # report metrics to tensorboard\n",
        "  load_best_model_at_end=True,    # load the best model at the end of training\n",
        ")"
      ],
      "metadata": {
        "id": "xApZuICNcqvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                        # the instantiated  Transformers model to be trained\n",
        "    args=training_args,                 # training arguments, defined above\n",
        "    data_collator=collate_fn,           # the data collator that will be used for batching\n",
        "    compute_metrics=compute_metrics,    # the metrics function that will be used for evaluation\n",
        "    train_dataset=train_dataset,        # training dataset\n",
        "    eval_dataset=val_dataset,           # evaluation dataset\n",
        "    tokenizer=image_processor,          # the processor that will be used for preprocessing the images\n",
        ")"
      ],
      "metadata": {
        "id": "CDA6IEUzc6M8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "0CI-Z4G_c9WT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}