{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MuhammadIrzam447/MultiModel/blob/master/Train_34.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Created this notebook for the ViT fine tuning for multi-label classification"
      ],
      "metadata": {
        "id": "GYQB7Sm9MzPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LhpxluXAy0g"
      },
      "outputs": [],
      "source": [
        "!pip install transformers evaluate datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBq4mm7cA4Cp"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import *\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlLN5LTVA5eJ"
      },
      "outputs": [],
      "source": [
        "model_name = \"google/vit-base-patch16-224\"\n",
        "image_processor = ViTImageProcessor.from_pretrained(model_name)\n",
        "model = ViTForImageClassification.from_pretrained(model_name).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading a Custom Dataset"
      ],
      "metadata": {
        "id": "u7rNHqmxDMeV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Dataset"
      ],
      "metadata": {
        "id": "YmwV3dmJEoQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "image_file_paths = []\n",
        "genre_labels = []\n",
        "\n",
        "image_folder_add = \"/content/Dataset(s)/mmimdb_new/train\"\n",
        "labels_file = \"/content/Dataset(s)/mmimdb_new/train_label.txt\"\n",
        "\n",
        "with open(labels_file, 'r') as file:\n",
        "    for line in file:\n",
        "        parts = line.strip().split('|')\n",
        "        filename = parts[0].strip()\n",
        "        labels = parts[1].strip().split(', ')  # Split labels by comma and remove leading/trailing spaces\n",
        "        image_path = os.path.join(image_folder_add, filename)\n",
        "        image_file_paths.append(image_path)\n",
        "        genre_labels.append(labels)"
      ],
      "metadata": {
        "id": "-kE4fd2hEngM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "label_counts = defaultdict(int)\n",
        "\n",
        "for labels in genre_labels:\n",
        "    for label in labels:\n",
        "        label_counts[label] += 1\n",
        "\n",
        "\n",
        "label_count_list = [(label, count) for label, count in label_counts.items()]\n",
        "sorted_label_count_list = sorted(label_count_list, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "for label, count in sorted_label_count_list:\n",
        "    print(f\"{label}: {count}\")\n",
        "\n",
        "print(\"Total Labels: \", len(label_count_list))"
      ],
      "metadata": {
        "id": "DwfqtkhZE8dH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1f0b0d9-f677-4dd9-8c06-75fa2b0faced"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drama: 16848\n",
            "Comedy: 10216\n",
            "Romance: 6452\n",
            "Thriller: 6226\n",
            "Crime: 4586\n",
            "Action: 4310\n",
            "Adventure: 3222\n",
            "Horror: 3206\n",
            "Documentary: 2468\n",
            "Mystery: 2462\n",
            "Sci-Fi: 2424\n",
            "Fantasy: 2324\n",
            "Family: 1956\n",
            "War: 1612\n",
            "Biography: 1576\n",
            "History: 1360\n",
            "Music: 1268\n",
            "Animation: 1172\n",
            "Musical: 1006\n",
            "Western: 846\n",
            "Sport: 758\n",
            "Short: 562\n",
            "Film-Noir: 404\n",
            "News: 78\n",
            "Talk-Show: 4\n",
            "Reality-TV: 2\n",
            "Total Labels:  26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "min_label_count = 400\n",
        "valid_labels = [label for label, count in label_counts.items() if count >= min_label_count]\n",
        "valid_labels = sorted(list(valid_labels))"
      ],
      "metadata": {
        "id": "UK-HdXUOFBlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_labels, len(valid_labels)"
      ],
      "metadata": {
        "id": "a9T3vO2VFCWD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61d1a761-73d4-42bb-d1a7-2f3463feeebf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['Action',\n",
              "  'Adventure',\n",
              "  'Animation',\n",
              "  'Biography',\n",
              "  'Comedy',\n",
              "  'Crime',\n",
              "  'Documentary',\n",
              "  'Drama',\n",
              "  'Family',\n",
              "  'Fantasy',\n",
              "  'Film-Noir',\n",
              "  'History',\n",
              "  'Horror',\n",
              "  'Music',\n",
              "  'Musical',\n",
              "  'Mystery',\n",
              "  'Romance',\n",
              "  'Sci-Fi',\n",
              "  'Short',\n",
              "  'Sport',\n",
              "  'Thriller',\n",
              "  'War',\n",
              "  'Western'],\n",
              " 23)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "multi_hot_labels = []\n",
        "\n",
        "for labels in genre_labels:\n",
        "    multi_hot = [1. if label in labels else 0 for label in valid_labels]\n",
        "    multi_hot_labels.append(multi_hot)"
      ],
      "metadata": {
        "id": "otg6ewBEFFIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "train_data = {'image': image_file_paths, 'label': multi_hot_labels}\n",
        "ds_train = Dataset.from_dict(train_data)"
      ],
      "metadata": {
        "id": "kF6DANefFLmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUm6kbDoCFrn",
        "outputId": "3d6c8a02-927a-48bf-b8a4-ac38d2b7fa91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['image', 'label'],\n",
              "    num_rows: 31104\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validation Dataset"
      ],
      "metadata": {
        "id": "cohpwOI7EKOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "test_image_file_paths = []\n",
        "test_genre_labels = []\n",
        "\n",
        "image_folder_add = \"/content/Dataset(s)/mmimdb_new/test\"\n",
        "labels_file = \"/content/Dataset(s)/mmimdb_new/test_label.txt\"\n",
        "\n",
        "with open(labels_file, 'r') as file:\n",
        "    for line in file:\n",
        "        parts = line.strip().split('|')\n",
        "        filename = parts[0].strip()\n",
        "        labels = parts[1].strip().split(', ')  # Split labels by comma and remove leading/trailing spaces\n",
        "\n",
        "        if not (filename.endswith(\"_1.png\") or filename.endswith(\"_2.png\")):\n",
        "            image_path = os.path.join(image_folder_add, filename)\n",
        "            test_image_file_paths.append(image_path)\n",
        "            test_genre_labels.append(labels)"
      ],
      "metadata": {
        "id": "ngBPu3QtDM9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_image_file_paths)"
      ],
      "metadata": {
        "id": "56oYAvLrDWkv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "062f38fa-9194-45b1-db6f-c4bb567553ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15598"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_image_file_paths[0:10]"
      ],
      "metadata": {
        "id": "opNehG84SNhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_multi_hot_labels = []\n",
        "\n",
        "for labels in test_genre_labels:\n",
        "    multi_hot = [1. if label in labels else 0 for label in valid_labels]\n",
        "    test_multi_hot_labels.append(multi_hot)"
      ],
      "metadata": {
        "id": "79PDi6zRDlGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_data = {'image': test_image_file_paths, 'label': test_multi_hot_labels}\n",
        "ds_val = Dataset.from_dict(val_data)"
      ],
      "metadata": {
        "id": "OhIZ93w5DsC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_val"
      ],
      "metadata": {
        "id": "8AZfNqNFDsrE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d126796f-19dd-4710-a0d1-a02384bcd08f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['image', 'label'],\n",
              "    num_rows: 15598\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Just Image Training"
      ],
      "metadata": {
        "id": "aLyKHM-Oj0bW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use _3.png for Filtering out Encoded Images\n",
        "# Use _4.png for Filtering out Just Images\n",
        "\n",
        "import os\n",
        "def filter_funtion(example):\n",
        "    img = example[\"image\"]\n",
        "    filename = os.path.basename(img.filename)\n",
        "\n",
        "    return filename.endswith(\"_4.png\")"
      ],
      "metadata": {
        "id": "yEflYIqPj01W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_train = ds_train.filter(filter_funtion)"
      ],
      "metadata": {
        "id": "ofF8M3Ajj2vP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_val = ds_val.filter(filter_funtion)"
      ],
      "metadata": {
        "id": "aiMn7wvWj8o_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring the Data"
      ],
      "metadata": {
        "id": "QVjb_p9kDr5Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgxqXVVrITqO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b58026f5-4316-47a7-a191-0eb18a84e947"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Action',\n",
              " 'Adventure',\n",
              " 'Animation',\n",
              " 'Biography',\n",
              " 'Comedy',\n",
              " 'Crime',\n",
              " 'Documentary',\n",
              " 'Drama',\n",
              " 'Family',\n",
              " 'Fantasy',\n",
              " 'Film-Noir',\n",
              " 'History',\n",
              " 'Horror',\n",
              " 'Music',\n",
              " 'Musical',\n",
              " 'Mystery',\n",
              " 'Romance',\n",
              " 'Sci-Fi',\n",
              " 'Short',\n",
              " 'Sport',\n",
              " 'Thriller',\n",
              " 'War',\n",
              " 'Western']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# labels = ds_train.features[\"label\"]\n",
        "# labels\n",
        "labels = valid_labels\n",
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozNlqVN3IgnR"
      },
      "outputs": [],
      "source": [
        "# labels.int2str(ds_train[532][\"label\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing the Data"
      ],
      "metadata": {
        "id": "W8jdhJCQKCOA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9x5QuhV8IwXB"
      },
      "outputs": [],
      "source": [
        "import PIL.Image as pil\n",
        "\n",
        "def transform(examples):\n",
        "  # inputs = image_processor([img.convert(\"RGB\") for img in examples[\"image\"]], return_tensors=\"pt\")\n",
        "  inputs = image_processor([pil.open(img).convert(\"RGB\") for img in examples[\"image\"]], return_tensors=\"pt\")\n",
        "  inputs[\"labels\"] = examples[\"label\"]\n",
        "  return inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vY7oXLDZJBaX"
      },
      "outputs": [],
      "source": [
        "# use the with_transform() method to apply the transform to the dataset on the fly during training\n",
        "train_dataset = ds_train.with_transform(transform)\n",
        "val_dataset = ds_val.with_transform(transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbWrF63YQdsE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1159e56-9207-40d3-bd3d-48a0364a1e49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 224, 224])\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]\n"
          ]
        }
      ],
      "source": [
        "for item in train_dataset:\n",
        "  print(item[\"pixel_values\"].shape)\n",
        "  print(item[\"labels\"])\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnrUwcqwKTOV"
      },
      "outputs": [],
      "source": [
        "# # extract the labels for our dataset\n",
        "# labels = ds_train.features[\"label\"].names\n",
        "# labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUukexdrJGob"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def collate_fn(batch):\n",
        "  return {\n",
        "      \"pixel_values\": torch.stack([x[\"pixel_values\"] for x in batch]),\n",
        "      \"labels\": torch.tensor([x[\"labels\"] for x in batch]),\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "id": "ZNg1HnXTteYK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd553ddc-c695-4a92-aa6f-2397777db292"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['image', 'label'],\n",
              "    num_rows: 31104\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset"
      ],
      "metadata": {
        "id": "Bbnt3V9ntfhJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "667e687e-0ce8-40a3-dce2-80c4a9c22f02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['image', 'label'],\n",
              "    num_rows: 15598\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining the Metrics"
      ],
      "metadata": {
        "id": "mNT_iBYyKGAE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crXIbHCeJYFs"
      },
      "outputs": [],
      "source": [
        "# from evaluate import load\n",
        "# import numpy as np\n",
        "# from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# # load the accuracy and f1 metrics from the evaluate module\n",
        "# accuracy = load(\"accuracy\")\n",
        "# f1 = load(\"f1\")\n",
        "\n",
        "# def compute_metrics(eval_pred):\n",
        "#   # compute the accuracy and f1 scores & return them\n",
        "#   accuracy_score = accuracy.compute(predictions=np.argmax(eval_pred.predictions, axis=1), references=eval_pred.label_ids)\n",
        "#   f1_score = f1.compute(predictions=np.argmax(eval_pred.predictions, axis=1), references=eval_pred.label_ids, average=\"macro\")\n",
        "\n",
        "#   # auroc_score = roc_auc_score(eval_pred.label_ids, np.argmax(eval_pred.predictions, axis=1))\n",
        "#   # print(f\"AUROC Score: {auroc_score:.4f}\")\n",
        "\n",
        "#   return {**accuracy_score, **f1_score}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits = eval_pred.predictions\n",
        "    label_ids = eval_pred.label_ids\n",
        "\n",
        "    # sigmoid_predictions = torch.sigmoid(logits)\n",
        "    logits_tensor = torch.tensor(logits)\n",
        "    sigmoid_predictions = torch.sigmoid(logits_tensor)\n",
        "\n",
        "    threshold = 0.5\n",
        "    thresholded_predictions = (sigmoid_predictions > threshold).cpu().numpy().astype(int)\n",
        "\n",
        "\n",
        "    accuracy = accuracy_score(label_ids, thresholded_predictions)\n",
        "    f1 = f1_score(label_ids, thresholded_predictions, average=\"macro\")\n",
        "\n",
        "    return {\"accuracy\": accuracy, \"f1\": f1}\n"
      ],
      "metadata": {
        "id": "hqT6AwlqP-Yj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the Model"
      ],
      "metadata": {
        "id": "2WNJaZeYKJZH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1MmFNVLKw_2"
      },
      "outputs": [],
      "source": [
        "# load the ViT model\n",
        "model = ViTForImageClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels= len(valid_labels),\n",
        "    label2id = {label: str(i) for i, label in enumerate(valid_labels)},\n",
        "    id2label = {str(i): label for i, label in enumerate(valid_labels)},\n",
        "    problem_type = \"multi_label_classification\",\n",
        "    ignore_mismatched_sizes=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install accelerate -U"
      ],
      "metadata": {
        "id": "ojq7BsY6cbCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install transformers[torch]"
      ],
      "metadata": {
        "id": "gIOh_AB-cmS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNnG1GUUK7nj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18e2d5bf-9e2e-421e-cbad-cf28a717b63b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n",
            "PyTorch: setting up devices\n"
          ]
        }
      ],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "  output_dir=\"/content/Model/Models-Train-34\", # output directory\n",
        "  per_device_train_batch_size=32, # batch size per device during training\n",
        "  evaluation_strategy=\"steps\",    # evaluation strategy to adopt during training\n",
        "  num_train_epochs=25,             # total number of training epochs\n",
        "  # fp16=True,                    # use mixed precision\n",
        "  save_steps=4000,                # number of update steps before saving checkpoint\n",
        "  eval_steps=4000,                # number of update steps before evaluating\n",
        "  logging_steps=4000,             # number of update steps before logging\n",
        "  # save_steps=50,\n",
        "  # eval_steps=50,\n",
        "  # logging_steps=50,\n",
        "  save_total_limit=4,             # limit the total amount of checkpoints on disk\n",
        "  remove_unused_columns=False,    # remove unused columns from the dataset\n",
        "  push_to_hub=False,              # do not push the model to the hub\n",
        "  report_to='tensorboard',        # report metrics to tensorboard\n",
        "  load_best_model_at_end=True,    # load the best model at the end of training\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "id": "ASOtjmcHdc2R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "221bdb83-6211-4d33-866e-4f69acafe7e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['image', 'label'],\n",
              "    num_rows: 31104\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset"
      ],
      "metadata": {
        "id": "y3M13NlzsyNT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6757749c-80a4-49ac-e1d2-daa49c172064"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['image', 'label'],\n",
              "    num_rows: 15598\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3n8Cv48_QKzr"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                        # the instantiated 🤗 Transformers model to be trained\n",
        "    args=training_args,                 # training arguments, defined above\n",
        "    data_collator=collate_fn,           # the data collator that will be used for batching\n",
        "    compute_metrics=compute_metrics,    # the metrics function that will be used for evaluation\n",
        "    train_dataset=train_dataset,        # training dataset\n",
        "    eval_dataset=val_dataset,           # evaluation dataset\n",
        "    tokenizer=image_processor,          # the processor that will be used for preprocessing the images\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUUoQyh-QPED",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 863
        },
        "outputId": "5d8b86f1-67ba-41b8-86e7-eb7084dafc87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 31,104\n",
            "  Num Epochs = 25\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 24,300\n",
            "  Number of trainable parameters = 85,816,343\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='21231' max='24300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [21231/24300 8:38:04 < 1:14:53, 0.68 it/s, Epoch 21.84/25]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.197900</td>\n",
              "      <td>0.247535</td>\n",
              "      <td>0.134697</td>\n",
              "      <td>0.341829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.088100</td>\n",
              "      <td>0.336976</td>\n",
              "      <td>0.124567</td>\n",
              "      <td>0.361222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12000</td>\n",
              "      <td>0.022400</td>\n",
              "      <td>0.463562</td>\n",
              "      <td>0.126683</td>\n",
              "      <td>0.353561</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16000</td>\n",
              "      <td>0.003500</td>\n",
              "      <td>0.558390</td>\n",
              "      <td>0.123926</td>\n",
              "      <td>0.353041</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20000</td>\n",
              "      <td>0.001600</td>\n",
              "      <td>0.608531</td>\n",
              "      <td>0.125016</td>\n",
              "      <td>0.352843</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 15598\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /content/Model/Models-Train-34/checkpoint-4000\n",
            "Configuration saved in /content/Model/Models-Train-34/checkpoint-4000/config.json\n",
            "Model weights saved in /content/Model/Models-Train-34/checkpoint-4000/pytorch_model.bin\n",
            "Image processor saved in /content/Model/Models-Train-34/checkpoint-4000/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 15598\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /content/Model/Models-Train-34/checkpoint-8000\n",
            "Configuration saved in /content/Model/Models-Train-34/checkpoint-8000/config.json\n",
            "Model weights saved in /content/Model/Models-Train-34/checkpoint-8000/pytorch_model.bin\n",
            "Image processor saved in /content/Model/Models-Train-34/checkpoint-8000/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 15598\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /content/Model/Models-Train-34/checkpoint-12000\n",
            "Configuration saved in /content/Model/Models-Train-34/checkpoint-12000/config.json\n",
            "Model weights saved in /content/Model/Models-Train-34/checkpoint-12000/pytorch_model.bin\n",
            "Image processor saved in /content/Model/Models-Train-34/checkpoint-12000/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 15598\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /content/Model/Models-Train-34/checkpoint-16000\n",
            "Configuration saved in /content/Model/Models-Train-34/checkpoint-16000/config.json\n",
            "Model weights saved in /content/Model/Models-Train-34/checkpoint-16000/pytorch_model.bin\n",
            "Image processor saved in /content/Model/Models-Train-34/checkpoint-16000/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 15598\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /content/Model/Models-Train-34/checkpoint-20000\n",
            "Configuration saved in /content/Model/Models-Train-34/checkpoint-20000/config.json\n",
            "Model weights saved in /content/Model/Models-Train-34/checkpoint-20000/pytorch_model.bin\n",
            "Image processor saved in /content/Model/Models-Train-34/checkpoint-20000/preprocessor_config.json\n",
            "Deleting older checkpoint [/content/Model/Models-Train-34/checkpoint-8000] due to args.save_total_limit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Buffered data was truncated after reaching the output size limit."
          ]
        }
      ],
      "source": [
        "# start training\n",
        "trainer.train()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "aLyKHM-Oj0bW"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}