{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "NzBfAyc7apgv",
        "LWIDbmsuZ6yr"
      ],
      "authorship_tag": "ABX9TyPDCfU+2aV4ujD6OdqYJV3L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MuhammadIrzam447/MultiModel/blob/master/Train_36.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Notebook to Train and Evaluate ResNet and ViT on 100 and __% datasets residing in a single directory for Multi Class Classification"
      ],
      "metadata": {
        "id": "VxazLvh8JMJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"/content/Dataset(s)/joint_ferramenta/mulitmodal_img_enc_txt\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRJUSURVJsfk",
        "outputId": "2ad7c735-c9ba-4ed0-e91c-ea2c8e490fd2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Dataset(s)/joint_ferramenta/mulitmodal_img_enc_txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data\n"
      ],
      "metadata": {
        "id": "jNTJQ9CCMOuR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "\n",
        "# # Define the path to your dataset folder\n",
        "# dataset_folder = './images-train'\n",
        "\n",
        "# # Define the path for the output text file\n",
        "# output_file = './train_dataset.txt'\n",
        "\n",
        "# def custom_sort_key(item):\n",
        "#     return item.replace('@', '')\n",
        "\n",
        "# with open(output_file, 'w') as f:\n",
        "#     for label in sorted(os.listdir(dataset_folder), key=custom_sort_key):\n",
        "#         label_path = os.path.join(dataset_folder, label)\n",
        "#         if os.path.isdir(label_path):\n",
        "#             class_name = label\n",
        "#             image_files = sorted(os.listdir(label_path))\n",
        "#             for image_file in image_files:\n",
        "#                 image_path = os.path.join(label_path, image_file)\n",
        "#                 f.write(f'{image_path}, {class_name}\\n')"
      ],
      "metadata": {
        "id": "UGxumxKWLV6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Dataset"
      ],
      "metadata": {
        "id": "XNma768QgFfU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "image_file_paths = []\n",
        "image_genre_labels = []\n",
        "enc_file_paths = []\n",
        "enc_genre_labels = []\n",
        "\n",
        "labels_file = \"./train_dataset.txt\"\n",
        "\n",
        "with open(labels_file, 'r') as file:\n",
        "    for line in file:\n",
        "        parts = line.strip().split(', ')\n",
        "        filename = parts[0].strip()\n",
        "        labels = parts[1].strip()\n",
        "\n",
        "        if filename.endswith(\"_3.png\"):\n",
        "            enc_file_paths.append(filename)\n",
        "            enc_genre_labels.append(labels)\n",
        "\n",
        "        elif filename.endswith(\"_4.png\"):\n",
        "            image_file_paths.append(filename)\n",
        "            image_genre_labels.append(labels)"
      ],
      "metadata": {
        "id": "OEjnk0W-Pc8Y"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(image_file_paths), len(enc_file_paths)"
      ],
      "metadata": {
        "id": "MmYmWXRJRaXy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47e50d29-0b32-4806-fcc0-33845b2c2ac7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(66141, 66141)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_file_paths[0:5] , image_genre_labels[0:5]"
      ],
      "metadata": {
        "id": "UwLJAYq6dhLs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70e8875d-c1da-417f-f193-f02fe7e60802"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['./images-train/@accessori_saldatore/113267806.jpg_4.png',\n",
              "  './images-train/@accessori_saldatore/115061965.jpg_4.png',\n",
              "  './images-train/@accessori_saldatore/119306609.jpg_4.png',\n",
              "  './images-train/@accessori_saldatore/119392789.jpg_4.png',\n",
              "  './images-train/@accessori_saldatore/122140907.jpg_4.png'],\n",
              " ['@accessori_saldatore',\n",
              "  '@accessori_saldatore',\n",
              "  '@accessori_saldatore',\n",
              "  '@accessori_saldatore',\n",
              "  '@accessori_saldatore'])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enc_file_paths[0:5] , enc_genre_labels[0:5]"
      ],
      "metadata": {
        "id": "WxClniG0fpwN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fde89310-508b-4eb2-cc9b-1d90eacf0dc0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['./images-train/@accessori_saldatore/113267806.jpg_3.png',\n",
              "  './images-train/@accessori_saldatore/115061965.jpg_3.png',\n",
              "  './images-train/@accessori_saldatore/119306609.jpg_3.png',\n",
              "  './images-train/@accessori_saldatore/119392789.jpg_3.png',\n",
              "  './images-train/@accessori_saldatore/122140907.jpg_3.png'],\n",
              " ['@accessori_saldatore',\n",
              "  '@accessori_saldatore',\n",
              "  '@accessori_saldatore',\n",
              "  '@accessori_saldatore',\n",
              "  '@accessori_saldatore'])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_labels = set(enc_genre_labels)\n",
        "unique_labels_list = list(unique_labels)\n",
        "\n",
        "print(\"Unique Labels:\", unique_labels_list)"
      ],
      "metadata": {
        "id": "slLNEQBQS-i-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "568f3eff-3751-4691-bb61-de24a24bfeb7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique Labels: ['@cazzuola_frattone', '@lame_sega_circolare', '@spatola_raschietto', '@armadio_cassettiera', '@banco', '@staffa_squadretta', '@pinza_punzonatrice_giratubo', '@cassaforte', '@spazzola', '@sega-a-tazza', '@lima_grattuggia_raspa', '@collare', '@scalpello', '@lucchetto', '@cerniera_bandella', '@pinzetta', '@carta-vetrata_carta-abrasiva', '@chiave-a-bussola_tubo', '@accessori_saldatore', '@martello_mazzetta_ascia', '@torcia-faro', '@chiave_inglese_poligonale', '@accessori_trapano', '@livella', '@pennello', '@segnaletica', '@cacciavite_giravite_cercafase', '@brugola', '@rubinetto_miscelatore_valvola', '@scaffale', '@maniglia_pomello', '@ruota', '@morsetto_strettoio', '@chiodo_rivetto', '@pistola', '@tassello', '@carrello', '@coltello', '@forbice_cesoia', '@metro_flessometro', '@nastro', '@scala', '@vernice', '@bancale_cargo', '@catena', '@chiavistello', '@vite_bullone', '@colla_silicone', '@seghetto_sega', '@serratura_cilindro', '@lama_seghetto_sega', '@estrattore']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = len(unique_labels_list)\n",
        "num_classes"
      ],
      "metadata": {
        "id": "HX2Qa1xQZu6Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f31f6179-cd7c-4ea4-86bb-e5fc7f956a14"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "52"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "\n",
        "percentage = 0.3\n",
        "\n",
        "label_counts = {label: 0 for label in unique_labels_list}\n",
        "\n",
        "num_samples_to_include_per_label = {}\n",
        "for label in unique_labels_list:\n",
        "    label_count = enc_genre_labels.count(label)\n",
        "    num_samples_to_include = int(label_count * percentage)\n",
        "    num_samples_to_include_per_label[label] = num_samples_to_include\n",
        "\n",
        "\n",
        "filtered_enc_file_paths = []\n",
        "filtered_enc_genre_labels = []\n",
        "\n",
        "\n",
        "selected_samples_per_label = {label: 0 for label in unique_labels_list}\n",
        "for path, label in zip(enc_file_paths, enc_genre_labels):\n",
        "    if selected_samples_per_label[label] < num_samples_to_include_per_label[label]:\n",
        "        filtered_enc_file_paths.append(path)\n",
        "        filtered_enc_genre_labels.append(label)\n",
        "        selected_samples_per_label[label] += 1"
      ],
      "metadata": {
        "id": "Q9xQzx8MV3_U"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(filtered_enc_file_paths)"
      ],
      "metadata": {
        "id": "_JgyuBkqf45r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cf2e89f-4f6a-4e7b-aca3-82ebdc532105"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19818"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_enc_file_paths[0:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgjQxqstrBhx",
        "outputId": "d12d0d73-71fa-4083-9b47-b3c910826642"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./images-train/@accessori_saldatore/113267806.jpg_3.png',\n",
              " './images-train/@accessori_saldatore/115061965.jpg_3.png',\n",
              " './images-train/@accessori_saldatore/119306609.jpg_3.png',\n",
              " './images-train/@accessori_saldatore/119392789.jpg_3.png',\n",
              " './images-train/@accessori_saldatore/122140907.jpg_3.png']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new lists to combine data from both lists\n",
        "train_file_paths = image_file_paths + filtered_enc_file_paths\n",
        "train_genre_labels = image_genre_labels + filtered_enc_genre_labels"
      ],
      "metadata": {
        "id": "2WTpdvsOXRgD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_file_paths)"
      ],
      "metadata": {
        "id": "72g7sZu8gAXW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e64021e1-0864-438c-a359-24823ca429a0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "85959"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Dataset"
      ],
      "metadata": {
        "id": "7D26BNoJbJ3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "test_image_file_paths = []\n",
        "test_image_genre_labels = []\n",
        "test_enc_file_paths = []\n",
        "test_enc_genre_labels = []\n",
        "\n",
        "test_labels_file = \"./test_dataset.txt\"\n",
        "\n",
        "with open(test_labels_file, 'r') as file:\n",
        "    for line in file:\n",
        "        parts = line.strip().split(', ')\n",
        "        filename = parts[0].strip()\n",
        "        labels = parts[1].strip()\n",
        "\n",
        "        if filename.endswith(\"_3.png\"):\n",
        "            test_enc_file_paths.append(filename)\n",
        "            test_enc_genre_labels.append(labels)\n",
        "\n",
        "        elif filename.endswith(\"_4.png\"):\n",
        "            test_image_file_paths.append(filename)\n",
        "            test_image_genre_labels.append(labels)"
      ],
      "metadata": {
        "id": "LnGxlWzqbLIj"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_image_file_paths), len(test_enc_file_paths)"
      ],
      "metadata": {
        "id": "yd9_b3EfbLdS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "030bec14-7fdc-40fd-a8bc-745d17f5c41b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(21869, 21869)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_image_file_paths[0:5] , test_image_genre_labels[0:5]"
      ],
      "metadata": {
        "id": "Kj5sNm7ngOhz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9723fb5-f9d1-4625-efeb-0ad26ffc136d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['./images-val/@accessori_saldatore/123031564.jpg_4.png',\n",
              "  './images-val/@accessori_saldatore/125066170.jpg_4.png',\n",
              "  './images-val/@accessori_saldatore/128436452.jpg_4.png',\n",
              "  './images-val/@accessori_saldatore/129979053.jpg_4.png',\n",
              "  './images-val/@accessori_saldatore/130236413.jpg_4.png'],\n",
              " ['@accessori_saldatore',\n",
              "  '@accessori_saldatore',\n",
              "  '@accessori_saldatore',\n",
              "  '@accessori_saldatore',\n",
              "  '@accessori_saldatore'])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "label_counts = {label: 0 for label in unique_labels_list}\n",
        "\n",
        "test_num_samples_to_include_per_label = {}\n",
        "for label in unique_labels_list:\n",
        "    label_count = test_enc_genre_labels.count(label)\n",
        "    num_samples_to_include = int(label_count * percentage)\n",
        "    test_num_samples_to_include_per_label[label] = num_samples_to_include\n",
        "\n",
        "\n",
        "test_filtered_enc_file_paths = []\n",
        "test_filtered_enc_genre_labels = []\n",
        "\n",
        "\n",
        "test_selected_samples_per_label = {label: 0 for label in unique_labels_list}\n",
        "for path, label in zip(test_enc_file_paths, test_enc_genre_labels):\n",
        "    if test_selected_samples_per_label[label] < test_num_samples_to_include_per_label[label]:\n",
        "        test_filtered_enc_file_paths.append(path)\n",
        "        test_filtered_enc_genre_labels.append(label)\n",
        "        test_selected_samples_per_label[label] += 1"
      ],
      "metadata": {
        "id": "rHhvWppqbLxi"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_filtered_enc_file_paths)"
      ],
      "metadata": {
        "id": "ZplC_8mxgfng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a5abae7-45c6-4654-f62c-4bd0b0674489"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6533"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new lists to combine data from both lists\n",
        "test_file_paths = test_image_file_paths + test_filtered_enc_file_paths\n",
        "test_genre_labels = test_image_genre_labels + test_filtered_enc_genre_labels"
      ],
      "metadata": {
        "id": "C6sjf4Ptbcdf"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_file_paths)"
      ],
      "metadata": {
        "id": "T0OH11Z2OJ_k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce77072e-5e35-42e0-906f-4277468b6807"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28402"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ResNet"
      ],
      "metadata": {
        "id": "NzBfAyc7apgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, file_paths, labels, transform=None):\n",
        "        self.file_paths = file_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.file_paths[idx]\n",
        "        image = Image.open(img_path)\n",
        "        label = int(self.labels[idx])\n",
        "        # label = torch.tensor(label,dtype=torch.float32)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "3Y7VXqUBYf_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CustomDataset(train_file_paths, train_genre_labels, transform=transform)\n",
        "test_dataset = CustomDataset(test_file_paths, test_genre_labels, transform=transform)"
      ],
      "metadata": {
        "id": "CjVU94XRZOXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 32  # Adjust as needed\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "niKEq0PUZVan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "resnet = models.resnet101(pretrained=True)\n",
        "num_features = resnet.fc.in_features\n",
        "resnet.fc = nn.Linear(num_features, num_classes)  # num_classes is the number of classes in your dataset\n",
        "resnet.to(device)\n",
        "print(resnet)\n"
      ],
      "metadata": {
        "id": "WkU78f5UZXqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.SGD(resnet.parameters(), lr=0.001, momentum=0.9)\n",
        "optimizer = optim.Adam(resnet.parameters(), lr=0.001)\n",
        "num_epochs = 30"
      ],
      "metadata": {
        "id": "kQnCFcJCZx5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, roc_auc_score\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "6Q79jKkChops"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_loss = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    resnet.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = resnet(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    training_loss.append(epoch_loss)\n",
        "\n",
        "    print(\"Training Loss==========================>>\")\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} Training Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    resnet.eval()\n",
        "    predicted_classes = []\n",
        "    actual_labels = []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = resnet(images)\n",
        "\n",
        "            _, predicted_label = torch.max(outputs, 1)\n",
        "\n",
        "            predicted_classes.extend(predicted_label.cpu().tolist())\n",
        "            actual_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "    accuracy = accuracy_score(actual_labels, predicted_classes)\n",
        "    # precision = precision_score(actual_labels, predicted_classes, average='weighted')\n",
        "    # recall = recall_score(actual_labels, predicted_classes, average='weighted')\n",
        "    # f1 = f1_score(actual_labels, predicted_classes, average='weighted')\n",
        "\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    # print(\"Precision:\", precision)\n",
        "    # print(\"Recall:\", recall)\n",
        "    # print(\"F1-score:\", f1)\n",
        "\n",
        "    print(classification_report(actual_labels, predicted_classes))\n",
        "    # print(\"Confusion Matrix: \")\n",
        "    # cm = confusion_matrix(actual_labels, predicted_classes)\n",
        "    # print(cm)\n",
        "    predicted_classes = np.array(predicted_classes)\n",
        "    actual_labels = np.array(actual_labels)\n",
        "    print(roc_auc_score(actual_labels, predicted_classes))\n",
        "\n",
        "\n",
        "    save_dir = \"/content/Model/Models-Train-30/\"\n",
        "    os.makedirs(save_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
        "\n",
        "    model_name = str(epoch+1) + \"_model.pth\"\n",
        "    save_path = os.path.join(save_dir, model_name)  # Specify the complete path to the model file\n",
        "    torch.save(resnet.state_dict(), save_path)"
      ],
      "metadata": {
        "id": "X70EPWQbZ_id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ViT"
      ],
      "metadata": {
        "id": "VmLtbV7uZiUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# !pip install transformers evaluate datasets\n",
        "import requests\n",
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import *\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "uO3ZsSpNeu9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model_name = \"google/vit-base-patch16-224\"\n",
        "image_processor = ViTImageProcessor.from_pretrained(model_name)\n",
        "model = ViTForImageClassification.from_pretrained(model_name).to(device)"
      ],
      "metadata": {
        "id": "5kwp9qmMZjkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Label Encoding"
      ],
      "metadata": {
        "id": "UGs6LJI9guRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "transformed_train_genre_labels = label_encoder.fit_transform(train_genre_labels)"
      ],
      "metadata": {
        "id": "x5qr0pvBcKi0"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# label_to_id_mapping = {label: id for label, id in zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))}\n",
        "# label_to_id_mapping"
      ],
      "metadata": {
        "id": "AhfBYrKeYoUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder1 = LabelEncoder()\n",
        "transformed_test_genre_labels = label_encoder1.fit_transform(test_genre_labels)"
      ],
      "metadata": {
        "id": "VJ-m2Q5rgsrj"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_to_id_mapping = {label: id for label, id in zip(label_encoder1.classes_, label_encoder1.transform(label_encoder1.classes_))}\n",
        "id_to_label_mapping = {id: label for id, label in enumerate(label_encoder1.classes_)}"
      ],
      "metadata": {
        "id": "jsF-RjLRYbcN"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert int64 to int in id_to_label_mapping and label_to_id_mapping\n",
        "id_to_label_mapping = {int(id): label for id, label in id_to_label_mapping.items()}\n",
        "label_to_id_mapping = {label: int(id) for label, id in label_to_id_mapping.items()}"
      ],
      "metadata": {
        "id": "Hnad1bYLakGs"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id_to_label_mapping"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUXBAY3Xa3lo",
        "outputId": "9fb79040-c714-4596-b931-4378b6dbf1ec"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: '@accessori_saldatore',\n",
              " 1: '@accessori_trapano',\n",
              " 2: '@armadio_cassettiera',\n",
              " 3: '@bancale_cargo',\n",
              " 4: '@banco',\n",
              " 5: '@brugola',\n",
              " 6: '@cacciavite_giravite_cercafase',\n",
              " 7: '@carrello',\n",
              " 8: '@carta-vetrata_carta-abrasiva',\n",
              " 9: '@cassaforte',\n",
              " 10: '@catena',\n",
              " 11: '@cazzuola_frattone',\n",
              " 12: '@cerniera_bandella',\n",
              " 13: '@chiave-a-bussola_tubo',\n",
              " 14: '@chiave_inglese_poligonale',\n",
              " 15: '@chiavistello',\n",
              " 16: '@chiodo_rivetto',\n",
              " 17: '@colla_silicone',\n",
              " 18: '@collare',\n",
              " 19: '@coltello',\n",
              " 20: '@estrattore',\n",
              " 21: '@forbice_cesoia',\n",
              " 22: '@lama_seghetto_sega',\n",
              " 23: '@lame_sega_circolare',\n",
              " 24: '@lima_grattuggia_raspa',\n",
              " 25: '@livella',\n",
              " 26: '@lucchetto',\n",
              " 27: '@maniglia_pomello',\n",
              " 28: '@martello_mazzetta_ascia',\n",
              " 29: '@metro_flessometro',\n",
              " 30: '@morsetto_strettoio',\n",
              " 31: '@nastro',\n",
              " 32: '@pennello',\n",
              " 33: '@pinza_punzonatrice_giratubo',\n",
              " 34: '@pinzetta',\n",
              " 35: '@pistola',\n",
              " 36: '@rubinetto_miscelatore_valvola',\n",
              " 37: '@ruota',\n",
              " 38: '@scaffale',\n",
              " 39: '@scala',\n",
              " 40: '@scalpello',\n",
              " 41: '@sega-a-tazza',\n",
              " 42: '@seghetto_sega',\n",
              " 43: '@segnaletica',\n",
              " 44: '@serratura_cilindro',\n",
              " 45: '@spatola_raschietto',\n",
              " 46: '@spazzola',\n",
              " 47: '@staffa_squadretta',\n",
              " 48: '@tassello',\n",
              " 49: '@torcia-faro',\n",
              " 50: '@vernice',\n",
              " 51: '@vite_bullone'}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_genre_labels), len(transformed_train_genre_labels)"
      ],
      "metadata": {
        "id": "Q8iTY22HfXsS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0bec980-2d1a-4173-bb16-2880792c1798"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(85959, 85959)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_genre_labels), len(transformed_test_genre_labels)"
      ],
      "metadata": {
        "id": "ClRZgMkgg4Sw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58016b26-6425-454d-9ec5-f3b27c9f897a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28402, 28402)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Creating"
      ],
      "metadata": {
        "id": "e297rXtbgwdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "train_data = {'image': train_file_paths, 'label': transformed_train_genre_labels}\n",
        "ds_train = Dataset.from_dict(train_data)"
      ],
      "metadata": {
        "id": "6XKv_SEBa6Sz"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_data = {'image': test_file_paths, 'label': transformed_test_genre_labels}\n",
        "ds_val = Dataset.from_dict(val_data)"
      ],
      "metadata": {
        "id": "djhbA0nCaDgq"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import PIL.Image as pil\n",
        "\n",
        "def transform(examples):\n",
        "  inputs = image_processor([pil.open(img).convert(\"RGB\") for img in examples[\"image\"]], return_tensors=\"pt\")\n",
        "  inputs[\"labels\"] = examples[\"label\"]\n",
        "  return inputs"
      ],
      "metadata": {
        "id": "9dEy0tTcbTfJ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = ds_train.with_transform(transform)\n",
        "val_dataset = ds_val.with_transform(transform)"
      ],
      "metadata": {
        "id": "owz6NIGTbVA7"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "id": "pet0z7BJdDZO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dab4e940-419c-4910-8e57-8d7f7c0b557a"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['image', 'label'],\n",
              "    num_rows: 85959\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset"
      ],
      "metadata": {
        "id": "00nnqJFvdHIj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7be1ab4-9867-471a-8386-1de1cf1a3387"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['image', 'label'],\n",
              "    num_rows: 28402\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for item in val_dataset:\n",
        "  print(item['pixel_values'].shape)\n",
        "  print(item[\"labels\"])\n",
        "  break"
      ],
      "metadata": {
        "id": "sPsIroeXbjlY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e68f961-3fa6-416e-a3be-ff43d41c6206"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 224, 224])\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def collate_fn(batch):\n",
        "  return {\n",
        "      \"pixel_values\": torch.stack([x[\"pixel_values\"] for x in batch]),\n",
        "      \"labels\": torch.tensor([x[\"labels\"] for x in batch]),\n",
        "  }"
      ],
      "metadata": {
        "id": "22a48uX4bnZ0"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the ViT model\n",
        "model = ViTForImageClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=len(unique_labels_list),\n",
        "    # id2label={str(i): c for i, c in enumerate(unique_labels_list)},\n",
        "    id2label=id_to_label_mapping,\n",
        "    label2id=label_to_id_mapping,\n",
        "    # label2id={c: str(i) for i, c in enumerate(unique_labels_list)},\n",
        "    ignore_mismatched_sizes=True,\n",
        ")"
      ],
      "metadata": {
        "id": "sIQenfhHbz_N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de80892d-4462-41a8-92f6-b1c653770669"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--vit-base-patch16-224/snapshots/3f49326eb077187dfe1c2a2bb15fbd74e6ab91e3/config.json\n",
            "Model config ViTConfig {\n",
            "  \"_name_or_path\": \"google/vit-base-patch16-224\",\n",
            "  \"architectures\": [\n",
            "    \"ViTForImageClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.0,\n",
            "  \"encoder_stride\": 16,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.0,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"@accessori_saldatore\",\n",
            "    \"1\": \"@accessori_trapano\",\n",
            "    \"2\": \"@armadio_cassettiera\",\n",
            "    \"3\": \"@bancale_cargo\",\n",
            "    \"4\": \"@banco\",\n",
            "    \"5\": \"@brugola\",\n",
            "    \"6\": \"@cacciavite_giravite_cercafase\",\n",
            "    \"7\": \"@carrello\",\n",
            "    \"8\": \"@carta-vetrata_carta-abrasiva\",\n",
            "    \"9\": \"@cassaforte\",\n",
            "    \"10\": \"@catena\",\n",
            "    \"11\": \"@cazzuola_frattone\",\n",
            "    \"12\": \"@cerniera_bandella\",\n",
            "    \"13\": \"@chiave-a-bussola_tubo\",\n",
            "    \"14\": \"@chiave_inglese_poligonale\",\n",
            "    \"15\": \"@chiavistello\",\n",
            "    \"16\": \"@chiodo_rivetto\",\n",
            "    \"17\": \"@colla_silicone\",\n",
            "    \"18\": \"@collare\",\n",
            "    \"19\": \"@coltello\",\n",
            "    \"20\": \"@estrattore\",\n",
            "    \"21\": \"@forbice_cesoia\",\n",
            "    \"22\": \"@lama_seghetto_sega\",\n",
            "    \"23\": \"@lame_sega_circolare\",\n",
            "    \"24\": \"@lima_grattuggia_raspa\",\n",
            "    \"25\": \"@livella\",\n",
            "    \"26\": \"@lucchetto\",\n",
            "    \"27\": \"@maniglia_pomello\",\n",
            "    \"28\": \"@martello_mazzetta_ascia\",\n",
            "    \"29\": \"@metro_flessometro\",\n",
            "    \"30\": \"@morsetto_strettoio\",\n",
            "    \"31\": \"@nastro\",\n",
            "    \"32\": \"@pennello\",\n",
            "    \"33\": \"@pinza_punzonatrice_giratubo\",\n",
            "    \"34\": \"@pinzetta\",\n",
            "    \"35\": \"@pistola\",\n",
            "    \"36\": \"@rubinetto_miscelatore_valvola\",\n",
            "    \"37\": \"@ruota\",\n",
            "    \"38\": \"@scaffale\",\n",
            "    \"39\": \"@scala\",\n",
            "    \"40\": \"@scalpello\",\n",
            "    \"41\": \"@sega-a-tazza\",\n",
            "    \"42\": \"@seghetto_sega\",\n",
            "    \"43\": \"@segnaletica\",\n",
            "    \"44\": \"@serratura_cilindro\",\n",
            "    \"45\": \"@spatola_raschietto\",\n",
            "    \"46\": \"@spazzola\",\n",
            "    \"47\": \"@staffa_squadretta\",\n",
            "    \"48\": \"@tassello\",\n",
            "    \"49\": \"@torcia-faro\",\n",
            "    \"50\": \"@vernice\",\n",
            "    \"51\": \"@vite_bullone\"\n",
            "  },\n",
            "  \"image_size\": 224,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"@accessori_saldatore\": 0,\n",
            "    \"@accessori_trapano\": 1,\n",
            "    \"@armadio_cassettiera\": 2,\n",
            "    \"@bancale_cargo\": 3,\n",
            "    \"@banco\": 4,\n",
            "    \"@brugola\": 5,\n",
            "    \"@cacciavite_giravite_cercafase\": 6,\n",
            "    \"@carrello\": 7,\n",
            "    \"@carta-vetrata_carta-abrasiva\": 8,\n",
            "    \"@cassaforte\": 9,\n",
            "    \"@catena\": 10,\n",
            "    \"@cazzuola_frattone\": 11,\n",
            "    \"@cerniera_bandella\": 12,\n",
            "    \"@chiave-a-bussola_tubo\": 13,\n",
            "    \"@chiave_inglese_poligonale\": 14,\n",
            "    \"@chiavistello\": 15,\n",
            "    \"@chiodo_rivetto\": 16,\n",
            "    \"@colla_silicone\": 17,\n",
            "    \"@collare\": 18,\n",
            "    \"@coltello\": 19,\n",
            "    \"@estrattore\": 20,\n",
            "    \"@forbice_cesoia\": 21,\n",
            "    \"@lama_seghetto_sega\": 22,\n",
            "    \"@lame_sega_circolare\": 23,\n",
            "    \"@lima_grattuggia_raspa\": 24,\n",
            "    \"@livella\": 25,\n",
            "    \"@lucchetto\": 26,\n",
            "    \"@maniglia_pomello\": 27,\n",
            "    \"@martello_mazzetta_ascia\": 28,\n",
            "    \"@metro_flessometro\": 29,\n",
            "    \"@morsetto_strettoio\": 30,\n",
            "    \"@nastro\": 31,\n",
            "    \"@pennello\": 32,\n",
            "    \"@pinza_punzonatrice_giratubo\": 33,\n",
            "    \"@pinzetta\": 34,\n",
            "    \"@pistola\": 35,\n",
            "    \"@rubinetto_miscelatore_valvola\": 36,\n",
            "    \"@ruota\": 37,\n",
            "    \"@scaffale\": 38,\n",
            "    \"@scala\": 39,\n",
            "    \"@scalpello\": 40,\n",
            "    \"@sega-a-tazza\": 41,\n",
            "    \"@seghetto_sega\": 42,\n",
            "    \"@segnaletica\": 43,\n",
            "    \"@serratura_cilindro\": 44,\n",
            "    \"@spatola_raschietto\": 45,\n",
            "    \"@spazzola\": 46,\n",
            "    \"@staffa_squadretta\": 47,\n",
            "    \"@tassello\": 48,\n",
            "    \"@torcia-faro\": 49,\n",
            "    \"@vernice\": 50,\n",
            "    \"@vite_bullone\": 51\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"model_type\": \"vit\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_channels\": 3,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"patch_size\": 16,\n",
            "  \"qkv_bias\": true,\n",
            "  \"transformers_version\": \"4.32.1\"\n",
            "}\n",
            "\n",
            "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--google--vit-base-patch16-224/snapshots/3f49326eb077187dfe1c2a2bb15fbd74e6ab91e3/model.safetensors\n",
            "All model checkpoint weights were used when initializing ViTForImageClassification.\n",
            "\n",
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
            "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([52]) in the model instantiated\n",
            "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([52, 768]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation Metric"
      ],
      "metadata": {
        "id": "KLVUJAqgcCRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from evaluate import load\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# load the accuracy and f1 metrics from the evaluate module\n",
        "accuracy = load(\"accuracy\")\n",
        "f1 = load(\"f1\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "  # compute the accuracy and f1 scores & return them\n",
        "  accuracy_score = accuracy.compute(predictions=np.argmax(eval_pred.predictions, axis=1), references=eval_pred.label_ids)\n",
        "  f1_score = f1.compute(predictions=np.argmax(eval_pred.predictions, axis=1), references=eval_pred.label_ids, average=\"macro\")\n",
        "\n",
        "  # auroc_score = roc_auc_score(eval_pred.label_ids, np.argmax(eval_pred.predictions, axis=1))\n",
        "  # print(f\"AUROC Score: {auroc_score:.4f}\")\n",
        "\n",
        "  return {**accuracy_score, **f1_score}"
      ],
      "metadata": {
        "id": "23DkFdTNcCtE"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "FuDfvAopcHUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install accelerate -U\n",
        "# !pip install transformers[torch]"
      ],
      "metadata": {
        "id": "665OJ9i4cudJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "  output_dir=\"/content/Model/Models-Train-36\", # output directory\n",
        "  per_device_train_batch_size=32, # batch size per device during training\n",
        "  evaluation_strategy=\"steps\",    # evaluation strategy to adopt during training\n",
        "  num_train_epochs=20,             # total number of training epochs\n",
        "  # fp16=True,                    # use mixed precision\n",
        "  save_steps=8000,                # number of update steps before saving checkpoint\n",
        "  eval_steps=8000,                # number of update steps before evaluating\n",
        "  logging_steps=8000,             # number of update steps before logging\n",
        "  # save_steps=50,\n",
        "  # eval_steps=50,\n",
        "  # logging_steps=50,\n",
        "  save_total_limit=4,             # limit the total amount of checkpoints on disk\n",
        "  remove_unused_columns=False,    # remove unused columns from the dataset\n",
        "  push_to_hub=False,              # do not push the model to the hub\n",
        "  report_to='tensorboard',        # report metrics to tensorboard\n",
        "  load_best_model_at_end=True,    # load the best model at the end of training\n",
        ")"
      ],
      "metadata": {
        "id": "xApZuICNcqvR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38ec248c-7c19-42ec-a5c6-e62c8513e011"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n",
            "PyTorch: setting up devices\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                        # the instantiated 🤗 Transformers model to be trained\n",
        "    args=training_args,                 # training arguments, defined above\n",
        "    data_collator=collate_fn,           # the data collator that will be used for batching\n",
        "    compute_metrics=compute_metrics,    # the metrics function that will be used for evaluation\n",
        "    train_dataset=train_dataset,        # training dataset\n",
        "    eval_dataset=val_dataset,           # evaluation dataset\n",
        "    tokenizer=image_processor,          # the processor that will be used for preprocessing the images\n",
        ")"
      ],
      "metadata": {
        "id": "CDA6IEUzc6M8"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "0CI-Z4G_c9WT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 962
        },
        "outputId": "35a2c040-d118-4f24-b190-31b2678aa01d"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 85,959\n",
            "  Num Epochs = 20\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 53,740\n",
            "  Number of trainable parameters = 85,838,644\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='18031' max='53740' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [18031/53740 7:23:29 < 14:38:23, 0.68 it/s, Epoch 6.71/20]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.297200</td>\n",
              "      <td>0.266801</td>\n",
              "      <td>0.933244</td>\n",
              "      <td>0.901933</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16000</td>\n",
              "      <td>0.056600</td>\n",
              "      <td>0.304085</td>\n",
              "      <td>0.939863</td>\n",
              "      <td>0.912514</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 28402\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /content/Model/Models-Train-36/checkpoint-8000\n",
            "Configuration saved in /content/Model/Models-Train-36/checkpoint-8000/config.json\n",
            "Model weights saved in /content/Model/Models-Train-36/checkpoint-8000/pytorch_model.bin\n",
            "Image processor saved in /content/Model/Models-Train-36/checkpoint-8000/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 28402\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /content/Model/Models-Train-36/checkpoint-16000\n",
            "Configuration saved in /content/Model/Models-Train-36/checkpoint-16000/config.json\n",
            "Model weights saved in /content/Model/Models-Train-36/checkpoint-16000/pytorch_model.bin\n",
            "Image processor saved in /content/Model/Models-Train-36/checkpoint-16000/preprocessor_config.json\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-3435b262f1ae>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1555\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1556\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1557\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1840\u001b[0m                     \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging_nan_inf_filter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1841\u001b[0m                     \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_torch_tpu_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1842\u001b[0;31m                     \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1843\u001b[0m                 ):\n\u001b[1;32m   1844\u001b[0m                     \u001b[0;31m# if loss is nan or inf simply add the average of previous logged losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Best Model Selection"
      ],
      "metadata": {
        "id": "LWIDbmsuZ6yr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 32\n",
        "valid_dataset_loader = DataLoader(test_dataset, collate_fn=collate_fn, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "j6YOll-WZ7RK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = 9000"
      ],
      "metadata": {
        "id": "b_DhOKQqcRsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
        "\n",
        "while checkpoint <= 45000:\n",
        "    model = ViTForImageClassification.from_pretrained(f\"/content/Model/Models-Train-28/checkpoint-{checkpoint}\").to(device)\n",
        "    image_processor = ViTImageProcessor.from_pretrained(f\"/content/Model/Models-Train-28/checkpoint-{checkpoint}\")\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    predictions, labels = [], []\n",
        "\n",
        "    for batch in valid_dataset_loader:\n",
        "        pixel_values = batch[\"pixel_values\"].to(device)\n",
        "        label_ids = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(pixel_values=pixel_values, labels=label_ids)\n",
        "        logits = outputs.logits.detach().cpu()\n",
        "\n",
        "        predictions.extend(logits.argmax(dim=-1).tolist())\n",
        "        labels.extend(label_ids.tolist())\n",
        "\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    precision = precision_score(labels, predictions, average='weighted')\n",
        "    recall = recall_score(labels, predictions, average='weighted')\n",
        "    f1 = f1_score(labels, predictions, average='weighted')\n",
        "\n",
        "    print(\"Accuracy: \", accuracy)\n",
        "    print(\"precision: \", precision)\n",
        "    print(\"f1_score: \", f1)\n",
        "    print(\"recall\", recall)\n",
        "    print(classification_report(labels, predictions))\n",
        "    checkpoint = checkpoint + 9000"
      ],
      "metadata": {
        "id": "OLAwNwLPbe8I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}