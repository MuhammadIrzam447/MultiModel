{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "NzBfAyc7apgv"
      ],
      "authorship_tag": "ABX9TyP7hwOjOkuY9B6Qas37w4ux",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MuhammadIrzam447/MultiModel/blob/master/Train_27.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data\n"
      ],
      "metadata": {
        "id": "jNTJQ9CCMOuR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "\n",
        "# # Define the path to your dataset folder\n",
        "# dataset_folder = '/content/Dataset(s)/jointHF-train+test_unseen/test'\n",
        "\n",
        "# # Define the path for the output text file\n",
        "# output_file = 'test_dataset(2).txt'\n",
        "\n",
        "# # Open the output file for writing\n",
        "# with open(output_file, 'w') as f:\n",
        "#     # Iterate through the subdirectories in the dataset folder\n",
        "#     for label in os.listdir(dataset_folder):\n",
        "#         label_path = os.path.join(dataset_folder, label)\n",
        "\n",
        "#         # Check if it's a directory (class folder)\n",
        "#         if os.path.isdir(label_path):\n",
        "#             # Get the label (class name) from the folder name\n",
        "#             class_name = label\n",
        "\n",
        "#             # Iterate through the images in the class folder\n",
        "#             for image_file in os.listdir(label_path):\n",
        "#                 image_path = os.path.join(label_path, image_file)\n",
        "\n",
        "#                 # Write the image path and label to the output file\n",
        "#                 f.write(f'{image_path}, {class_name}\\n')\n"
      ],
      "metadata": {
        "id": "3ScHa74gfAXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Dataset"
      ],
      "metadata": {
        "id": "XNma768QgFfU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "image_file_paths = []\n",
        "image_genre_labels = []\n",
        "enc_file_paths = []\n",
        "enc_genre_labels = []\n",
        "\n",
        "labels_file = \"/content/Dataset(s)/jointHF-train+test_unseen/train_dataset(2).txt\"\n",
        "\n",
        "with open(labels_file, 'r') as file:\n",
        "    for line in file:\n",
        "        parts = line.strip().split(', ')\n",
        "        filename = parts[0].strip()\n",
        "        labels = parts[1].strip()\n",
        "\n",
        "        if filename.endswith(\"_3.png\"):\n",
        "            enc_file_paths.append(filename)\n",
        "            enc_genre_labels.append(labels)\n",
        "\n",
        "        elif filename.endswith(\"_4.png\"):\n",
        "            image_file_paths.append(filename)\n",
        "            image_genre_labels.append(labels)"
      ],
      "metadata": {
        "id": "OEjnk0W-Pc8Y"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(image_file_paths), len(enc_file_paths)"
      ],
      "metadata": {
        "id": "MmYmWXRJRaXy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79897a4d-a2c7-4548-e290-50dee5617dd3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9500, 9500)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_file_paths[0:5] , image_genre_labels[0:5]"
      ],
      "metadata": {
        "id": "UwLJAYq6dhLs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c190be9-87bd-4f1b-be1b-f90d7723dcdc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['/content/Dataset(s)/jointHF-train+test_unseen/train/1/10547.jpg_4.png',\n",
              "  '/content/Dataset(s)/jointHF-train+test_unseen/train/1/29380.jpg_4.png',\n",
              "  '/content/Dataset(s)/jointHF-train+test_unseen/train/1/53927.jpg_4.png',\n",
              "  '/content/Dataset(s)/jointHF-train+test_unseen/train/1/23485.jpg_4.png',\n",
              "  '/content/Dataset(s)/jointHF-train+test_unseen/train/1/26495.jpg_4.png'],\n",
              " ['1', '1', '1', '1', '1'])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enc_file_paths[0:5] , enc_genre_labels[0:5]"
      ],
      "metadata": {
        "id": "WxClniG0fpwN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cf1bf46-7ef0-4abf-e30b-762ce18b6214"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['/content/Dataset(s)/jointHF-train+test_unseen/train/1/70623.jpg_3.png',\n",
              "  '/content/Dataset(s)/jointHF-train+test_unseen/train/1/25314.jpg_3.png',\n",
              "  '/content/Dataset(s)/jointHF-train+test_unseen/train/1/08637.jpg_3.png',\n",
              "  '/content/Dataset(s)/jointHF-train+test_unseen/train/1/72509.jpg_3.png',\n",
              "  '/content/Dataset(s)/jointHF-train+test_unseen/train/1/45802.jpg_3.png'],\n",
              " ['1', '1', '1', '1', '1'])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_labels = set(enc_genre_labels)\n",
        "unique_labels_list = list(unique_labels)\n",
        "\n",
        "print(\"Unique Labels:\", unique_labels_list)"
      ],
      "metadata": {
        "id": "slLNEQBQS-i-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5ad7a89-279a-4c76-9a97-c344b1fadcd1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique Labels: ['0', '1']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = len(unique_labels_list)"
      ],
      "metadata": {
        "id": "HX2Qa1xQZu6Y"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "\n",
        "percentage = 0.3\n",
        "\n",
        "label_counts = {label: 0 for label in unique_labels_list}\n",
        "\n",
        "num_samples_to_include_per_label = {}\n",
        "for label in unique_labels_list:\n",
        "    label_count = enc_genre_labels.count(label)\n",
        "    num_samples_to_include = int(label_count * percentage)\n",
        "    num_samples_to_include_per_label[label] = num_samples_to_include\n",
        "\n",
        "\n",
        "filtered_enc_file_paths = []\n",
        "filtered_enc_genre_labels = []\n",
        "\n",
        "\n",
        "selected_samples_per_label = {label: 0 for label in unique_labels_list}\n",
        "for path, label in zip(enc_file_paths, enc_genre_labels):\n",
        "    if selected_samples_per_label[label] < num_samples_to_include_per_label[label]:\n",
        "        filtered_enc_file_paths.append(path)\n",
        "        filtered_enc_genre_labels.append(label)\n",
        "        selected_samples_per_label[label] += 1"
      ],
      "metadata": {
        "id": "Q9xQzx8MV3_U"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(filtered_enc_file_paths)"
      ],
      "metadata": {
        "id": "_JgyuBkqf45r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b3f9f40-100c-46a7-ef87-62e7ab1bd5fe"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2850"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new lists to combine data from both lists\n",
        "train_file_paths = image_file_paths + filtered_enc_file_paths\n",
        "train_genre_labels = image_genre_labels + filtered_enc_genre_labels"
      ],
      "metadata": {
        "id": "2WTpdvsOXRgD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_file_paths)"
      ],
      "metadata": {
        "id": "72g7sZu8gAXW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00f45d5e-81f2-4079-b4f1-f95535748c52"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12350"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Dataset"
      ],
      "metadata": {
        "id": "7D26BNoJbJ3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "test_image_file_paths = []\n",
        "test_image_genre_labels = []\n",
        "test_enc_file_paths = []\n",
        "test_enc_genre_labels = []\n",
        "\n",
        "test_labels_file = \"/content/Dataset(s)/jointHF-train+test_unseen/test_dataset(2).txt\"\n",
        "\n",
        "with open(test_labels_file, 'r') as file:\n",
        "    for line in file:\n",
        "        parts = line.strip().split(', ')\n",
        "        filename = parts[0].strip()\n",
        "        labels = parts[1].strip()\n",
        "\n",
        "        if filename.endswith(\"_3.png\"):\n",
        "            test_enc_file_paths.append(filename)\n",
        "            test_enc_genre_labels.append(labels)\n",
        "\n",
        "        elif filename.endswith(\"_4.png\"):\n",
        "            test_image_file_paths.append(filename)\n",
        "            test_image_genre_labels.append(labels)"
      ],
      "metadata": {
        "id": "LnGxlWzqbLIj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_image_file_paths), len(test_enc_file_paths)"
      ],
      "metadata": {
        "id": "yd9_b3EfbLdS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c973bd23-a57a-48f5-f936-186e3731c4b9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 2000)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_image_file_paths[0:5] , test_image_genre_labels[0:5]"
      ],
      "metadata": {
        "id": "Kj5sNm7ngOhz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3422bb7-f6dc-493d-c274-2b75a67c7a62"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['/content/Dataset(s)/jointHF-train+test_unseen/test/1/95876.jpg_4.png',\n",
              "  '/content/Dataset(s)/jointHF-train+test_unseen/test/1/65803.jpg_4.png',\n",
              "  '/content/Dataset(s)/jointHF-train+test_unseen/test/1/02564.jpg_4.png',\n",
              "  '/content/Dataset(s)/jointHF-train+test_unseen/test/1/81329.jpg_4.png',\n",
              "  '/content/Dataset(s)/jointHF-train+test_unseen/test/1/59814.jpg_4.png'],\n",
              " ['1', '1', '1', '1', '1'])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "label_counts = {label: 0 for label in unique_labels_list}\n",
        "\n",
        "test_num_samples_to_include_per_label = {}\n",
        "for label in unique_labels_list:\n",
        "    label_count = test_enc_genre_labels.count(label)\n",
        "    num_samples_to_include = int(label_count * percentage)\n",
        "    test_num_samples_to_include_per_label[label] = num_samples_to_include\n",
        "\n",
        "\n",
        "test_filtered_enc_file_paths = []\n",
        "test_filtered_enc_genre_labels = []\n",
        "\n",
        "\n",
        "test_selected_samples_per_label = {label: 0 for label in unique_labels_list}\n",
        "for path, label in zip(test_enc_file_paths, test_enc_genre_labels):\n",
        "    if test_selected_samples_per_label[label] < test_num_samples_to_include_per_label[label]:\n",
        "        test_filtered_enc_file_paths.append(path)\n",
        "        test_filtered_enc_genre_labels.append(label)\n",
        "        test_selected_samples_per_label[label] += 1"
      ],
      "metadata": {
        "id": "rHhvWppqbLxi"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_filtered_enc_file_paths)"
      ],
      "metadata": {
        "id": "ZplC_8mxgfng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a77fefd-fc9d-4a73-aa79-e3e455631d72"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "600"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new lists to combine data from both lists\n",
        "test_file_paths = test_image_file_paths + test_filtered_enc_file_paths                # check again\n",
        "test_genre_labels = test_image_genre_labels + test_filtered_enc_genre_labels"
      ],
      "metadata": {
        "id": "C6sjf4Ptbcdf"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_file_paths)"
      ],
      "metadata": {
        "id": "T0OH11Z2OJ_k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d1cc72f-2ae8-40ac-b642-607f85ed06e3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2600"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ResNet"
      ],
      "metadata": {
        "id": "NzBfAyc7apgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, file_paths, labels, transform=None):\n",
        "        self.file_paths = file_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.file_paths[idx]\n",
        "        image = Image.open(img_path)\n",
        "        label = int(self.labels[idx])\n",
        "        # label = torch.tensor(label,dtype=torch.float32)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "3Y7VXqUBYf_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CustomDataset(train_file_paths, train_genre_labels, transform=transform)\n",
        "test_dataset = CustomDataset(test_file_paths, test_genre_labels, transform=transform)"
      ],
      "metadata": {
        "id": "CjVU94XRZOXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 32  # Adjust as needed\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "niKEq0PUZVan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "resnet = models.resnet101(pretrained=True)\n",
        "num_features = resnet.fc.in_features\n",
        "resnet.fc = nn.Linear(num_features, num_classes)  # num_classes is the number of classes in your dataset\n",
        "resnet.to(device)\n",
        "print(resnet)\n"
      ],
      "metadata": {
        "id": "WkU78f5UZXqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.SGD(resnet.parameters(), lr=0.001, momentum=0.9)\n",
        "optimizer = optim.Adam(resnet.parameters(), lr=0.001)\n",
        "num_epochs = 20"
      ],
      "metadata": {
        "id": "kQnCFcJCZx5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, roc_auc_score\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "6Q79jKkChops"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 31"
      ],
      "metadata": {
        "id": "gFPdYrGOHpPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_loss = []\n",
        "\n",
        "for epoch in range(20,num_epochs):\n",
        "    resnet.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = resnet(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    training_loss.append(epoch_loss)\n",
        "\n",
        "    print(\"Training Loss==========================>>\")\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} Training Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    resnet.eval()\n",
        "    predicted_classes = []\n",
        "    actual_labels = []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = resnet(images)\n",
        "\n",
        "            _, predicted_label = torch.max(outputs, 1)\n",
        "\n",
        "            predicted_classes.extend(predicted_label.cpu().tolist())\n",
        "            actual_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "    accuracy = accuracy_score(actual_labels, predicted_classes)\n",
        "    # precision = precision_score(actual_labels, predicted_classes, average='weighted')\n",
        "    # recall = recall_score(actual_labels, predicted_classes, average='weighted')\n",
        "    # f1 = f1_score(actual_labels, predicted_classes, average='weighted')\n",
        "\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    # print(\"Precision:\", precision)\n",
        "    # print(\"Recall:\", recall)\n",
        "    # print(\"F1-score:\", f1)\n",
        "\n",
        "    print(classification_report(actual_labels, predicted_classes))\n",
        "    # print(\"Confusion Matrix: \")\n",
        "    # cm = confusion_matrix(actual_labels, predicted_classes)\n",
        "    # print(cm)\n",
        "    predicted_classes = np.array(predicted_classes)\n",
        "    actual_labels = np.array(actual_labels)\n",
        "    print(roc_auc_score(actual_labels, predicted_classes))\n",
        "\n",
        "\n",
        "    save_dir = \"/content/Model/Models-Train-27/\"\n",
        "    os.makedirs(save_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
        "\n",
        "    model_name = str(epoch+1) + \"_model.pth\"\n",
        "    save_path = os.path.join(save_dir, model_name)  # Specify the complete path to the model file\n",
        "    torch.save(resnet.state_dict(), save_path)"
      ],
      "metadata": {
        "id": "X70EPWQbZ_id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ViT"
      ],
      "metadata": {
        "id": "VmLtbV7uZiUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "!pip install transformers evaluate datasets\n",
        "import requests\n",
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import *\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "uO3ZsSpNeu9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model_name = \"google/vit-base-patch16-224\"\n",
        "image_processor = ViTImageProcessor.from_pretrained(model_name)\n",
        "model = ViTForImageClassification.from_pretrained(model_name).to(device)"
      ],
      "metadata": {
        "id": "5kwp9qmMZjkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Label Encoding"
      ],
      "metadata": {
        "id": "UGs6LJI9guRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "transformed_train_genre_labels = label_encoder.fit_transform(train_genre_labels)"
      ],
      "metadata": {
        "id": "x5qr0pvBcKi0"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder1 = LabelEncoder()\n",
        "transformed_test_genre_labels = label_encoder1.fit_transform(test_genre_labels)"
      ],
      "metadata": {
        "id": "VJ-m2Q5rgsrj"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_genre_labels), len(transformed_train_genre_labels)"
      ],
      "metadata": {
        "id": "Q8iTY22HfXsS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56f91abc-0f2f-4847-f73c-ef0f0bf3825d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12350, 12350)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_genre_labels), len(transformed_test_genre_labels)"
      ],
      "metadata": {
        "id": "ClRZgMkgg4Sw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "827c5c3c-3565-454b-e2ea-ad32437401f1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2600, 2600)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Creating"
      ],
      "metadata": {
        "id": "e297rXtbgwdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "train_data = {'image': train_file_paths, 'label': transformed_train_genre_labels}\n",
        "ds_train = Dataset.from_dict(train_data)"
      ],
      "metadata": {
        "id": "6XKv_SEBa6Sz"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_data = {'image': test_file_paths, 'label': transformed_test_genre_labels}\n",
        "ds_val = Dataset.from_dict(val_data)"
      ],
      "metadata": {
        "id": "djhbA0nCaDgq"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = unique_labels_list\n",
        "labels"
      ],
      "metadata": {
        "id": "5TGQqMVrbMW3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c1f5626-e4cc-4c98-c4af-34d48d9289d6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['0', '1']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PIL.Image as pil\n",
        "\n",
        "def transform(examples):\n",
        "  inputs = image_processor([pil.open(img).convert(\"RGB\") for img in examples[\"image\"]], return_tensors=\"pt\")\n",
        "  inputs[\"labels\"] = examples[\"label\"]\n",
        "  return inputs"
      ],
      "metadata": {
        "id": "9dEy0tTcbTfJ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = ds_train.with_transform(transform)\n",
        "val_dataset = ds_val.with_transform(transform)"
      ],
      "metadata": {
        "id": "owz6NIGTbVA7"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "id": "pet0z7BJdDZO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4cf6364-612a-4edd-f833-554bb497a4e3"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['image', 'label'],\n",
              "    num_rows: 12350\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset"
      ],
      "metadata": {
        "id": "00nnqJFvdHIj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "871c55f2-ff61-478c-a448-40aa3cc3eddf"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['image', 'label'],\n",
              "    num_rows: 2600\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for item in val_dataset:\n",
        "  print(item['pixel_values'].shape)\n",
        "  print(item[\"labels\"])\n",
        "  break"
      ],
      "metadata": {
        "id": "sPsIroeXbjlY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02769e68-3778-4fed-93c6-21b401f0ee9c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 224, 224])\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def collate_fn(batch):\n",
        "  return {\n",
        "      \"pixel_values\": torch.stack([x[\"pixel_values\"] for x in batch]),\n",
        "      \"labels\": torch.tensor([x[\"labels\"] for x in batch]),\n",
        "  }"
      ],
      "metadata": {
        "id": "22a48uX4bnZ0"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the ViT model\n",
        "model = ViTForImageClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=len(unique_labels_list),\n",
        "    id2label={str(i): c for i, c in enumerate(unique_labels_list)},\n",
        "    label2id={c: str(i) for i, c in enumerate(unique_labels_list)},\n",
        "    ignore_mismatched_sizes=True,\n",
        ")"
      ],
      "metadata": {
        "id": "sIQenfhHbz_N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21bfb8c4-ffa2-49e4-8b6a-a89888a5d1ff"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--vit-base-patch16-224/snapshots/3f49326eb077187dfe1c2a2bb15fbd74e6ab91e3/config.json\n",
            "Model config ViTConfig {\n",
            "  \"_name_or_path\": \"google/vit-base-patch16-224\",\n",
            "  \"architectures\": [\n",
            "    \"ViTForImageClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.0,\n",
            "  \"encoder_stride\": 16,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.0,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"0\",\n",
            "    \"1\": \"1\"\n",
            "  },\n",
            "  \"image_size\": 224,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"0\": \"0\",\n",
            "    \"1\": \"1\"\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"model_type\": \"vit\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_channels\": 3,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"patch_size\": 16,\n",
            "  \"qkv_bias\": true,\n",
            "  \"transformers_version\": \"4.32.1\"\n",
            "}\n",
            "\n",
            "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--google--vit-base-patch16-224/snapshots/3f49326eb077187dfe1c2a2bb15fbd74e6ab91e3/model.safetensors\n",
            "All model checkpoint weights were used when initializing ViTForImageClassification.\n",
            "\n",
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
            "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
            "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation Metric"
      ],
      "metadata": {
        "id": "KLVUJAqgcCRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from evaluate import load\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# load the accuracy and f1 metrics from the evaluate module\n",
        "accuracy = load(\"accuracy\")\n",
        "f1 = load(\"f1\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "  # compute the accuracy and f1 scores & return them\n",
        "  accuracy_score = accuracy.compute(predictions=np.argmax(eval_pred.predictions, axis=1), references=eval_pred.label_ids)\n",
        "  f1_score = f1.compute(predictions=np.argmax(eval_pred.predictions, axis=1), references=eval_pred.label_ids, average=\"macro\")\n",
        "\n",
        "  auroc_score = roc_auc_score(eval_pred.label_ids, np.argmax(eval_pred.predictions, axis=1))\n",
        "  print(f\"AUROC Score: {auroc_score:.4f}\")\n",
        "\n",
        "  return {**accuracy_score, **f1_score}"
      ],
      "metadata": {
        "id": "23DkFdTNcCtE"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "FuDfvAopcHUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install accelerate -U\n",
        "# !pip install transformers[torch]"
      ],
      "metadata": {
        "id": "665OJ9i4cudJ"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "  output_dir=\"/content/Model/Models-Train-27\", # output directory\n",
        "  per_device_train_batch_size=32, # batch size per device during training\n",
        "  evaluation_strategy=\"steps\",    # evaluation strategy to adopt during training\n",
        "  num_train_epochs=20,             # total number of training epochs\n",
        "  # fp16=True,                    # use mixed precision\n",
        "  save_steps=1500,                # number of update steps before saving checkpoint\n",
        "  eval_steps=1500,                # number of update steps before evaluating\n",
        "  logging_steps=10,             # number of update steps before logging\n",
        "  # save_steps=50,\n",
        "  # eval_steps=50,\n",
        "  # logging_steps=50,\n",
        "  save_total_limit=6,             # limit the total amount of checkpoints on disk\n",
        "  remove_unused_columns=False,    # remove unused columns from the dataset\n",
        "  push_to_hub=False,              # do not push the model to the hub\n",
        "  report_to='tensorboard',        # report metrics to tensorboard\n",
        "  load_best_model_at_end=True,    # load the best model at the end of training\n",
        ")"
      ],
      "metadata": {
        "id": "xApZuICNcqvR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2cdecbb-8328-4ed1-9dbb-90bf498d877b"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n",
            "PyTorch: setting up devices\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                        # the instantiated ðŸ¤— Transformers model to be trained\n",
        "    args=training_args,                 # training arguments, defined above\n",
        "    data_collator=collate_fn,           # the data collator that will be used for batching\n",
        "    compute_metrics=compute_metrics,    # the metrics function that will be used for evaluation\n",
        "    train_dataset=train_dataset,        # training dataset\n",
        "    eval_dataset=val_dataset,           # evaluation dataset\n",
        "    tokenizer=image_processor,          # the processor that will be used for preprocessing the images\n",
        ")"
      ],
      "metadata": {
        "id": "CDA6IEUzc6M8"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "0CI-Z4G_c9WT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8bdb902e-885f-431c-f67e-68f2fe356aa5"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 12,350\n",
            "  Num Epochs = 20\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 7,720\n",
            "  Number of trainable parameters = 85,800,194\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7720' max='7720' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7720/7720 3:09:00, Epoch 20/20]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.431300</td>\n",
              "      <td>0.646766</td>\n",
              "      <td>0.680769</td>\n",
              "      <td>0.637530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.178100</td>\n",
              "      <td>1.174437</td>\n",
              "      <td>0.708462</td>\n",
              "      <td>0.679302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.085800</td>\n",
              "      <td>1.534736</td>\n",
              "      <td>0.680769</td>\n",
              "      <td>0.665123</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.061000</td>\n",
              "      <td>1.836337</td>\n",
              "      <td>0.673846</td>\n",
              "      <td>0.659256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>0.020700</td>\n",
              "      <td>2.034945</td>\n",
              "      <td>0.682692</td>\n",
              "      <td>0.667544</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 2600\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUROC Score: 0.6341\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to /content/Model/Models-Train-27/checkpoint-1500\n",
            "Configuration saved in /content/Model/Models-Train-27/checkpoint-1500/config.json\n",
            "Model weights saved in /content/Model/Models-Train-27/checkpoint-1500/pytorch_model.bin\n",
            "Image processor saved in /content/Model/Models-Train-27/checkpoint-1500/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2600\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUROC Score: 0.6753\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to /content/Model/Models-Train-27/checkpoint-3000\n",
            "Configuration saved in /content/Model/Models-Train-27/checkpoint-3000/config.json\n",
            "Model weights saved in /content/Model/Models-Train-27/checkpoint-3000/pytorch_model.bin\n",
            "Image processor saved in /content/Model/Models-Train-27/checkpoint-3000/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2600\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUROC Score: 0.6685\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to /content/Model/Models-Train-27/checkpoint-4500\n",
            "Configuration saved in /content/Model/Models-Train-27/checkpoint-4500/config.json\n",
            "Model weights saved in /content/Model/Models-Train-27/checkpoint-4500/pytorch_model.bin\n",
            "Image processor saved in /content/Model/Models-Train-27/checkpoint-4500/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2600\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUROC Score: 0.6636\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to /content/Model/Models-Train-27/checkpoint-6000\n",
            "Configuration saved in /content/Model/Models-Train-27/checkpoint-6000/config.json\n",
            "Model weights saved in /content/Model/Models-Train-27/checkpoint-6000/pytorch_model.bin\n",
            "Image processor saved in /content/Model/Models-Train-27/checkpoint-6000/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2600\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUROC Score: 0.6713\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to /content/Model/Models-Train-27/checkpoint-7500\n",
            "Configuration saved in /content/Model/Models-Train-27/checkpoint-7500/config.json\n",
            "Model weights saved in /content/Model/Models-Train-27/checkpoint-7500/pytorch_model.bin\n",
            "Image processor saved in /content/Model/Models-Train-27/checkpoint-7500/preprocessor_config.json\n",
            "Deleting older checkpoint [/content/Model/Models-Train-27/checkpoint-10] due to args.save_total_limit\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from /content/Model/Models-Train-27/checkpoint-1500 (score: 0.6467663645744324).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=7720, training_loss=0.19072365330947186, metrics={'train_runtime': 11343.8185, 'train_samples_per_second': 21.774, 'train_steps_per_second': 0.681, 'total_flos': 1.9140521434804224e+19, 'train_loss': 0.19072365330947186, 'epoch': 20.0})"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    }
  ]
}