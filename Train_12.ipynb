{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MuhammadIrzam447/MultiModel/blob/master/Train_12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quQ4Wf-5rN4j"
      },
      "outputs": [],
      "source": [
        "# google/vit-base-patch16-224"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9F-pNaIFdRHi"
      },
      "outputs": [],
      "source": [
        "# !gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nvujlUCYdi0L"
      },
      "outputs": [],
      "source": [
        "# !unzip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yV2GxQWZw4L"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKSy44hXZx8a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.transforms import transforms\n",
        "from transformers import ViTForImageClassification, ViTFeatureExtractor, AdamW\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix, roc_auc_score\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCC0cmnIZzTH"
      },
      "outputs": [],
      "source": [
        "# Define the path to your training and validation data\n",
        "train_data_root = \"/content/Dataset(s)/fused-food-101-train\"\n",
        "val_data_root = \"/content/Dataset(s)/fused-food-101-test\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcVrx7RkZzdV"
      },
      "outputs": [],
      "source": [
        "from transformers import ViTImageProcessor\n",
        "\n",
        "processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
        "image_mean, image_std = processor.image_mean, processor.image_std\n",
        "size = processor.size[\"height\"]\n",
        "\n",
        "# Define transformations for the input images\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=image_mean, std=image_std)\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ysx46DEHZznC"
      },
      "outputs": [],
      "source": [
        "# Load the dataset using ImageFolder and apply transformations\n",
        "train_dataset = ImageFolder(train_data_root, transform=transform)\n",
        "val_dataset = ImageFolder(val_data_root, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0bNIiTMZzwL"
      },
      "outputs": [],
      "source": [
        "# Create label2id and id2label dictionaries based on the class names in the dataset\n",
        "label2id = {class_name: idx for class_name, idx in train_dataset.class_to_idx.items()}\n",
        "id2label = {idx: class_name for class_name, idx in train_dataset.class_to_idx.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9zA4YBKZ0SA"
      },
      "outputs": [],
      "source": [
        "# Initialize the feature extractor\n",
        "# feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224\")\n",
        "\n",
        "# Define batch size and number of workers (adjust based on your system's resources)\n",
        "batch_size = 32\n",
        "num_workers = 1\n",
        "\n",
        "# Create DataLoader for the dataset\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3y3_g9PHZ0by",
        "outputId": "bdfa57ab-f365-4377-a027-a293617d5cc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "101\n"
          ]
        }
      ],
      "source": [
        "num_classes = len(train_dataset.classes)\n",
        "print(num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_dir = '/content/Model/Models-Train-12/'\n",
        "load_path = os.path.join(save_dir, '8_model.pth')\n",
        "\n",
        "vit = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224\", id2label=id2label, label2id=label2id, ignore_mismatched_sizes=True, num_labels=num_classes)\n",
        "vit.load_state_dict(torch.load(load_path))\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "vit.to(device)\n",
        "print(vit)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrkGkNzNKXXK",
        "outputId": "be09c345-5d6b-4ca4-b573-7f64987d52e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
            "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([101, 768]) in the model instantiated\n",
            "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([101]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ViTForImageClassification(\n",
            "  (vit): ViTModel(\n",
            "    (embeddings): ViTEmbeddings(\n",
            "      (patch_embeddings): ViTPatchEmbeddings(\n",
            "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
            "      )\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (encoder): ViTEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0-11): 12 x ViTLayer(\n",
            "          (attention): ViTAttention(\n",
            "            (attention): ViTSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (output): ViTSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): ViTIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): ViTOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "  )\n",
            "  (classifier): Linear(in_features=768, out_features=101, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5Yqjhf2Z0kl"
      },
      "outputs": [],
      "source": [
        "# vit = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224\", id2label=id2label, label2id=label2id, ignore_mismatched_sizes=True)\n",
        "# vit.classifier = nn.Linear(vit.config.hidden_size, num_classes)\n",
        "\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# vit.to(device)\n",
        "# print(vit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdqAwRBSaEGC",
        "outputId": "b9b4c8b6-da70-4071-a91e-192310c1545b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = AdamW(vit.parameters(), lr=1e-5)\n",
        "# optimizer = optim.SGD(vit.parameters(), lr=0.001, momentum=0.9)\n",
        "num_epochs = 12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVhg3NjSaGwr",
        "outputId": "1afb6ff3-d83f-448a-8358-b27a52fb405a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/12 - Training Loss: 0.0150 - Validation Loss: 0.9687 - Accuracy: 0.8216\n",
            "Accuracy: 0.8215795034337031\n",
            "Precision: 0.8251463022019719\n",
            "Recall: 0.8215795034337031\n",
            "F1-score: 0.8220048196654802\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.84      0.82       234\n",
            "           1       0.92      0.93      0.92       221\n",
            "           2       0.92      0.92      0.92       226\n",
            "           3       0.80      0.77      0.78       222\n",
            "           4       0.77      0.55      0.64       225\n",
            "           5       0.89      0.79      0.84       224\n",
            "           6       0.80      0.77      0.79       224\n",
            "           7       0.90      0.76      0.83       225\n",
            "           8       0.80      0.76      0.78       226\n",
            "           9       0.77      0.82      0.79       214\n",
            "          10       0.82      0.84      0.83       231\n",
            "          11       0.85      0.89      0.87       227\n",
            "          12       0.85      0.86      0.85       230\n",
            "          13       0.88      0.91      0.90       220\n",
            "          14       0.75      0.82      0.78       231\n",
            "          15       0.85      0.88      0.87       227\n",
            "          16       0.67      0.64      0.65       224\n",
            "          17       0.88      0.79      0.83       233\n",
            "          18       0.86      0.84      0.85       222\n",
            "          19       0.76      0.80      0.78       220\n",
            "          20       0.85      0.91      0.88       219\n",
            "          21       0.82      0.87      0.85       232\n",
            "          22       0.90      0.91      0.91       224\n",
            "          23       0.91      0.93      0.92       230\n",
            "          24       0.90      0.93      0.92       224\n",
            "          25       0.90      0.87      0.88       220\n",
            "          26       0.83      0.78      0.81       221\n",
            "          27       0.92      0.88      0.90       225\n",
            "          28       0.86      0.90      0.88       224\n",
            "          29       0.81      0.69      0.74       228\n",
            "          30       0.97      0.98      0.97       229\n",
            "          31       0.82      0.76      0.79       232\n",
            "          32       0.66      0.70      0.68       228\n",
            "          33       0.80      0.76      0.78       231\n",
            "          34       0.89      0.90      0.89       213\n",
            "          35       0.73      0.85      0.79       202\n",
            "          36       0.82      0.87      0.84       228\n",
            "          37       0.83      0.82      0.82       212\n",
            "          38       0.84      0.66      0.74       208\n",
            "          39       0.81      0.80      0.80       216\n",
            "          40       0.84      0.81      0.82       212\n",
            "          41       0.83      0.90      0.86       223\n",
            "          42       0.82      0.75      0.79       238\n",
            "          43       0.84      0.83      0.84       212\n",
            "          44       0.79      0.82      0.80       229\n",
            "          45       0.86      0.92      0.89       233\n",
            "          46       0.82      0.79      0.80       224\n",
            "          47       0.75      0.79      0.77       234\n",
            "          48       0.94      0.81      0.87       223\n",
            "          49       0.81      0.88      0.84       229\n",
            "          50       0.82      0.83      0.82       225\n",
            "          51       0.82      0.88      0.85       228\n",
            "          52       0.80      0.76      0.78       232\n",
            "          53       0.74      0.70      0.72       233\n",
            "          54       0.80      0.86      0.83       203\n",
            "          55       0.74      0.71      0.72       234\n",
            "          56       0.88      0.94      0.91       223\n",
            "          57       0.88      0.84      0.86       234\n",
            "          58       0.86      0.85      0.86       232\n",
            "          59       0.84      0.88      0.86       233\n",
            "          60       0.82      0.75      0.79       197\n",
            "          61       0.53      0.66      0.59       215\n",
            "          62       0.83      0.76      0.80       238\n",
            "          63       0.91      0.93      0.92       231\n",
            "          64       0.86      0.80      0.83       227\n",
            "          65       0.96      0.89      0.93       234\n",
            "          66       0.81      0.86      0.83       224\n",
            "          67       0.87      0.80      0.84       231\n",
            "          68       0.88      0.82      0.85       217\n",
            "          69       0.68      0.70      0.69       223\n",
            "          70       0.84      0.80      0.82       230\n",
            "          71       0.88      0.88      0.88       228\n",
            "          72       0.84      0.81      0.83       234\n",
            "          73       0.91      0.95      0.93       225\n",
            "          74       0.72      0.78      0.75       211\n",
            "          75       0.89      0.83      0.86       218\n",
            "          76       0.80      0.83      0.81       235\n",
            "          77       0.74      0.81      0.77       227\n",
            "          78       0.84      0.76      0.79       210\n",
            "          79       0.96      0.92      0.94       220\n",
            "          80       0.86      0.93      0.89       224\n",
            "          81       0.68      0.75      0.71       232\n",
            "          82       0.72      0.68      0.70       236\n",
            "          83       0.87      0.88      0.88       227\n",
            "          84       0.83      0.80      0.81       236\n",
            "          85       0.93      0.90      0.91       215\n",
            "          86       0.47      0.69      0.56       226\n",
            "          87       0.75      0.77      0.76       228\n",
            "          88       0.89      0.83      0.86       219\n",
            "          89       0.79      0.83      0.81       228\n",
            "          90       0.88      0.91      0.90       213\n",
            "          91       0.92      0.92      0.92       218\n",
            "          92       0.88      0.82      0.85       230\n",
            "          93       0.74      0.70      0.72       229\n",
            "          94       0.89      0.88      0.89       223\n",
            "          95       0.81      0.78      0.79       237\n",
            "          96       0.75      0.83      0.79       233\n",
            "          97       0.76      0.84      0.80       223\n",
            "          98       0.87      0.88      0.87       232\n",
            "          99       0.73      0.73      0.73       211\n",
            "         100       0.89      0.83      0.86       235\n",
            "\n",
            "    accuracy                           0.82     22716\n",
            "   macro avg       0.83      0.82      0.82     22716\n",
            "weighted avg       0.83      0.82      0.82     22716\n",
            "\n",
            "Confusion Matrix:\n",
            "[[196   0   0 ...   1   0   2]\n",
            " [  0 205   0 ...   0   0   0]\n",
            " [  2   0 207 ...   1   0   0]\n",
            " ...\n",
            " [  0   0   2 ... 204   0   0]\n",
            " [  0   0   0 ...   0 153   0]\n",
            " [  1   0   0 ...   2   0 195]]\n",
            "Epoch 2/12 - Training Loss: 0.0134 - Validation Loss: 0.9841 - Accuracy: 0.8232\n",
            "Accuracy: 0.8231642894875858\n",
            "Precision: 0.8247001398013954\n",
            "Recall: 0.8231642894875858\n",
            "F1-score: 0.8225062582861411\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.79      0.82       234\n",
            "           1       0.90      0.92      0.91       221\n",
            "           2       0.91      0.92      0.92       226\n",
            "           3       0.65      0.85      0.73       222\n",
            "           4       0.68      0.56      0.62       225\n",
            "           5       0.84      0.87      0.85       224\n",
            "           6       0.74      0.79      0.77       224\n",
            "           7       0.79      0.78      0.79       225\n",
            "           8       0.79      0.80      0.80       226\n",
            "           9       0.79      0.80      0.80       214\n",
            "          10       0.79      0.81      0.80       231\n",
            "          11       0.87      0.85      0.86       227\n",
            "          12       0.87      0.85      0.86       230\n",
            "          13       0.84      0.89      0.86       220\n",
            "          14       0.79      0.82      0.81       231\n",
            "          15       0.86      0.87      0.87       227\n",
            "          16       0.77      0.61      0.68       224\n",
            "          17       0.83      0.87      0.85       233\n",
            "          18       0.84      0.84      0.84       222\n",
            "          19       0.84      0.77      0.80       220\n",
            "          20       0.87      0.91      0.89       219\n",
            "          21       0.81      0.85      0.83       232\n",
            "          22       0.89      0.92      0.90       224\n",
            "          23       0.89      0.92      0.90       230\n",
            "          24       0.89      0.95      0.92       224\n",
            "          25       0.84      0.90      0.87       220\n",
            "          26       0.76      0.85      0.80       221\n",
            "          27       0.92      0.88      0.90       225\n",
            "          28       0.80      0.92      0.86       224\n",
            "          29       0.79      0.72      0.75       228\n",
            "          30       0.96      0.98      0.97       229\n",
            "          31       0.80      0.78      0.79       232\n",
            "          32       0.72      0.68      0.70       228\n",
            "          33       0.89      0.65      0.75       231\n",
            "          34       0.92      0.89      0.90       213\n",
            "          35       0.75      0.84      0.79       202\n",
            "          36       0.83      0.88      0.86       228\n",
            "          37       0.80      0.83      0.82       212\n",
            "          38       0.82      0.69      0.75       208\n",
            "          39       0.82      0.80      0.81       216\n",
            "          40       0.87      0.77      0.82       212\n",
            "          41       0.82      0.91      0.86       223\n",
            "          42       0.86      0.74      0.80       238\n",
            "          43       0.84      0.81      0.82       212\n",
            "          44       0.81      0.79      0.80       229\n",
            "          45       0.88      0.90      0.89       233\n",
            "          46       0.86      0.76      0.81       224\n",
            "          47       0.79      0.83      0.81       234\n",
            "          48       0.82      0.87      0.85       223\n",
            "          49       0.88      0.82      0.85       229\n",
            "          50       0.79      0.81      0.80       225\n",
            "          51       0.91      0.89      0.90       228\n",
            "          52       0.79      0.81      0.80       232\n",
            "          53       0.75      0.72      0.74       233\n",
            "          54       0.88      0.83      0.86       203\n",
            "          55       0.72      0.71      0.71       234\n",
            "          56       0.88      0.94      0.91       223\n",
            "          57       0.82      0.89      0.85       234\n",
            "          58       0.89      0.82      0.86       232\n",
            "          59       0.85      0.88      0.86       233\n",
            "          60       0.85      0.80      0.82       197\n",
            "          61       0.64      0.61      0.63       215\n",
            "          62       0.81      0.79      0.80       238\n",
            "          63       0.93      0.92      0.92       231\n",
            "          64       0.74      0.85      0.79       227\n",
            "          65       0.95      0.88      0.92       234\n",
            "          66       0.85      0.85      0.85       224\n",
            "          67       0.87      0.78      0.83       231\n",
            "          68       0.87      0.84      0.86       217\n",
            "          69       0.77      0.70      0.73       223\n",
            "          70       0.76      0.84      0.80       230\n",
            "          71       0.87      0.89      0.88       228\n",
            "          72       0.78      0.82      0.80       234\n",
            "          73       0.92      0.94      0.93       225\n",
            "          74       0.69      0.82      0.75       211\n",
            "          75       0.87      0.83      0.85       218\n",
            "          76       0.73      0.86      0.79       235\n",
            "          77       0.74      0.81      0.77       227\n",
            "          78       0.87      0.75      0.80       210\n",
            "          79       0.91      0.92      0.92       220\n",
            "          80       0.89      0.93      0.91       224\n",
            "          81       0.75      0.72      0.73       232\n",
            "          82       0.75      0.69      0.72       236\n",
            "          83       0.84      0.90      0.87       227\n",
            "          84       0.87      0.78      0.82       236\n",
            "          85       0.91      0.90      0.90       215\n",
            "          86       0.62      0.61      0.62       226\n",
            "          87       0.80      0.75      0.78       228\n",
            "          88       0.83      0.87      0.85       219\n",
            "          89       0.83      0.86      0.84       228\n",
            "          90       0.85      0.91      0.88       213\n",
            "          91       0.91      0.92      0.92       218\n",
            "          92       0.87      0.82      0.84       230\n",
            "          93       0.75      0.69      0.72       229\n",
            "          94       0.89      0.87      0.88       223\n",
            "          95       0.80      0.76      0.78       237\n",
            "          96       0.75      0.80      0.78       233\n",
            "          97       0.72      0.83      0.77       223\n",
            "          98       0.92      0.84      0.88       232\n",
            "          99       0.74      0.72      0.73       211\n",
            "         100       0.85      0.87      0.86       235\n",
            "\n",
            "    accuracy                           0.82     22716\n",
            "   macro avg       0.82      0.82      0.82     22716\n",
            "weighted avg       0.82      0.82      0.82     22716\n",
            "\n",
            "Confusion Matrix:\n",
            "[[186   1   1 ...   0   0   6]\n",
            " [  0 203   0 ...   0   0   0]\n",
            " [  2   0 208 ...   1   0   1]\n",
            " ...\n",
            " [  0   0   2 ... 195   0   1]\n",
            " [  0   0   0 ...   0 151   0]\n",
            " [  0   0   0 ...   0   0 205]]\n",
            "Epoch 3/12 - Training Loss: 0.0123 - Validation Loss: 0.9792 - Accuracy: 0.8254\n",
            "Accuracy: 0.8254094030639197\n",
            "Precision: 0.8281737508686614\n",
            "Recall: 0.8254094030639197\n",
            "F1-score: 0.8252897043684212\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.86      0.79       234\n",
            "           1       0.96      0.90      0.93       221\n",
            "           2       0.87      0.92      0.90       226\n",
            "           3       0.83      0.79      0.81       222\n",
            "           4       0.71      0.57      0.63       225\n",
            "           5       0.85      0.85      0.85       224\n",
            "           6       0.66      0.84      0.74       224\n",
            "           7       0.90      0.76      0.83       225\n",
            "           8       0.80      0.79      0.79       226\n",
            "           9       0.81      0.80      0.81       214\n",
            "          10       0.76      0.84      0.80       231\n",
            "          11       0.86      0.87      0.87       227\n",
            "          12       0.85      0.84      0.84       230\n",
            "          13       0.88      0.90      0.89       220\n",
            "          14       0.82      0.78      0.80       231\n",
            "          15       0.88      0.87      0.87       227\n",
            "          16       0.72      0.66      0.69       224\n",
            "          17       0.81      0.87      0.84       233\n",
            "          18       0.88      0.81      0.84       222\n",
            "          19       0.83      0.78      0.80       220\n",
            "          20       0.93      0.88      0.90       219\n",
            "          21       0.83      0.84      0.83       232\n",
            "          22       0.86      0.92      0.89       224\n",
            "          23       0.90      0.94      0.92       230\n",
            "          24       0.90      0.93      0.91       224\n",
            "          25       0.85      0.88      0.87       220\n",
            "          26       0.72      0.87      0.79       221\n",
            "          27       0.92      0.90      0.91       225\n",
            "          28       0.86      0.90      0.88       224\n",
            "          29       0.78      0.76      0.77       228\n",
            "          30       0.97      0.98      0.97       229\n",
            "          31       0.83      0.79      0.81       232\n",
            "          32       0.77      0.69      0.73       228\n",
            "          33       0.79      0.75      0.77       231\n",
            "          34       0.90      0.89      0.89       213\n",
            "          35       0.70      0.85      0.77       202\n",
            "          36       0.85      0.89      0.87       228\n",
            "          37       0.92      0.83      0.87       212\n",
            "          38       0.84      0.63      0.72       208\n",
            "          39       0.83      0.80      0.82       216\n",
            "          40       0.84      0.79      0.82       212\n",
            "          41       0.80      0.90      0.85       223\n",
            "          42       0.87      0.73      0.79       238\n",
            "          43       0.88      0.81      0.85       212\n",
            "          44       0.82      0.79      0.81       229\n",
            "          45       0.86      0.91      0.89       233\n",
            "          46       0.79      0.82      0.80       224\n",
            "          47       0.79      0.81      0.80       234\n",
            "          48       0.85      0.89      0.87       223\n",
            "          49       0.82      0.86      0.84       229\n",
            "          50       0.72      0.86      0.79       225\n",
            "          51       0.93      0.87      0.90       228\n",
            "          52       0.79      0.81      0.80       232\n",
            "          53       0.77      0.69      0.73       233\n",
            "          54       0.88      0.78      0.83       203\n",
            "          55       0.73      0.72      0.73       234\n",
            "          56       0.95      0.92      0.93       223\n",
            "          57       0.87      0.88      0.87       234\n",
            "          58       0.85      0.86      0.86       232\n",
            "          59       0.88      0.85      0.87       233\n",
            "          60       0.84      0.79      0.81       197\n",
            "          61       0.59      0.67      0.62       215\n",
            "          62       0.82      0.78      0.80       238\n",
            "          63       0.91      0.93      0.92       231\n",
            "          64       0.79      0.84      0.81       227\n",
            "          65       0.90      0.91      0.90       234\n",
            "          66       0.84      0.83      0.83       224\n",
            "          67       0.82      0.80      0.81       231\n",
            "          68       0.91      0.83      0.87       217\n",
            "          69       0.73      0.68      0.70       223\n",
            "          70       0.78      0.86      0.82       230\n",
            "          71       0.90      0.88      0.89       228\n",
            "          72       0.81      0.80      0.81       234\n",
            "          73       0.92      0.96      0.94       225\n",
            "          74       0.78      0.72      0.75       211\n",
            "          75       0.83      0.85      0.84       218\n",
            "          76       0.77      0.83      0.80       235\n",
            "          77       0.76      0.78      0.77       227\n",
            "          78       0.80      0.77      0.79       210\n",
            "          79       0.94      0.92      0.93       220\n",
            "          80       0.88      0.93      0.90       224\n",
            "          81       0.80      0.60      0.69       232\n",
            "          82       0.73      0.71      0.72       236\n",
            "          83       0.90      0.86      0.88       227\n",
            "          84       0.86      0.80      0.83       236\n",
            "          85       0.87      0.91      0.89       215\n",
            "          86       0.57      0.63      0.60       226\n",
            "          87       0.66      0.80      0.73       228\n",
            "          88       0.78      0.87      0.83       219\n",
            "          89       0.89      0.84      0.87       228\n",
            "          90       0.86      0.90      0.88       213\n",
            "          91       0.92      0.89      0.91       218\n",
            "          92       0.83      0.84      0.84       230\n",
            "          93       0.77      0.74      0.76       229\n",
            "          94       0.89      0.87      0.88       223\n",
            "          95       0.85      0.79      0.82       237\n",
            "          96       0.82      0.78      0.80       233\n",
            "          97       0.75      0.85      0.79       223\n",
            "          98       0.92      0.87      0.89       232\n",
            "          99       0.71      0.79      0.75       211\n",
            "         100       0.89      0.86      0.87       235\n",
            "\n",
            "    accuracy                           0.83     22716\n",
            "   macro avg       0.83      0.83      0.83     22716\n",
            "weighted avg       0.83      0.83      0.83     22716\n",
            "\n",
            "Confusion Matrix:\n",
            "[[201   0   3 ...   1   0   3]\n",
            " [  0 200   0 ...   0   0   0]\n",
            " [  3   0 209 ...   1   0   0]\n",
            " ...\n",
            " [  0   0   2 ... 201   0   1]\n",
            " [  0   0   0 ...   0 167   0]\n",
            " [  1   0   0 ...   0   0 201]]\n",
            "Epoch 4/12 - Training Loss: 0.0124 - Validation Loss: 0.9996 - Accuracy: 0.8268\n",
            "Accuracy: 0.826774079943652\n",
            "Precision: 0.8290986543045447\n",
            "Recall: 0.826774079943652\n",
            "F1-score: 0.8266125568945561\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.83      0.83       234\n",
            "           1       0.91      0.92      0.91       221\n",
            "           2       0.98      0.88      0.93       226\n",
            "           3       0.75      0.83      0.79       222\n",
            "           4       0.70      0.56      0.63       225\n",
            "           5       0.81      0.88      0.85       224\n",
            "           6       0.75      0.78      0.76       224\n",
            "           7       0.89      0.76      0.82       225\n",
            "           8       0.82      0.81      0.81       226\n",
            "           9       0.74      0.83      0.79       214\n",
            "          10       0.77      0.84      0.80       231\n",
            "          11       0.87      0.89      0.88       227\n",
            "          12       0.90      0.86      0.88       230\n",
            "          13       0.82      0.92      0.87       220\n",
            "          14       0.77      0.82      0.80       231\n",
            "          15       0.85      0.88      0.87       227\n",
            "          16       0.73      0.64      0.68       224\n",
            "          17       0.75      0.91      0.82       233\n",
            "          18       0.80      0.88      0.84       222\n",
            "          19       0.80      0.79      0.79       220\n",
            "          20       0.89      0.91      0.90       219\n",
            "          21       0.82      0.81      0.82       232\n",
            "          22       0.86      0.91      0.89       224\n",
            "          23       0.92      0.93      0.92       230\n",
            "          24       0.89      0.95      0.92       224\n",
            "          25       0.86      0.89      0.88       220\n",
            "          26       0.70      0.87      0.78       221\n",
            "          27       0.92      0.89      0.90       225\n",
            "          28       0.85      0.90      0.88       224\n",
            "          29       0.76      0.75      0.76       228\n",
            "          30       0.97      0.97      0.97       229\n",
            "          31       0.78      0.81      0.80       232\n",
            "          32       0.71      0.71      0.71       228\n",
            "          33       0.85      0.73      0.79       231\n",
            "          34       0.88      0.91      0.89       213\n",
            "          35       0.83      0.81      0.82       202\n",
            "          36       0.82      0.90      0.86       228\n",
            "          37       0.82      0.84      0.83       212\n",
            "          38       0.82      0.71      0.76       208\n",
            "          39       0.81      0.79      0.80       216\n",
            "          40       0.83      0.80      0.81       212\n",
            "          41       0.84      0.90      0.87       223\n",
            "          42       0.85      0.74      0.79       238\n",
            "          43       0.84      0.80      0.82       212\n",
            "          44       0.78      0.83      0.81       229\n",
            "          45       0.86      0.90      0.88       233\n",
            "          46       0.85      0.78      0.81       224\n",
            "          47       0.73      0.83      0.78       234\n",
            "          48       0.83      0.89      0.86       223\n",
            "          49       0.82      0.87      0.85       229\n",
            "          50       0.80      0.82      0.81       225\n",
            "          51       0.92      0.89      0.90       228\n",
            "          52       0.81      0.76      0.79       232\n",
            "          53       0.81      0.72      0.76       233\n",
            "          54       0.81      0.83      0.82       203\n",
            "          55       0.68      0.72      0.70       234\n",
            "          56       0.94      0.92      0.93       223\n",
            "          57       0.82      0.90      0.86       234\n",
            "          58       0.90      0.80      0.85       232\n",
            "          59       0.87      0.85      0.86       233\n",
            "          60       0.86      0.83      0.84       197\n",
            "          61       0.60      0.64      0.62       215\n",
            "          62       0.81      0.76      0.78       238\n",
            "          63       0.95      0.93      0.94       231\n",
            "          64       0.78      0.87      0.82       227\n",
            "          65       0.97      0.85      0.91       234\n",
            "          66       0.86      0.83      0.84       224\n",
            "          67       0.87      0.81      0.84       231\n",
            "          68       0.91      0.84      0.88       217\n",
            "          69       0.80      0.69      0.74       223\n",
            "          70       0.86      0.84      0.85       230\n",
            "          71       0.88      0.87      0.88       228\n",
            "          72       0.81      0.82      0.82       234\n",
            "          73       0.89      0.95      0.92       225\n",
            "          74       0.73      0.77      0.75       211\n",
            "          75       0.90      0.84      0.87       218\n",
            "          76       0.78      0.82      0.80       235\n",
            "          77       0.78      0.78      0.78       227\n",
            "          78       0.76      0.80      0.78       210\n",
            "          79       0.96      0.91      0.94       220\n",
            "          80       0.87      0.92      0.89       224\n",
            "          81       0.71      0.75      0.73       232\n",
            "          82       0.82      0.69      0.75       236\n",
            "          83       0.89      0.86      0.88       227\n",
            "          84       0.83      0.81      0.82       236\n",
            "          85       0.95      0.89      0.92       215\n",
            "          86       0.62      0.58      0.60       226\n",
            "          87       0.69      0.77      0.73       228\n",
            "          88       0.88      0.83      0.85       219\n",
            "          89       0.87      0.82      0.85       228\n",
            "          90       0.90      0.90      0.90       213\n",
            "          91       0.90      0.91      0.90       218\n",
            "          92       0.85      0.83      0.84       230\n",
            "          93       0.78      0.70      0.74       229\n",
            "          94       0.87      0.90      0.88       223\n",
            "          95       0.85      0.77      0.81       237\n",
            "          96       0.85      0.76      0.81       233\n",
            "          97       0.70      0.84      0.76       223\n",
            "          98       0.91      0.86      0.89       232\n",
            "          99       0.70      0.76      0.73       211\n",
            "         100       0.90      0.84      0.87       235\n",
            "\n",
            "    accuracy                           0.83     22716\n",
            "   macro avg       0.83      0.83      0.83     22716\n",
            "weighted avg       0.83      0.83      0.83     22716\n",
            "\n",
            "Confusion Matrix:\n",
            "[[195   0   0 ...   0   0   2]\n",
            " [  0 203   0 ...   0   0   0]\n",
            " [  3   0 200 ...   0   1   0]\n",
            " ...\n",
            " [  0   0   0 ... 200   0   0]\n",
            " [  0   0   0 ...   0 160   0]\n",
            " [  1   0   0 ...   1   0 197]]\n",
            "Epoch 5/12 - Training Loss: 0.0115 - Validation Loss: 0.9841 - Accuracy: 0.8270\n",
            "Accuracy: 0.8269501672829723\n",
            "Precision: 0.8296702300485206\n",
            "Recall: 0.8269501672829723\n",
            "F1-score: 0.8269669678510759\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.91      0.82       234\n",
            "           1       0.88      0.92      0.90       221\n",
            "           2       0.97      0.86      0.91       226\n",
            "           3       0.82      0.78      0.80       222\n",
            "           4       0.72      0.59      0.65       225\n",
            "           5       0.85      0.84      0.84       224\n",
            "           6       0.78      0.80      0.79       224\n",
            "           7       0.88      0.78      0.83       225\n",
            "           8       0.71      0.85      0.78       226\n",
            "           9       0.83      0.78      0.80       214\n",
            "          10       0.82      0.81      0.81       231\n",
            "          11       0.90      0.87      0.88       227\n",
            "          12       0.89      0.86      0.87       230\n",
            "          13       0.90      0.88      0.89       220\n",
            "          14       0.81      0.81      0.81       231\n",
            "          15       0.91      0.87      0.89       227\n",
            "          16       0.72      0.66      0.69       224\n",
            "          17       0.81      0.88      0.84       233\n",
            "          18       0.87      0.86      0.86       222\n",
            "          19       0.81      0.79      0.80       220\n",
            "          20       0.80      0.94      0.86       219\n",
            "          21       0.79      0.84      0.82       232\n",
            "          22       0.90      0.91      0.91       224\n",
            "          23       0.92      0.92      0.92       230\n",
            "          24       0.92      0.95      0.93       224\n",
            "          25       0.85      0.90      0.87       220\n",
            "          26       0.77      0.84      0.80       221\n",
            "          27       0.92      0.87      0.89       225\n",
            "          28       0.85      0.89      0.87       224\n",
            "          29       0.78      0.73      0.76       228\n",
            "          30       0.97      0.97      0.97       229\n",
            "          31       0.84      0.77      0.80       232\n",
            "          32       0.60      0.80      0.69       228\n",
            "          33       0.76      0.79      0.77       231\n",
            "          34       0.91      0.88      0.90       213\n",
            "          35       0.74      0.84      0.79       202\n",
            "          36       0.85      0.89      0.87       228\n",
            "          37       0.81      0.85      0.83       212\n",
            "          38       0.78      0.68      0.73       208\n",
            "          39       0.83      0.81      0.82       216\n",
            "          40       0.79      0.83      0.81       212\n",
            "          41       0.84      0.91      0.88       223\n",
            "          42       0.81      0.77      0.79       238\n",
            "          43       0.90      0.77      0.83       212\n",
            "          44       0.76      0.80      0.78       229\n",
            "          45       0.87      0.93      0.90       233\n",
            "          46       0.86      0.78      0.82       224\n",
            "          47       0.80      0.83      0.81       234\n",
            "          48       0.82      0.87      0.85       223\n",
            "          49       0.82      0.87      0.84       229\n",
            "          50       0.83      0.82      0.82       225\n",
            "          51       0.93      0.86      0.89       228\n",
            "          52       0.81      0.78      0.79       232\n",
            "          53       0.76      0.69      0.73       233\n",
            "          54       0.87      0.86      0.86       203\n",
            "          55       0.64      0.72      0.68       234\n",
            "          56       0.91      0.93      0.92       223\n",
            "          57       0.84      0.91      0.87       234\n",
            "          58       0.89      0.88      0.89       232\n",
            "          59       0.86      0.89      0.87       233\n",
            "          60       0.79      0.82      0.81       197\n",
            "          61       0.62      0.61      0.62       215\n",
            "          62       0.78      0.79      0.79       238\n",
            "          63       0.94      0.94      0.94       231\n",
            "          64       0.79      0.86      0.82       227\n",
            "          65       0.94      0.89      0.91       234\n",
            "          66       0.85      0.83      0.84       224\n",
            "          67       0.83      0.81      0.82       231\n",
            "          68       0.86      0.83      0.84       217\n",
            "          69       0.78      0.70      0.73       223\n",
            "          70       0.81      0.87      0.83       230\n",
            "          71       0.87      0.85      0.86       228\n",
            "          72       0.80      0.84      0.82       234\n",
            "          73       0.93      0.94      0.93       225\n",
            "          74       0.78      0.75      0.76       211\n",
            "          75       0.87      0.86      0.87       218\n",
            "          76       0.78      0.83      0.81       235\n",
            "          77       0.74      0.81      0.78       227\n",
            "          78       0.85      0.73      0.78       210\n",
            "          79       0.92      0.92      0.92       220\n",
            "          80       0.89      0.94      0.91       224\n",
            "          81       0.75      0.69      0.72       232\n",
            "          82       0.69      0.75      0.72       236\n",
            "          83       0.91      0.86      0.88       227\n",
            "          84       0.83      0.78      0.81       236\n",
            "          85       0.94      0.88      0.91       215\n",
            "          86       0.60      0.61      0.61       226\n",
            "          87       0.76      0.76      0.76       228\n",
            "          88       0.86      0.83      0.85       219\n",
            "          89       0.91      0.76      0.83       228\n",
            "          90       0.89      0.92      0.90       213\n",
            "          91       0.94      0.89      0.92       218\n",
            "          92       0.82      0.86      0.84       230\n",
            "          93       0.78      0.70      0.74       229\n",
            "          94       0.89      0.89      0.89       223\n",
            "          95       0.86      0.76      0.81       237\n",
            "          96       0.71      0.80      0.75       233\n",
            "          97       0.85      0.80      0.82       223\n",
            "          98       0.91      0.88      0.89       232\n",
            "          99       0.77      0.69      0.73       211\n",
            "         100       0.92      0.81      0.86       235\n",
            "\n",
            "    accuracy                           0.83     22716\n",
            "   macro avg       0.83      0.83      0.83     22716\n",
            "weighted avg       0.83      0.83      0.83     22716\n",
            "\n",
            "Confusion Matrix:\n",
            "[[214   0   0 ...   0   0   2]\n",
            " [  0 204   0 ...   0   0   0]\n",
            " [  6   0 194 ...   2   0   0]\n",
            " ...\n",
            " [  0   0   0 ... 204   0   0]\n",
            " [  0   0   0 ...   0 145   0]\n",
            " [  2   0   0 ...   3   0 190]]\n",
            "Epoch 6/12 - Training Loss: 0.0108 - Validation Loss: 0.9964 - Accuracy: 0.8258\n",
            "Accuracy: 0.8257615777425603\n",
            "Precision: 0.8282315040977571\n",
            "Recall: 0.8257615777425603\n",
            "F1-score: 0.8253346079013292\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.90      0.82       234\n",
            "           1       0.93      0.92      0.92       221\n",
            "           2       0.89      0.93      0.91       226\n",
            "           3       0.76      0.82      0.79       222\n",
            "           4       0.60      0.66      0.63       225\n",
            "           5       0.86      0.82      0.84       224\n",
            "           6       0.81      0.79      0.80       224\n",
            "           7       0.84      0.79      0.81       225\n",
            "           8       0.86      0.78      0.82       226\n",
            "           9       0.81      0.82      0.82       214\n",
            "          10       0.76      0.85      0.80       231\n",
            "          11       0.88      0.88      0.88       227\n",
            "          12       0.90      0.84      0.87       230\n",
            "          13       0.84      0.92      0.88       220\n",
            "          14       0.81      0.82      0.82       231\n",
            "          15       0.84      0.89      0.87       227\n",
            "          16       0.73      0.66      0.69       224\n",
            "          17       0.83      0.85      0.84       233\n",
            "          18       0.89      0.87      0.88       222\n",
            "          19       0.81      0.80      0.81       220\n",
            "          20       0.89      0.93      0.91       219\n",
            "          21       0.81      0.84      0.83       232\n",
            "          22       0.88      0.88      0.88       224\n",
            "          23       0.93      0.93      0.93       230\n",
            "          24       0.89      0.95      0.92       224\n",
            "          25       0.81      0.90      0.85       220\n",
            "          26       0.79      0.83      0.81       221\n",
            "          27       0.93      0.88      0.90       225\n",
            "          28       0.85      0.88      0.86       224\n",
            "          29       0.79      0.73      0.76       228\n",
            "          30       0.97      0.97      0.97       229\n",
            "          31       0.82      0.77      0.79       232\n",
            "          32       0.70      0.70      0.70       228\n",
            "          33       0.81      0.77      0.79       231\n",
            "          34       0.87      0.93      0.90       213\n",
            "          35       0.81      0.79      0.80       202\n",
            "          36       0.81      0.91      0.86       228\n",
            "          37       0.83      0.84      0.84       212\n",
            "          38       0.77      0.75      0.76       208\n",
            "          39       0.85      0.77      0.81       216\n",
            "          40       0.88      0.76      0.82       212\n",
            "          41       0.87      0.90      0.88       223\n",
            "          42       0.89      0.75      0.82       238\n",
            "          43       0.85      0.81      0.83       212\n",
            "          44       0.79      0.79      0.79       229\n",
            "          45       0.90      0.86      0.88       233\n",
            "          46       0.85      0.77      0.81       224\n",
            "          47       0.68      0.84      0.75       234\n",
            "          48       0.79      0.89      0.84       223\n",
            "          49       0.84      0.87      0.86       229\n",
            "          50       0.82      0.81      0.82       225\n",
            "          51       0.92      0.88      0.90       228\n",
            "          52       0.74      0.78      0.76       232\n",
            "          53       0.76      0.67      0.71       233\n",
            "          54       0.85      0.86      0.85       203\n",
            "          55       0.75      0.70      0.72       234\n",
            "          56       0.97      0.92      0.95       223\n",
            "          57       0.86      0.88      0.87       234\n",
            "          58       0.85      0.85      0.85       232\n",
            "          59       0.78      0.92      0.84       233\n",
            "          60       0.81      0.82      0.82       197\n",
            "          61       0.56      0.64      0.59       215\n",
            "          62       0.79      0.81      0.80       238\n",
            "          63       0.89      0.95      0.92       231\n",
            "          64       0.86      0.78      0.82       227\n",
            "          65       0.86      0.92      0.89       234\n",
            "          66       0.89      0.84      0.86       224\n",
            "          67       0.79      0.84      0.81       231\n",
            "          68       0.91      0.82      0.86       217\n",
            "          69       0.67      0.71      0.69       223\n",
            "          70       0.72      0.86      0.78       230\n",
            "          71       0.87      0.87      0.87       228\n",
            "          72       0.79      0.84      0.81       234\n",
            "          73       0.91      0.95      0.93       225\n",
            "          74       0.95      0.67      0.78       211\n",
            "          75       0.81      0.83      0.82       218\n",
            "          76       0.78      0.86      0.81       235\n",
            "          77       0.83      0.74      0.78       227\n",
            "          78       0.77      0.80      0.78       210\n",
            "          79       0.91      0.92      0.92       220\n",
            "          80       0.86      0.93      0.89       224\n",
            "          81       0.81      0.65      0.72       232\n",
            "          82       0.86      0.62      0.72       236\n",
            "          83       0.87      0.87      0.87       227\n",
            "          84       0.83      0.81      0.82       236\n",
            "          85       0.85      0.90      0.88       215\n",
            "          86       0.65      0.60      0.62       226\n",
            "          87       0.73      0.77      0.75       228\n",
            "          88       0.88      0.84      0.86       219\n",
            "          89       0.85      0.85      0.85       228\n",
            "          90       0.86      0.91      0.88       213\n",
            "          91       0.93      0.92      0.92       218\n",
            "          92       0.84      0.83      0.83       230\n",
            "          93       0.74      0.70      0.72       229\n",
            "          94       0.89      0.88      0.89       223\n",
            "          95       0.88      0.73      0.80       237\n",
            "          96       0.84      0.80      0.82       233\n",
            "          97       0.74      0.83      0.79       223\n",
            "          98       0.88      0.88      0.88       232\n",
            "          99       0.80      0.69      0.74       211\n",
            "         100       0.91      0.83      0.86       235\n",
            "\n",
            "    accuracy                           0.83     22716\n",
            "   macro avg       0.83      0.83      0.83     22716\n",
            "weighted avg       0.83      0.83      0.83     22716\n",
            "\n",
            "Confusion Matrix:\n",
            "[[210   0   1 ...   0   0   1]\n",
            " [  0 203   0 ...   0   0   0]\n",
            " [  2   0 211 ...   1   0   0]\n",
            " ...\n",
            " [  0   0   2 ... 203   0   0]\n",
            " [  0   0   0 ...   0 145   0]\n",
            " [  3   0   0 ...   2   0 194]]\n",
            "Epoch 7/12 - Training Loss: 0.0100 - Validation Loss: 1.0653 - Accuracy: 0.8196\n",
            "Accuracy: 0.8195544990315197\n",
            "Precision: 0.8251840075722694\n",
            "Recall: 0.8195544990315197\n",
            "F1-score: 0.8202933506655182\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.85      0.84       234\n",
            "           1       0.92      0.94      0.93       221\n",
            "           2       0.94      0.89      0.91       226\n",
            "           3       0.72      0.85      0.78       222\n",
            "           4       0.66      0.60      0.63       225\n",
            "           5       0.85      0.83      0.84       224\n",
            "           6       0.75      0.79      0.77       224\n",
            "           7       0.86      0.73      0.79       225\n",
            "           8       0.81      0.83      0.82       226\n",
            "           9       0.82      0.82      0.82       214\n",
            "          10       0.82      0.81      0.81       231\n",
            "          11       0.85      0.89      0.87       227\n",
            "          12       0.94      0.82      0.88       230\n",
            "          13       0.87      0.87      0.87       220\n",
            "          14       0.81      0.78      0.80       231\n",
            "          15       0.89      0.87      0.88       227\n",
            "          16       0.59      0.69      0.63       224\n",
            "          17       0.74      0.93      0.82       233\n",
            "          18       0.88      0.83      0.85       222\n",
            "          19       0.70      0.81      0.75       220\n",
            "          20       0.86      0.89      0.88       219\n",
            "          21       0.82      0.85      0.83       232\n",
            "          22       0.92      0.90      0.91       224\n",
            "          23       0.93      0.93      0.93       230\n",
            "          24       0.87      0.95      0.91       224\n",
            "          25       0.84      0.90      0.87       220\n",
            "          26       0.80      0.84      0.82       221\n",
            "          27       0.89      0.89      0.89       225\n",
            "          28       0.78      0.90      0.84       224\n",
            "          29       0.71      0.78      0.74       228\n",
            "          30       0.98      0.98      0.98       229\n",
            "          31       0.83      0.77      0.80       232\n",
            "          32       0.73      0.71      0.72       228\n",
            "          33       0.79      0.74      0.77       231\n",
            "          34       0.91      0.86      0.89       213\n",
            "          35       0.78      0.80      0.79       202\n",
            "          36       0.94      0.87      0.90       228\n",
            "          37       0.74      0.87      0.80       212\n",
            "          38       0.72      0.71      0.71       208\n",
            "          39       0.87      0.76      0.81       216\n",
            "          40       0.82      0.77      0.79       212\n",
            "          41       0.77      0.93      0.85       223\n",
            "          42       0.83      0.76      0.80       238\n",
            "          43       0.88      0.77      0.82       212\n",
            "          44       0.69      0.85      0.76       229\n",
            "          45       0.93      0.82      0.87       233\n",
            "          46       0.83      0.77      0.80       224\n",
            "          47       0.80      0.80      0.80       234\n",
            "          48       0.74      0.92      0.82       223\n",
            "          49       0.82      0.89      0.85       229\n",
            "          50       0.75      0.84      0.79       225\n",
            "          51       0.93      0.86      0.89       228\n",
            "          52       0.79      0.76      0.78       232\n",
            "          53       0.75      0.72      0.74       233\n",
            "          54       0.84      0.84      0.84       203\n",
            "          55       0.84      0.66      0.74       234\n",
            "          56       0.96      0.92      0.94       223\n",
            "          57       0.88      0.87      0.87       234\n",
            "          58       0.84      0.84      0.84       232\n",
            "          59       0.88      0.86      0.87       233\n",
            "          60       0.79      0.83      0.81       197\n",
            "          61       0.55      0.62      0.58       215\n",
            "          62       0.78      0.80      0.79       238\n",
            "          63       0.95      0.90      0.92       231\n",
            "          64       0.77      0.85      0.81       227\n",
            "          65       0.93      0.88      0.91       234\n",
            "          66       0.85      0.85      0.85       224\n",
            "          67       0.86      0.78      0.82       231\n",
            "          68       0.91      0.83      0.87       217\n",
            "          69       0.76      0.69      0.72       223\n",
            "          70       0.96      0.71      0.82       230\n",
            "          71       0.88      0.88      0.88       228\n",
            "          72       0.79      0.79      0.79       234\n",
            "          73       0.93      0.95      0.94       225\n",
            "          74       0.72      0.77      0.74       211\n",
            "          75       0.85      0.83      0.84       218\n",
            "          76       0.81      0.83      0.82       235\n",
            "          77       0.73      0.79      0.76       227\n",
            "          78       0.76      0.79      0.77       210\n",
            "          79       0.92      0.92      0.92       220\n",
            "          80       0.88      0.92      0.90       224\n",
            "          81       0.73      0.71      0.72       232\n",
            "          82       0.70      0.71      0.70       236\n",
            "          83       0.88      0.88      0.88       227\n",
            "          84       0.82      0.77      0.79       236\n",
            "          85       0.94      0.88      0.91       215\n",
            "          86       0.47      0.64      0.54       226\n",
            "          87       0.71      0.77      0.74       228\n",
            "          88       0.82      0.84      0.83       219\n",
            "          89       0.86      0.80      0.83       228\n",
            "          90       0.89      0.91      0.90       213\n",
            "          91       0.96      0.89      0.92       218\n",
            "          92       0.87      0.82      0.84       230\n",
            "          93       0.80      0.69      0.74       229\n",
            "          94       0.87      0.89      0.88       223\n",
            "          95       0.86      0.68      0.75       237\n",
            "          96       0.85      0.78      0.82       233\n",
            "          97       0.73      0.84      0.78       223\n",
            "          98       0.93      0.86      0.89       232\n",
            "          99       0.82      0.59      0.69       211\n",
            "         100       0.87      0.86      0.86       235\n",
            "\n",
            "    accuracy                           0.82     22716\n",
            "   macro avg       0.82      0.82      0.82     22716\n",
            "weighted avg       0.83      0.82      0.82     22716\n",
            "\n",
            "Confusion Matrix:\n",
            "[[199   0   0 ...   0   0   3]\n",
            " [  0 207   0 ...   0   0   0]\n",
            " [  3   0 202 ...   2   0   0]\n",
            " ...\n",
            " [  0   0   0 ... 199   0   0]\n",
            " [  0   0   0 ...   0 125   0]\n",
            " [  0   0   0 ...   1   0 201]]\n",
            "Epoch 8/12 - Training Loss: 0.0099 - Validation Loss: 1.0002 - Accuracy: 0.8293\n",
            "Accuracy: 0.8293273463637965\n",
            "Precision: 0.8312245484332453\n",
            "Recall: 0.8293273463637965\n",
            "F1-score: 0.8291526346740948\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.82      0.83       234\n",
            "           1       0.92      0.93      0.93       221\n",
            "           2       0.87      0.92      0.90       226\n",
            "           3       0.76      0.81      0.78       222\n",
            "           4       0.65      0.64      0.64       225\n",
            "           5       0.86      0.85      0.86       224\n",
            "           6       0.73      0.83      0.78       224\n",
            "           7       0.89      0.76      0.82       225\n",
            "           8       0.82      0.82      0.82       226\n",
            "           9       0.72      0.85      0.78       214\n",
            "          10       0.74      0.86      0.80       231\n",
            "          11       0.86      0.88      0.87       227\n",
            "          12       0.88      0.88      0.88       230\n",
            "          13       0.87      0.92      0.89       220\n",
            "          14       0.77      0.81      0.79       231\n",
            "          15       0.92      0.88      0.90       227\n",
            "          16       0.69      0.66      0.68       224\n",
            "          17       0.87      0.83      0.85       233\n",
            "          18       0.87      0.84      0.85       222\n",
            "          19       0.82      0.77      0.79       220\n",
            "          20       0.89      0.91      0.90       219\n",
            "          21       0.80      0.87      0.83       232\n",
            "          22       0.90      0.92      0.91       224\n",
            "          23       0.90      0.95      0.93       230\n",
            "          24       0.92      0.94      0.93       224\n",
            "          25       0.80      0.91      0.85       220\n",
            "          26       0.82      0.79      0.80       221\n",
            "          27       0.92      0.88      0.90       225\n",
            "          28       0.90      0.88      0.89       224\n",
            "          29       0.80      0.73      0.77       228\n",
            "          30       0.98      0.98      0.98       229\n",
            "          31       0.85      0.75      0.80       232\n",
            "          32       0.75      0.70      0.72       228\n",
            "          33       0.80      0.74      0.77       231\n",
            "          34       0.89      0.92      0.90       213\n",
            "          35       0.72      0.84      0.77       202\n",
            "          36       0.81      0.89      0.85       228\n",
            "          37       0.88      0.82      0.85       212\n",
            "          38       0.80      0.75      0.77       208\n",
            "          39       0.78      0.81      0.80       216\n",
            "          40       0.83      0.81      0.82       212\n",
            "          41       0.83      0.91      0.87       223\n",
            "          42       0.84      0.79      0.81       238\n",
            "          43       0.83      0.81      0.82       212\n",
            "          44       0.78      0.79      0.79       229\n",
            "          45       0.94      0.86      0.90       233\n",
            "          46       0.76      0.82      0.79       224\n",
            "          47       0.86      0.79      0.83       234\n",
            "          48       0.83      0.90      0.86       223\n",
            "          49       0.87      0.83      0.85       229\n",
            "          50       0.82      0.84      0.83       225\n",
            "          51       0.92      0.89      0.90       228\n",
            "          52       0.91      0.78      0.84       232\n",
            "          53       0.75      0.71      0.73       233\n",
            "          54       0.93      0.81      0.86       203\n",
            "          55       0.69      0.73      0.71       234\n",
            "          56       0.93      0.94      0.94       223\n",
            "          57       0.86      0.88      0.87       234\n",
            "          58       0.87      0.88      0.88       232\n",
            "          59       0.76      0.93      0.84       233\n",
            "          60       0.76      0.82      0.79       197\n",
            "          61       0.57      0.64      0.60       215\n",
            "          62       0.83      0.77      0.80       238\n",
            "          63       0.97      0.92      0.94       231\n",
            "          64       0.82      0.85      0.83       227\n",
            "          65       0.89      0.89      0.89       234\n",
            "          66       0.82      0.88      0.85       224\n",
            "          67       0.83      0.78      0.81       231\n",
            "          68       0.86      0.85      0.86       217\n",
            "          69       0.66      0.70      0.68       223\n",
            "          70       0.86      0.83      0.85       230\n",
            "          71       0.91      0.86      0.88       228\n",
            "          72       0.82      0.79      0.81       234\n",
            "          73       0.92      0.96      0.94       225\n",
            "          74       0.87      0.73      0.79       211\n",
            "          75       0.76      0.89      0.82       218\n",
            "          76       0.84      0.78      0.81       235\n",
            "          77       0.80      0.79      0.80       227\n",
            "          78       0.83      0.77      0.80       210\n",
            "          79       0.93      0.91      0.92       220\n",
            "          80       0.85      0.94      0.89       224\n",
            "          81       0.73      0.69      0.71       232\n",
            "          82       0.77      0.69      0.72       236\n",
            "          83       0.86      0.91      0.88       227\n",
            "          84       0.77      0.79      0.78       236\n",
            "          85       0.95      0.88      0.91       215\n",
            "          86       0.69      0.61      0.64       226\n",
            "          87       0.78      0.77      0.78       228\n",
            "          88       0.84      0.84      0.84       219\n",
            "          89       0.84      0.81      0.83       228\n",
            "          90       0.84      0.93      0.88       213\n",
            "          91       0.94      0.89      0.92       218\n",
            "          92       0.82      0.84      0.83       230\n",
            "          93       0.76      0.74      0.75       229\n",
            "          94       0.84      0.92      0.88       223\n",
            "          95       0.83      0.78      0.80       237\n",
            "          96       0.84      0.76      0.80       233\n",
            "          97       0.82      0.81      0.81       223\n",
            "          98       0.88      0.92      0.90       232\n",
            "          99       0.74      0.74      0.74       211\n",
            "         100       0.91      0.84      0.87       235\n",
            "\n",
            "    accuracy                           0.83     22716\n",
            "   macro avg       0.83      0.83      0.83     22716\n",
            "weighted avg       0.83      0.83      0.83     22716\n",
            "\n",
            "Confusion Matrix:\n",
            "[[192   1   2 ...   0   0   1]\n",
            " [  0 206   0 ...   0   0   0]\n",
            " [  2   0 209 ...   0   0   0]\n",
            " ...\n",
            " [  0   0   1 ... 213   0   0]\n",
            " [  0   0   0 ...   0 156   0]\n",
            " [  0   0   0 ...   3   0 197]]\n",
            "Epoch 9/12 - Training Loss: 0.0095 - Validation Loss: 1.0105 - Accuracy: 0.8293\n",
            "Accuracy: 0.8293273463637965\n",
            "Precision: 0.8320388786984166\n",
            "Recall: 0.8293273463637965\n",
            "F1-score: 0.8293250137018796\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.85      0.85       234\n",
            "           1       0.92      0.93      0.92       221\n",
            "           2       0.88      0.92      0.90       226\n",
            "           3       0.75      0.82      0.79       222\n",
            "           4       0.64      0.62      0.63       225\n",
            "           5       0.84      0.85      0.84       224\n",
            "           6       0.76      0.83      0.79       224\n",
            "           7       0.89      0.76      0.82       225\n",
            "           8       0.81      0.81      0.81       226\n",
            "           9       0.82      0.81      0.81       214\n",
            "          10       0.84      0.78      0.81       231\n",
            "          11       0.88      0.89      0.89       227\n",
            "          12       0.89      0.86      0.87       230\n",
            "          13       0.85      0.91      0.88       220\n",
            "          14       0.81      0.80      0.81       231\n",
            "          15       0.89      0.89      0.89       227\n",
            "          16       0.72      0.62      0.67       224\n",
            "          17       0.84      0.89      0.87       233\n",
            "          18       0.85      0.86      0.85       222\n",
            "          19       0.80      0.80      0.80       220\n",
            "          20       0.90      0.91      0.90       219\n",
            "          21       0.80      0.88      0.84       232\n",
            "          22       0.90      0.91      0.91       224\n",
            "          23       0.93      0.92      0.93       230\n",
            "          24       0.92      0.94      0.93       224\n",
            "          25       0.82      0.90      0.86       220\n",
            "          26       0.81      0.81      0.81       221\n",
            "          27       0.94      0.88      0.91       225\n",
            "          28       0.84      0.88      0.86       224\n",
            "          29       0.79      0.77      0.78       228\n",
            "          30       0.97      0.98      0.97       229\n",
            "          31       0.84      0.77      0.80       232\n",
            "          32       0.76      0.70      0.73       228\n",
            "          33       0.74      0.78      0.76       231\n",
            "          34       0.91      0.90      0.91       213\n",
            "          35       0.74      0.82      0.78       202\n",
            "          36       0.84      0.91      0.87       228\n",
            "          37       0.83      0.84      0.83       212\n",
            "          38       0.84      0.68      0.75       208\n",
            "          39       0.83      0.80      0.82       216\n",
            "          40       0.84      0.78      0.81       212\n",
            "          41       0.88      0.90      0.89       223\n",
            "          42       0.85      0.74      0.79       238\n",
            "          43       0.81      0.82      0.81       212\n",
            "          44       0.81      0.79      0.80       229\n",
            "          45       0.89      0.93      0.91       233\n",
            "          46       0.70      0.85      0.77       224\n",
            "          47       0.74      0.85      0.79       234\n",
            "          48       0.86      0.87      0.87       223\n",
            "          49       0.86      0.85      0.85       229\n",
            "          50       0.80      0.85      0.83       225\n",
            "          51       0.92      0.89      0.91       228\n",
            "          52       0.84      0.78      0.81       232\n",
            "          53       0.78      0.69      0.73       233\n",
            "          54       0.84      0.88      0.86       203\n",
            "          55       0.72      0.68      0.70       234\n",
            "          56       0.92      0.92      0.92       223\n",
            "          57       0.84      0.88      0.86       234\n",
            "          58       0.83      0.89      0.86       232\n",
            "          59       0.88      0.88      0.88       233\n",
            "          60       0.83      0.82      0.82       197\n",
            "          61       0.54      0.70      0.61       215\n",
            "          62       0.80      0.83      0.82       238\n",
            "          63       0.94      0.93      0.93       231\n",
            "          64       0.81      0.87      0.84       227\n",
            "          65       0.93      0.91      0.92       234\n",
            "          66       0.86      0.83      0.85       224\n",
            "          67       0.86      0.81      0.83       231\n",
            "          68       0.91      0.82      0.86       217\n",
            "          69       0.65      0.72      0.68       223\n",
            "          70       0.87      0.82      0.84       230\n",
            "          71       0.91      0.83      0.87       228\n",
            "          72       0.77      0.86      0.81       234\n",
            "          73       0.90      0.96      0.93       225\n",
            "          74       0.77      0.77      0.77       211\n",
            "          75       0.80      0.88      0.84       218\n",
            "          76       0.84      0.81      0.83       235\n",
            "          77       0.83      0.76      0.80       227\n",
            "          78       0.79      0.77      0.78       210\n",
            "          79       0.93      0.92      0.92       220\n",
            "          80       0.88      0.92      0.90       224\n",
            "          81       0.74      0.73      0.73       232\n",
            "          82       0.74      0.74      0.74       236\n",
            "          83       0.90      0.86      0.88       227\n",
            "          84       0.80      0.81      0.81       236\n",
            "          85       0.91      0.89      0.90       215\n",
            "          86       0.56      0.65      0.60       226\n",
            "          87       0.77      0.75      0.76       228\n",
            "          88       0.83      0.84      0.83       219\n",
            "          89       0.88      0.82      0.85       228\n",
            "          90       0.89      0.92      0.91       213\n",
            "          91       0.90      0.92      0.91       218\n",
            "          92       0.85      0.87      0.86       230\n",
            "          93       0.84      0.59      0.70       229\n",
            "          94       0.88      0.88      0.88       223\n",
            "          95       0.85      0.74      0.79       237\n",
            "          96       0.88      0.74      0.81       233\n",
            "          97       0.68      0.85      0.76       223\n",
            "          98       0.91      0.90      0.90       232\n",
            "          99       0.76      0.69      0.72       211\n",
            "         100       0.91      0.82      0.86       235\n",
            "\n",
            "    accuracy                           0.83     22716\n",
            "   macro avg       0.83      0.83      0.83     22716\n",
            "weighted avg       0.83      0.83      0.83     22716\n",
            "\n",
            "Confusion Matrix:\n",
            "[[199   0   2 ...   0   0   3]\n",
            " [  0 205   0 ...   0   0   0]\n",
            " [  2   0 207 ...   0   1   0]\n",
            " ...\n",
            " [  0   0   1 ... 208   0   0]\n",
            " [  0   0   0 ...   0 146   0]\n",
            " [  0   0   0 ...   2   0 192]]\n",
            "Epoch 10/12 - Training Loss: 0.0094 - Validation Loss: 1.0312 - Accuracy: 0.8276\n",
            "Accuracy: 0.8276104948054235\n",
            "Precision: 0.8333129631089038\n",
            "Recall: 0.8276104948054235\n",
            "F1-score: 0.8285029414137359\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.82      0.84       234\n",
            "           1       0.89      0.91      0.90       221\n",
            "           2       0.95      0.92      0.93       226\n",
            "           3       0.77      0.83      0.80       222\n",
            "           4       0.57      0.68      0.62       225\n",
            "           5       0.86      0.82      0.84       224\n",
            "           6       0.68      0.82      0.74       224\n",
            "           7       0.84      0.80      0.82       225\n",
            "           8       0.86      0.81      0.83       226\n",
            "           9       0.76      0.83      0.79       214\n",
            "          10       0.80      0.81      0.80       231\n",
            "          11       0.90      0.88      0.89       227\n",
            "          12       0.85      0.87      0.86       230\n",
            "          13       0.84      0.93      0.88       220\n",
            "          14       0.81      0.82      0.82       231\n",
            "          15       0.84      0.91      0.88       227\n",
            "          16       0.62      0.68      0.65       224\n",
            "          17       0.83      0.88      0.85       233\n",
            "          18       0.97      0.72      0.82       222\n",
            "          19       0.83      0.78      0.81       220\n",
            "          20       0.84      0.91      0.88       219\n",
            "          21       0.85      0.81      0.83       232\n",
            "          22       0.89      0.94      0.91       224\n",
            "          23       0.92      0.93      0.92       230\n",
            "          24       0.90      0.96      0.93       224\n",
            "          25       0.82      0.90      0.86       220\n",
            "          26       0.76      0.86      0.81       221\n",
            "          27       0.94      0.89      0.92       225\n",
            "          28       0.78      0.92      0.84       224\n",
            "          29       0.82      0.71      0.77       228\n",
            "          30       0.97      0.97      0.97       229\n",
            "          31       0.81      0.79      0.80       232\n",
            "          32       0.77      0.68      0.72       228\n",
            "          33       0.82      0.72      0.77       231\n",
            "          34       0.90      0.90      0.90       213\n",
            "          35       0.77      0.81      0.79       202\n",
            "          36       0.91      0.86      0.89       228\n",
            "          37       0.84      0.86      0.85       212\n",
            "          38       0.79      0.70      0.74       208\n",
            "          39       0.84      0.76      0.80       216\n",
            "          40       0.81      0.81      0.81       212\n",
            "          41       0.84      0.91      0.87       223\n",
            "          42       0.85      0.75      0.80       238\n",
            "          43       0.81      0.82      0.81       212\n",
            "          44       0.86      0.75      0.80       229\n",
            "          45       0.88      0.90      0.89       233\n",
            "          46       0.85      0.77      0.81       224\n",
            "          47       0.70      0.84      0.76       234\n",
            "          48       0.84      0.88      0.86       223\n",
            "          49       0.89      0.84      0.87       229\n",
            "          50       0.76      0.85      0.80       225\n",
            "          51       0.95      0.86      0.90       228\n",
            "          52       0.79      0.82      0.81       232\n",
            "          53       0.75      0.72      0.73       233\n",
            "          54       0.88      0.84      0.86       203\n",
            "          55       0.74      0.69      0.71       234\n",
            "          56       0.89      0.94      0.91       223\n",
            "          57       0.89      0.85      0.87       234\n",
            "          58       0.89      0.84      0.87       232\n",
            "          59       0.81      0.90      0.85       233\n",
            "          60       0.76      0.81      0.78       197\n",
            "          61       0.47      0.69      0.56       215\n",
            "          62       0.72      0.83      0.77       238\n",
            "          63       0.94      0.94      0.94       231\n",
            "          64       0.86      0.84      0.85       227\n",
            "          65       0.94      0.90      0.92       234\n",
            "          66       0.90      0.87      0.88       224\n",
            "          67       0.77      0.84      0.80       231\n",
            "          68       0.95      0.78      0.86       217\n",
            "          69       0.66      0.71      0.68       223\n",
            "          70       0.90      0.83      0.86       230\n",
            "          71       0.88      0.90      0.89       228\n",
            "          72       0.78      0.85      0.81       234\n",
            "          73       0.90      0.97      0.93       225\n",
            "          74       0.85      0.74      0.79       211\n",
            "          75       0.91      0.84      0.87       218\n",
            "          76       0.74      0.87      0.80       235\n",
            "          77       0.81      0.77      0.79       227\n",
            "          78       0.66      0.81      0.73       210\n",
            "          79       0.94      0.93      0.93       220\n",
            "          80       0.89      0.92      0.91       224\n",
            "          81       0.76      0.76      0.76       232\n",
            "          82       0.77      0.70      0.73       236\n",
            "          83       0.88      0.89      0.89       227\n",
            "          84       0.83      0.74      0.78       236\n",
            "          85       0.94      0.88      0.91       215\n",
            "          86       0.66      0.61      0.64       226\n",
            "          87       0.77      0.75      0.76       228\n",
            "          88       0.91      0.84      0.87       219\n",
            "          89       0.90      0.85      0.87       228\n",
            "          90       0.92      0.90      0.91       213\n",
            "          91       0.93      0.89      0.91       218\n",
            "          92       0.89      0.82      0.86       230\n",
            "          93       0.74      0.73      0.74       229\n",
            "          94       0.88      0.89      0.88       223\n",
            "          95       0.85      0.77      0.81       237\n",
            "          96       0.91      0.73      0.81       233\n",
            "          97       0.76      0.84      0.80       223\n",
            "          98       0.96      0.88      0.92       232\n",
            "          99       0.78      0.69      0.73       211\n",
            "         100       0.93      0.82      0.87       235\n",
            "\n",
            "    accuracy                           0.83     22716\n",
            "   macro avg       0.83      0.83      0.83     22716\n",
            "weighted avg       0.83      0.83      0.83     22716\n",
            "\n",
            "Confusion Matrix:\n",
            "[[193   1   0 ...   0   0   1]\n",
            " [  0 201   0 ...   0   0   0]\n",
            " [  2   0 207 ...   0   0   0]\n",
            " ...\n",
            " [  0   0   0 ... 203   0   0]\n",
            " [  0   0   0 ...   0 145   0]\n",
            " [  0   0   0 ...   1   0 193]]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-fe672cf153fe>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m                                                f\"but got {result}.\")\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/optimization.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    465\u001b[0m                 \u001b[0;31m# In-place operations to update the averages at the same time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eps\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    vit.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = vit(images).logits\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * images.size(0)\n",
        "\n",
        "    # Calculate average loss for this epoch\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "\n",
        "    save_dir = \"/content/Model/Models-Train-12/\"\n",
        "    os.makedirs(save_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
        "\n",
        "    model_name = str((epoch+9)) + \"_model.pth\"\n",
        "    save_path = os.path.join(save_dir, model_name)  # Specify the complete path to the model file\n",
        "    torch.save(vit.state_dict(), save_path)\n",
        "\n",
        "    # Validation\n",
        "    vit.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    predicted_classes = []\n",
        "    actual_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = vit(images).logits\n",
        "            val_loss += criterion(outputs, labels).item() * images.size(0)\n",
        "\n",
        "            probabilities = torch.softmax(outputs, dim=1)\n",
        "            predicted = torch.argmax(probabilities, dim=1)\n",
        "\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            predicted_classes.extend(predicted.cpu().numpy())\n",
        "            actual_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate average loss and accuracy for validation set\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "    accuracy = correct / len(val_loader.dataset)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Training Loss: {train_loss:.4f} - Validation Loss: {val_loss:.4f} - Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    # Compute evaluation metrics using the predicted_classes and actual_labels lists\n",
        "    accuracy = accuracy_score(actual_labels, predicted_classes)\n",
        "    precision = precision_score(actual_labels, predicted_classes, average='weighted')\n",
        "    recall = recall_score(actual_labels, predicted_classes, average='weighted')\n",
        "    f1 = f1_score(actual_labels, predicted_classes, average='weighted')\n",
        "\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1-score:\", f1)\n",
        "    print(classification_report(actual_labels, predicted_classes))\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(actual_labels, predicted_classes))\n",
        "    # print(\"AUROC:\", roc_auc_score(actual_labels, predicted_classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dPB8YABeK7K"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}