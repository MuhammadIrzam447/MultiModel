{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MuhammadIrzam447/MultiModel/blob/master/Train_35.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WVy99Qm3peCz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e8d893f-c38a-46b8-8df0-a980bebcbfd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.5)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.32.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.3.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.16.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets transformers\n",
        "from datasets import Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HztNdeXykDNE"
      },
      "source": [
        "Download Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"/content/Dataset(s)/text-image-ferramenta/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59nT4j7KmwI1",
        "outputId": "13fcd6df-002e-4989-bbd6-7d7f963c220b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Dataset(s)/text-image-ferramenta\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRlciFFZ5nS7"
      },
      "outputs": [],
      "source": [
        "# !gdown https://drive.google.com/uc?id=16whYrlXvaWMgMrn1yCqv1EmFSW-PykNH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_IeKvau5pg_"
      },
      "outputs": [],
      "source": [
        "# !gdown https://drive.google.com/uc?id=179e1TGYPti49AKHG8v5JZ9yMaNJgQwyZ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnkSUhGz5vn5"
      },
      "outputs": [],
      "source": [
        "# !gdown https://drive.google.com/uc?id=1ILTZBqbZ8g5pnuSBSd0uCsra0lEnJYRb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7o1BVLyg50yO"
      },
      "outputs": [],
      "source": [
        "# !gdown https://drive.google.com/uc?id=1KlN-nBCyraJsfYLyFdUQQbZMZWFNG-S1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyuX0HmvkFWs"
      },
      "source": [
        "Extract Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQ6BRaD5zDr6"
      },
      "outputs": [],
      "source": [
        "import tarfile\n",
        "\n",
        "with tarfile.open('./text-val.tar.gz', 'r:gz') as tar_ref:\n",
        "    tar_ref.extractall('./ferramenta_texts/')\n",
        "\n",
        "with tarfile.open('./text-train.tar.gz', 'r:gz') as tar_ref:\n",
        "    tar_ref.extractall('./ferramenta_texts')\n",
        "\n",
        "with tarfile.open('./images-val.tar.gz', 'r:gz') as tar_ref:\n",
        "    tar_ref.extractall('./ferramenta_Images/')\n",
        "\n",
        "with tarfile.open('./images-train.tar.gz', 'r:gz') as tar_ref:\n",
        "    tar_ref.extractall('./ferramenta_Images/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxB4E52dkHsz"
      },
      "source": [
        "Create Label Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNYNj3vp7JYY"
      },
      "outputs": [],
      "source": [
        "#Create txt file for Image Paths and Labels\n",
        "import os\n",
        "\n",
        "dataset_folder = './ferramenta_Images/images-train'\n",
        "output_file = './ferramenta_Images/train_dataset.txt'\n",
        "\n",
        "def custom_sort_key(item):\n",
        "    return item.replace('@', '')\n",
        "\n",
        "with open(output_file, 'w') as f:\n",
        "    for label in sorted(os.listdir(dataset_folder), key=custom_sort_key):\n",
        "        label_path = os.path.join(dataset_folder, label)\n",
        "        if os.path.isdir(label_path):\n",
        "            class_name = label\n",
        "            image_files = sorted(os.listdir(label_path))\n",
        "            for image_file in image_files:\n",
        "                image_path = os.path.join(label_path, image_file)\n",
        "                f.write(f'{image_path}, {class_name}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "foMzU02WkEJ4"
      },
      "outputs": [],
      "source": [
        "#Create txt file for Image Paths and Labels\n",
        "import os\n",
        "\n",
        "dataset_folder = './ferramenta_Images/images-val'\n",
        "output_file = './ferramenta_Images/test_dataset.txt'\n",
        "\n",
        "def custom_sort_key(item):\n",
        "    return item.replace('@', '')\n",
        "\n",
        "with open(output_file, 'w') as f:\n",
        "    for label in sorted(os.listdir(dataset_folder), key=custom_sort_key):\n",
        "        label_path = os.path.join(dataset_folder, label)\n",
        "        if os.path.isdir(label_path):\n",
        "            class_name = label\n",
        "            image_files = sorted(os.listdir(label_path))\n",
        "            for image_file in image_files:\n",
        "                image_path = os.path.join(label_path, image_file)\n",
        "                f.write(f'{image_path}, {class_name}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnEIc61XkMVP"
      },
      "source": [
        "Load Image Data From Label Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "P3fwjA0ikTKC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "image_file_paths = []\n",
        "image_genre_labels = []\n",
        "\n",
        "labels_file = \"./ferramenta_Images/train_dataset.txt\"\n",
        "\n",
        "with open(labels_file, 'r') as file:\n",
        "    for line in file:\n",
        "        parts = line.strip().split(', ')\n",
        "        filename = parts[0].strip()\n",
        "        labels = parts[1].strip()\n",
        "        image_file_paths.append(filename)\n",
        "        image_genre_labels.append(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tx2nBn5Llige",
        "outputId": "6ce5366c-405e-4979-a53d-7b1473d294e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "66141\n",
            "66141\n"
          ]
        }
      ],
      "source": [
        "print(len(image_file_paths))\n",
        "print(len(image_genre_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8dnbr4g3kJ7k"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "test_image_file_paths = []\n",
        "test_image_genre_labels = []\n",
        "\n",
        "labels_file = \"./ferramenta_Images/test_dataset.txt\"\n",
        "\n",
        "with open(labels_file, 'r') as file:\n",
        "    for line in file:\n",
        "        parts = line.strip().split(', ')\n",
        "        filename = parts[0].strip()\n",
        "        labels = parts[1].strip()\n",
        "        test_image_file_paths.append(filename)\n",
        "        test_image_genre_labels.append(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDJp8ABikRjx",
        "outputId": "33b19c19-d0e8-4f4e-c8a3-c15b9075c3c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21869\n",
            "21869\n"
          ]
        }
      ],
      "source": [
        "print(len(test_image_file_paths))\n",
        "print(len(test_image_genre_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7opM21S8kpWR"
      },
      "source": [
        "Load Text Data From CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mwoBm1Qy6bBF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"./ferramenta_texts/text-train.csv\", index_col='image')\n",
        "df_test = pd.read_csv(\"./ferramenta_texts/text-val.csv\", index_col='image')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDgedj3smErT"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "X-k3Snt17bU0"
      },
      "outputs": [],
      "source": [
        "df['text'] = df[\"title\"] + df['description']\n",
        "df_test['text'] = df_test[\"title\"] + df_test['description']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "O5VIz-tlvHEn"
      },
      "outputs": [],
      "source": [
        "# Remove all the numeric data from text ! Why ? i dont know, feels right to do so.\n",
        "import re\n",
        "\n",
        "def remove_numbers(text):\n",
        "    if isinstance(text, str):\n",
        "        return re.sub(r'\\d+', '', text)\n",
        "    else:\n",
        "        return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "8yO3N43WvJE4"
      },
      "outputs": [],
      "source": [
        "df['text'] = df['text'].apply(remove_numbers)\n",
        "df_test['text'] = df_test['text'].apply(remove_numbers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "u2Vfol933Sj-"
      },
      "outputs": [],
      "source": [
        "df['text'].fillna(\"no text available\", inplace=True)\n",
        "df_test['text'].fillna(\"no text available\", inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "cwuBdJmq9R9c"
      },
      "outputs": [],
      "source": [
        "texts_csv_lst = df['text'].tolist()\n",
        "test_texts_csv_lst = df_test['text'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWEwpMeal_6R",
        "outputId": "19218282-51f9-48cd-9189-74a94044c1ba"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(66141, 21869)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "len(texts_csv_lst) , len(test_texts_csv_lst)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQJGR8BanX0C"
      },
      "outputs": [],
      "source": [
        "# texts_csv_lst[1000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Xugxf_HlIVa"
      },
      "source": [
        "Label Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "dg39H-jZlI1T",
        "outputId": "f4397d09-a121-4786-915e-11d589944ad8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LabelEncoder()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LabelEncoder</label><div class=\"sk-toggleable__content\"><pre>LabelEncoder()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(image_genre_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "J3xT1b8ptN6L"
      },
      "outputs": [],
      "source": [
        "transformed_genre_labels = label_encoder.transform(image_genre_labels)\n",
        "test_transformed_genre_labels = label_encoder.transform(test_image_genre_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "pdU2u2hZszxo"
      },
      "outputs": [],
      "source": [
        "transformed_genre_labels = transformed_genre_labels.tolist()\n",
        "test_transformed_genre_labels = test_transformed_genre_labels.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g65oz_CinqqM",
        "outputId": "17ac44e4-3c19-4a8d-e467-7f36783c3a20"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(66141, 21869)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "len(transformed_genre_labels), len(test_transformed_genre_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZASZmQcekz77"
      },
      "source": [
        "Create Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cAOi7Vlh4dvd"
      },
      "outputs": [],
      "source": [
        "# val_data = {\n",
        "#     'image': image_file_paths,\n",
        "#     'text': texts_csv_lst,\n",
        "#     'label': transformed_genre_labels,\n",
        "# }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TR7yXnxnmtwP"
      },
      "outputs": [],
      "source": [
        "image_file_paths[0], texts_csv_lst[0], transformed_genre_labels[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcgR0MQ4lKB7"
      },
      "outputs": [],
      "source": [
        "test_image_file_paths[0], test_texts_csv_lst[0], test_transformed_genre_labels[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Nl3jJ_Y3O44t"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from PIL import Image as pil\n",
        "\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, images, texts, labels, processor):\n",
        "        self.images = images\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.processor = processor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.processor(pil.open(image).convert(\"RGB\"), text,  max_length=32, add_special_tokens=True,  is_split_into_words=False, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "        for k,v in encoding.items():\n",
        "          encoding[k] = v.squeeze()\n",
        "\n",
        "        # encoding[\"labels\"] = label\n",
        "        encoding[\"labels\"] = torch.tensor(label)\n",
        "\n",
        "        return encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ROjHdLt2P545"
      },
      "outputs": [],
      "source": [
        "from transformers import ViltProcessor\n",
        "\n",
        "processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-nlvr2\")\n",
        "\n",
        "dataset = CustomDataset(images=image_file_paths,\n",
        "                     texts=texts_csv_lst,\n",
        "                     labels=transformed_genre_labels,\n",
        "                     processor=processor)\n",
        "\n",
        "test_dataset = CustomDataset(images=test_image_file_paths,\n",
        "                     texts=test_texts_csv_lst,\n",
        "                     labels=test_transformed_genre_labels,\n",
        "                     processor=processor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AroGwpiBO1nH"
      },
      "outputs": [],
      "source": [
        "dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7sqW62hlUfN"
      },
      "outputs": [],
      "source": [
        "test_dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrpTc8SKQYBx"
      },
      "source": [
        "Collate_fun and Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "NBFlOmB5QcDA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def collate_fn(batch):\n",
        "  return {\n",
        "      \"input_ids\": torch.stack([x[\"input_ids\"] for x in batch]),\n",
        "      \"pixel_values\": torch.stack([x[\"pixel_values\"] for x in batch]),\n",
        "      # \"attention_mask\": torch.stack([x[\"attention_mask\"] for x in batch]),\n",
        "      # \"token_type_ids\": torch.stack([x[\"token_type_ids\"] for x in batch]),\n",
        "      # \"pixel_mask\": torch.stack([x[\"pixel_mask\"] for x in batch]),\n",
        "      \"labels\": torch.stack([torch.tensor(x[\"labels\"]) for x in batch]),\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "R5nJBScMnPkR"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "train_dataloader = DataLoader(dataset, collate_fn=collate_fn, batch_size=12, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, collate_fn=collate_fn, batch_size=12, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jl4iHa6yRw6b"
      },
      "outputs": [],
      "source": [
        "# We have to check which one of these collate_fun will work for our case !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9C4gCxrbgjqh"
      },
      "outputs": [],
      "source": [
        "# from torch.utils.data import DataLoader\n",
        "\n",
        "# def collate_fn(batch):\n",
        "#   input_ids = [item['input_ids'] for item in batch]\n",
        "#   pixel_values = [item['pixel_values'] for item in batch]\n",
        "#   attention_mask = [item['attention_mask'] for item in batch]\n",
        "#   token_type_ids = [item['token_type_ids'] for item in batch]\n",
        "#   labels = [item['labels'] for item in batch]\n",
        "\n",
        "#   # create padded pixel values and corresponding pixel mask\n",
        "#   encoding = processor.image_processor.pad(pixel_values, return_tensors=\"pt\")\n",
        "\n",
        "#   # create new batch\n",
        "#   batch = {}\n",
        "#   batch['input_ids'] = torch.stack(input_ids)\n",
        "#   batch['attention_mask'] = torch.stack(attention_mask)\n",
        "#   batch['token_type_ids'] = torch.stack(token_type_ids)\n",
        "#   batch['pixel_values'] = encoding['pixel_values']\n",
        "#   batch['pixel_mask'] = encoding['pixel_mask']\n",
        "#   batch['labels'] = torch.stack(labels)\n",
        "\n",
        "#   return batch\n",
        "\n",
        "# train_dataloader = DataLoader(dataset, collate_fn=collate_fn, batch_size=4, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMwrFaC2DY59"
      },
      "source": [
        "Check Dimensions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dkCEPAsDb0w"
      },
      "outputs": [],
      "source": [
        "batch = next(iter(train_dataloader))\n",
        "test_batch = next(iter(test_dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0y42y2P6DbST"
      },
      "outputs": [],
      "source": [
        "for k,v in batch.items():\n",
        "  print(k, v.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpm-ZrV_lbln"
      },
      "outputs": [],
      "source": [
        "for k,v in test_batch.items():\n",
        "  print(k, v.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8LmlwZbFmn3"
      },
      "outputs": [],
      "source": [
        "# input_ids torch.Size([4, 40])\n",
        "# attention_mask torch.Size([4, 40])\n",
        "# token_type_ids torch.Size([4, 40])\n",
        "# pixel_values torch.Size([4, 3, 448, 576])\n",
        "# pixel_mask torch.Size([4, 448, 576])\n",
        "# labels torch.Size([4, 3129])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MM56RFQ8Ic4W"
      },
      "outputs": [],
      "source": [
        "# processor.decode(batch[\"input_ids\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJmPSLCEIhoQ"
      },
      "outputs": [],
      "source": [
        "# batch[\"pixel_values\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFgKeVAZpPYz"
      },
      "source": [
        "import Model and Processor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQQnZxGslekZ"
      },
      "outputs": [],
      "source": [
        "# label2id = {label: encoded_id for label, encoded_id in zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))}\n",
        "# id2label = {encoded_id: label for encoded_id, label in enumerate(label_encoder.classes_)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRD6FkWRlika"
      },
      "outputs": [],
      "source": [
        "# label2id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "jAhjQH3bmi6D"
      },
      "outputs": [],
      "source": [
        "valid_labels = label_encoder.classes_.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUDOVYRzms_A"
      },
      "outputs": [],
      "source": [
        "# valid_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3Dvf-GrpOux"
      },
      "outputs": [],
      "source": [
        "from transformers import ViltForImagesAndTextClassification\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = ViltForImagesAndTextClassification.from_pretrained(\"dandelin/vilt-b32-finetuned-nlvr2\",\n",
        "                                                                label2id = {label: str(i) for i, label in enumerate(valid_labels)},\n",
        "                                                                id2label = {str(i): label for i, label in enumerate(valid_labels)},\n",
        "                                                                num_images = 1,\n",
        "                                                                ignore_mismatched_sizes=True)\n",
        "\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5iL6pUhLfd-"
      },
      "outputs": [],
      "source": [
        "model.config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4dJNlYITN1l"
      },
      "source": [
        "Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "VuZ69KYTju-K"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJu2uVb2-UHu"
      },
      "outputs": [],
      "source": [
        "checkpoint_dir = \"/content/Model/Models-Train-35/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7R63NqnDRkmA"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(50):\n",
        "   print(f\"Epoch: {epoch}\")\n",
        "   train_loss = 0\n",
        "   for batch in tqdm(train_dataloader):\n",
        "        # get the inputs;\n",
        "        batch = {k:v.to(device) for k,v in batch.items()}\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        # print(\"Loss:\", loss.item())\n",
        "        train_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "   print(\"Training Loss: \", train_loss)\n",
        "   print(f\"Saving Model at epoch: {epoch}\")\n",
        "\n",
        "   processor.save_pretrained(checkpoint_dir)\n",
        "   model.save_pretrained(checkpoint_dir)\n",
        "\n",
        "   model.eval()\n",
        "   preds = []\n",
        "   orignal = []\n",
        "   for batch in tqdm(test_dataloader):\n",
        "       batch = {k:v.to(device) for k,v in batch.items()}\n",
        "\n",
        "       outputs = model(**batch)\n",
        "\n",
        "       scores = outputs.logits\n",
        "       preds_current = torch.argmax(scores, dim=1)\n",
        "\n",
        "       preds += preds_current.cpu().numpy().tolist()\n",
        "       orignal += batch[\"labels\"].detach().cpu().numpy().tolist()\n",
        "\n",
        "   accuracy = accuracy_score(orignal, preds)\n",
        "   precision = precision_score(orignal, preds, average='macro')\n",
        "   recall = recall_score(orignal, preds, average='macro')\n",
        "   f1 = f1_score(orignal, preds, average='macro')\n",
        "\n",
        "   print(f\"Accuracy: {accuracy}\")\n",
        "   print(f\"Precision: {precision}\")\n",
        "   print(f\"Recall: {recall}\")\n",
        "   print(f\"F1-Score: {f1}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8NTxeFKU-lj"
      },
      "outputs": [],
      "source": [
        "# Based on which collate_fun we can use, we will decide which training loop can be used !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6wg2b-SUd_a"
      },
      "outputs": [],
      "source": [
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# model.train()\n",
        "# for epoch in range(50):  # loop over the dataset multiple times\n",
        "#    print(f\"Epoch: {epoch}\")\n",
        "#    for batch in train_dataloader:\n",
        "#         # get the inputs;\n",
        "#         pixel_values = batch[\"pixel_values\"].to(device)\n",
        "#         input_ids = batch[\"input_ids\"].to(device)\n",
        "#         label_ids = batch[\"labels\"].to(device)\n",
        "\n",
        "#         # batch = {k:v.to(device) for k,v in batch.items()}\n",
        "\n",
        "#         # zero the parameter gradients\n",
        "#         optimizer.zero_grad()\n",
        "\n",
        "#         # forward + backward + optimize\n",
        "#         outputs = model(input_ids=input_ids, pixel_values=pixel_values.unsqueeze(0), labels=label_ids)\n",
        "#         # outputs = model(**batch)\n",
        "#         loss = outputs.loss\n",
        "#         print(\"Loss:\", loss.item())\n",
        "#         loss.backward()\n",
        "#         optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5O2rU7UsWcQF"
      },
      "outputs": [],
      "source": [
        "# We also need to save the best performing model\n",
        "# and for that we need to validate model after each epoch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ViltForImagesAndTextClassification\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = ViltForImagesAndTextClassification.from_pretrained(\"/content/Model/Models-Train-35/\")\n",
        "\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9t0kwFjH1M-",
        "outputId": "957bf064-6259-4f2b-adba-332e2c86a216"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ViltForImagesAndTextClassification(\n",
              "  (vilt): ViltModel(\n",
              "    (embeddings): ViltEmbeddings(\n",
              "      (text_embeddings): TextEmbeddings(\n",
              "        (word_embeddings): Embedding(30522, 768)\n",
              "        (position_embeddings): Embedding(40, 768)\n",
              "        (token_type_embeddings): Embedding(2, 768)\n",
              "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (patch_embeddings): ViltPatchEmbeddings(\n",
              "        (projection): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n",
              "      )\n",
              "      (token_type_embeddings): Embedding(3, 768)\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (encoder): ViltEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x ViltLayer(\n",
              "          (attention): ViltAttention(\n",
              "            (attention): ViltSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (output): ViltSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ViltIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): ViltOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (pooler): ViltPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (2): GELU(approximate='none')\n",
              "    (3): Linear(in_features=768, out_features=52, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "MEiTKG5CQV4t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105,
          "referenced_widgets": [
            "88777a58d7a4456ba142e049774294ee",
            "3e0c0999e1cb49c8a124f569afd62b9f",
            "7060f567ea0f4dcb9160a6a5bafa3d17",
            "8c367087738648eeae1e8f316793ce15",
            "76c8a9525234470d95925794b0215cd0",
            "3f4adfe088374a16ae4d7f0bb3c208b9",
            "438d2f7adb2d4fff9c05745c8c5c31c5",
            "bd208706ce244fd18e6cc8023509808e",
            "0dd8e79242e046ec8568dddb4f653629",
            "d587476b2b2f41b59f4150718a6130cd",
            "e4e993052ec94c9f85faaceed482363b"
          ]
        },
        "outputId": "19d2121f-a631-4955-e744-493868bef47a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1823 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "88777a58d7a4456ba142e049774294ee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-6185ed4ca984>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \"labels\": torch.stack([torch.tensor(x[\"labels\"]) for x in batch]),\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "# correct, total, all_true = 0, 0, 0\n",
        "preds = []\n",
        "orignal = []\n",
        "\n",
        "for batch in tqdm(test_dataloader):\n",
        "    batch = {k:v.to(device) for k,v in batch.items()}\n",
        "    # pixel_values = batch[\"pixel_values\"].to(device)\n",
        "    # input_ids = batch[\"input_ids\"].to(device)\n",
        "    # label_ids = batch[\"labels\"].to(device)\n",
        "\n",
        "    # outputs = model(input_ids=input_ids, pixel_values=pixel_values.unsqueeze(0))\n",
        "    outputs = model(**batch)\n",
        "    # logits = outputs.logits\n",
        "\n",
        "    scores = outputs.logits\n",
        "    preds_current = torch.argmax(scores, dim=1)\n",
        "    # print(preds_current, batch[\"labels\"])\n",
        "    # correct += sum( batch[\"labels\"] == preds_current)\n",
        "    preds += preds_current.cpu().numpy().tolist()\n",
        "    orignal += batch[\"labels\"].detach().cpu().numpy().tolist()\n",
        "    # total+=batch[\"pixel_values\"].shape[0]\n",
        "    # all_true += sum(batch[\"labels\"])\n",
        "\n",
        "\n",
        "    # loss = outputs.loss\n",
        "    # print(logits)\n",
        "    # idx = np.argmax(logits.detach).item()\n",
        "    # print(\"Predicted answer:\", model.config.id2label[idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "le97wBG-_yAX"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "accuracy = accuracy_score(orignal, preds)\n",
        "precision = precision_score(orignal, preds, average='macro')\n",
        "recall = recall_score(orignal, preds, average='macro')\n",
        "f1 = f1_score(orignal, preds, average='macro')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "bW6fK5X4R21O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d09f70ef-f59f-4e02-be55-af1b064a04b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9590744890026979\n",
            "Precision: 0.935976297050587\n",
            "Recall: 0.9399313059455687\n",
            "F1-Score: 0.9367510465858238\n"
          ]
        }
      ],
      "source": [
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-Score: {f1}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "88777a58d7a4456ba142e049774294ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3e0c0999e1cb49c8a124f569afd62b9f",
              "IPY_MODEL_7060f567ea0f4dcb9160a6a5bafa3d17",
              "IPY_MODEL_8c367087738648eeae1e8f316793ce15"
            ],
            "layout": "IPY_MODEL_76c8a9525234470d95925794b0215cd0"
          }
        },
        "3e0c0999e1cb49c8a124f569afd62b9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f4adfe088374a16ae4d7f0bb3c208b9",
            "placeholder": "​",
            "style": "IPY_MODEL_438d2f7adb2d4fff9c05745c8c5c31c5",
            "value": "100%"
          }
        },
        "7060f567ea0f4dcb9160a6a5bafa3d17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd208706ce244fd18e6cc8023509808e",
            "max": 1823,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0dd8e79242e046ec8568dddb4f653629",
            "value": 1823
          }
        },
        "8c367087738648eeae1e8f316793ce15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d587476b2b2f41b59f4150718a6130cd",
            "placeholder": "​",
            "style": "IPY_MODEL_e4e993052ec94c9f85faaceed482363b",
            "value": " 1823/1823 [09:03&lt;00:00,  4.12it/s]"
          }
        },
        "76c8a9525234470d95925794b0215cd0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f4adfe088374a16ae4d7f0bb3c208b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "438d2f7adb2d4fff9c05745c8c5c31c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bd208706ce244fd18e6cc8023509808e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0dd8e79242e046ec8568dddb4f653629": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d587476b2b2f41b59f4150718a6130cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4e993052ec94c9f85faaceed482363b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}