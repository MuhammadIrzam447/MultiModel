{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MuhammadIrzam447/MultiModel/blob/master/Train_11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quQ4Wf-5rN4j"
      },
      "outputs": [],
      "source": [
        "# google/vit-base-patch16-224 on fused train+test_unseen Hateful Memes dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9F-pNaIFdRHi",
        "outputId": "281877dd-6ff6-4e11-ab53-932981dae052"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1wgl3QGXZ4m2aLg3T-1TDXQqSP31RuXgL\n",
            "To: /content/hateful_train+test_unseen.zip\n",
            "100% 3.27G/3.27G [00:29<00:00, 109MB/s]\n"
          ]
        }
      ],
      "source": [
        "# !gdown https://drive.google.com/uc?id=1wgl3QGXZ4m2aLg3T-1TDXQqSP31RuXgL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nvujlUCYdi0L"
      },
      "outputs": [],
      "source": [
        "# !unzip \"/content/hateful_train+test_unseen.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yV2GxQWZw4L"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKSy44hXZx8a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.transforms import transforms\n",
        "from transformers import ViTForImageClassification, ViTFeatureExtractor, AdamW\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix, roc_auc_score\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCC0cmnIZzTH"
      },
      "outputs": [],
      "source": [
        "# Define the path to your training and validation data\n",
        "train_data_root = \"/content/hateful_ViT1/train\"\n",
        "val_data_root = \"/content/hateful_ViT1/test\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "02306081bd8545358326896a3a572058",
            "ad40aaf3ab034d9fa2646677bba33e55",
            "3c19e5f502f84f8f8b587f0706b68709",
            "fea61fd8f8f54e0980fddd2cfbe20b73",
            "e167b821a87b48b1b3582ee668c700e7",
            "663646ba23f6498982b11a93b3531e46",
            "e3214b8307ea43418360afe55ea4a459",
            "b0e7faac8b034f2f9bbf62b092986346",
            "93a6248fe7134107a872fa2974ee32a5",
            "4eb3b05cdb4d4e81bf43f759a1b5c471",
            "0d4619f48cf5416697dfcf66ec40d9a2"
          ]
        },
        "id": "VcVrx7RkZzdV",
        "outputId": "81bf41e1-4b52-4237-8915-2dfbf9ee88f8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "02306081bd8545358326896a3a572058",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (â€¦)rocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import ViTImageProcessor\n",
        "\n",
        "processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
        "image_mean, image_std = processor.image_mean, processor.image_std\n",
        "size = processor.size[\"height\"]\n",
        "\n",
        "# Define transformations for the input images\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=image_mean, std=image_std)\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ysx46DEHZznC"
      },
      "outputs": [],
      "source": [
        "# Load the dataset using ImageFolder and apply transformations\n",
        "train_dataset = ImageFolder(train_data_root, transform=transform)\n",
        "val_dataset = ImageFolder(val_data_root, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0bNIiTMZzwL"
      },
      "outputs": [],
      "source": [
        "# Create label2id and id2label dictionaries based on the class names in the dataset\n",
        "label2id = {class_name: idx for class_name, idx in train_dataset.class_to_idx.items()}\n",
        "id2label = {idx: class_name for class_name, idx in train_dataset.class_to_idx.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9zA4YBKZ0SA"
      },
      "outputs": [],
      "source": [
        "# Initialize the feature extractor\n",
        "# feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224\")\n",
        "\n",
        "# Define batch size and number of workers (adjust based on your system's resources)\n",
        "batch_size = 32\n",
        "num_workers = 1\n",
        "\n",
        "# Create DataLoader for the dataset\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3y3_g9PHZ0by",
        "outputId": "e056e75a-74bc-45af-e298-6115b2998aac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2\n"
          ]
        }
      ],
      "source": [
        "num_classes = len(train_dataset.classes)\n",
        "print(num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5Yqjhf2Z0kl",
        "outputId": "3ad73bbc-2577-407a-d421-68bc679ffaaa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
            "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
            "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ViTForImageClassification(\n",
            "  (vit): ViTModel(\n",
            "    (embeddings): ViTEmbeddings(\n",
            "      (patch_embeddings): ViTPatchEmbeddings(\n",
            "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
            "      )\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (encoder): ViTEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0-11): 12 x ViTLayer(\n",
            "          (attention): ViTAttention(\n",
            "            (attention): ViTSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (output): ViTSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): ViTIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): ViTOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "  )\n",
            "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "vit = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224\", id2label=id2label, label2id=label2id, ignore_mismatched_sizes=True)\n",
        "vit.classifier = nn.Linear(vit.config.hidden_size, num_classes)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "vit.to(device)\n",
        "print(vit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdqAwRBSaEGC",
        "outputId": "c030037d-0359-4442-df8b-49f1a960067b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = AdamW(vit.parameters(), lr=1e-5)\n",
        "# optimizer = optim.SGD(vit.parameters(), lr=0.001, momentum=0.9)\n",
        "num_epochs = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVhg3NjSaGwr",
        "outputId": "82e9fc2a-a909-42de-bab4-29d75631d74d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20 - Training Loss: 0.5937 - Validation Loss: 0.6370 - Accuracy: 0.6585\n",
            "Accuracy: 0.6585\n",
            "Precision: 0.6445506128258667\n",
            "Recall: 0.6585\n",
            "F1-score: 0.6154596422229518\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.90      0.77      1250\n",
            "           1       0.60      0.26      0.36       750\n",
            "\n",
            "    accuracy                           0.66      2000\n",
            "   macro avg       0.64      0.58      0.57      2000\n",
            "weighted avg       0.64      0.66      0.62      2000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1122  128]\n",
            " [ 555  195]]\n",
            "AUROC: 0.5788\n",
            "Epoch 2/20 - Training Loss: 0.4393 - Validation Loss: 0.6195 - Accuracy: 0.6860\n",
            "Accuracy: 0.686\n",
            "Precision: 0.6766658481398979\n",
            "Recall: 0.686\n",
            "F1-score: 0.6621873069241491\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.87      0.78      1250\n",
            "           1       0.64      0.37      0.47       750\n",
            "\n",
            "    accuracy                           0.69      2000\n",
            "   macro avg       0.67      0.62      0.62      2000\n",
            "weighted avg       0.68      0.69      0.66      2000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1092  158]\n",
            " [ 470  280]]\n",
            "AUROC: 0.6234666666666666\n",
            "Epoch 3/20 - Training Loss: 0.2743 - Validation Loss: 0.6104 - Accuracy: 0.7145\n",
            "Accuracy: 0.7145\n",
            "Precision: 0.7094118723775731\n",
            "Recall: 0.7145\n",
            "F1-score: 0.7106188188174506\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.81      0.78      1250\n",
            "           1       0.63      0.56      0.60       750\n",
            "\n",
            "    accuracy                           0.71      2000\n",
            "   macro avg       0.69      0.68      0.69      2000\n",
            "weighted avg       0.71      0.71      0.71      2000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1007  243]\n",
            " [ 328  422]]\n",
            "AUROC: 0.6841333333333333\n",
            "Epoch 4/20 - Training Loss: 0.1364 - Validation Loss: 0.6604 - Accuracy: 0.7440\n",
            "Accuracy: 0.744\n",
            "Precision: 0.7395300467859679\n",
            "Recall: 0.744\n",
            "F1-score: 0.7398811112245897\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.83      0.80      1250\n",
            "           1       0.68      0.59      0.63       750\n",
            "\n",
            "    accuracy                           0.74      2000\n",
            "   macro avg       0.73      0.71      0.72      2000\n",
            "weighted avg       0.74      0.74      0.74      2000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1043  207]\n",
            " [ 305  445]]\n",
            "AUROC: 0.7138666666666666\n",
            "Epoch 5/20 - Training Loss: 0.0642 - Validation Loss: 0.8185 - Accuracy: 0.7390\n",
            "Accuracy: 0.739\n",
            "Precision: 0.7342877492877493\n",
            "Recall: 0.739\n",
            "F1-score: 0.7346978021978022\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.83      0.80      1250\n",
            "           1       0.68      0.59      0.63       750\n",
            "\n",
            "    accuracy                           0.74      2000\n",
            "   macro avg       0.72      0.71      0.71      2000\n",
            "weighted avg       0.73      0.74      0.73      2000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1039  211]\n",
            " [ 311  439]]\n",
            "AUROC: 0.7082666666666666\n",
            "Epoch 6/20 - Training Loss: 0.0386 - Validation Loss: 0.8751 - Accuracy: 0.7305\n",
            "Accuracy: 0.7305\n",
            "Precision: 0.7324259381171824\n",
            "Recall: 0.7305\n",
            "F1-score: 0.7313479052823314\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.77      0.78      1250\n",
            "           1       0.64      0.66      0.65       750\n",
            "\n",
            "    accuracy                           0.73      2000\n",
            "   macro avg       0.71      0.72      0.71      2000\n",
            "weighted avg       0.73      0.73      0.73      2000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[968 282]\n",
            " [257 493]]\n",
            "AUROC: 0.7158666666666667\n",
            "Epoch 7/20 - Training Loss: 0.0256 - Validation Loss: 1.0652 - Accuracy: 0.7655\n",
            "Accuracy: 0.7655\n",
            "Precision: 0.764172908828658\n",
            "Recall: 0.7655\n",
            "F1-score: 0.7560827697893091\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.89      0.83      1250\n",
            "           1       0.76      0.55      0.64       750\n",
            "\n",
            "    accuracy                           0.77      2000\n",
            "   macro avg       0.76      0.72      0.73      2000\n",
            "weighted avg       0.76      0.77      0.76      2000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1116  134]\n",
            " [ 335  415]]\n",
            "AUROC: 0.7230666666666667\n",
            "Epoch 8/20 - Training Loss: 0.0224 - Validation Loss: 1.0676 - Accuracy: 0.7535\n",
            "Accuracy: 0.7535\n",
            "Precision: 0.7493592810789796\n",
            "Recall: 0.7535\n",
            "F1-score: 0.7464918544655172\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.86      0.81      1250\n",
            "           1       0.72      0.57      0.63       750\n",
            "\n",
            "    accuracy                           0.75      2000\n",
            "   macro avg       0.74      0.72      0.72      2000\n",
            "weighted avg       0.75      0.75      0.75      2000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1081  169]\n",
            " [ 324  426]]\n",
            "AUROC: 0.7164\n",
            "Epoch 9/20 - Training Loss: 0.0201 - Validation Loss: 1.0878 - Accuracy: 0.7560\n",
            "Accuracy: 0.756\n",
            "Precision: 0.751788218793829\n",
            "Recall: 0.756\n",
            "F1-score: 0.750453775915184\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.86      0.81      1250\n",
            "           1       0.71      0.59      0.64       750\n",
            "\n",
            "    accuracy                           0.76      2000\n",
            "   macro avg       0.74      0.72      0.73      2000\n",
            "weighted avg       0.75      0.76      0.75      2000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1071  179]\n",
            " [ 309  441]]\n",
            "AUROC: 0.7223999999999999\n",
            "Epoch 10/20 - Training Loss: 0.0141 - Validation Loss: 1.1110 - Accuracy: 0.7400\n",
            "Accuracy: 0.74\n",
            "Precision: 0.7368143483192183\n",
            "Recall: 0.74\n",
            "F1-score: 0.7378066490664517\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.81      0.80      1250\n",
            "           1       0.67      0.62      0.64       750\n",
            "\n",
            "    accuracy                           0.74      2000\n",
            "   macro avg       0.72      0.72      0.72      2000\n",
            "weighted avg       0.74      0.74      0.74      2000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1018  232]\n",
            " [ 288  462]]\n",
            "AUROC: 0.7152000000000001\n",
            "Epoch 11/20 - Training Loss: 0.0174 - Validation Loss: 1.1403 - Accuracy: 0.7645\n",
            "Accuracy: 0.7645\n",
            "Precision: 0.7614025458416674\n",
            "Recall: 0.7645\n",
            "F1-score: 0.757351918910941\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.88      0.82      1250\n",
            "           1       0.74      0.58      0.65       750\n",
            "\n",
            "    accuracy                           0.76      2000\n",
            "   macro avg       0.76      0.73      0.74      2000\n",
            "weighted avg       0.76      0.76      0.76      2000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1096  154]\n",
            " [ 317  433]]\n",
            "AUROC: 0.7270666666666666\n",
            "Epoch 12/20 - Training Loss: 0.0138 - Validation Loss: 1.1681 - Accuracy: 0.7415\n",
            "Accuracy: 0.7415\n",
            "Precision: 0.7402703260643418\n",
            "Recall: 0.7415\n",
            "F1-score: 0.7408167206743937\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.80      0.79      1250\n",
            "           1       0.66      0.64      0.65       750\n",
            "\n",
            "    accuracy                           0.74      2000\n",
            "   macro avg       0.72      0.72      0.72      2000\n",
            "weighted avg       0.74      0.74      0.74      2000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1001  249]\n",
            " [ 268  482]]\n",
            "AUROC: 0.7217333333333333\n",
            "Epoch 13/20 - Training Loss: 0.0101 - Validation Loss: 1.2188 - Accuracy: 0.7495\n",
            "Accuracy: 0.7495\n",
            "Precision: 0.7470588830510495\n",
            "Recall: 0.7495\n",
            "F1-score: 0.7479214072516883\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.82      0.80      1250\n",
            "           1       0.68      0.64      0.66       750\n",
            "\n",
            "    accuracy                           0.75      2000\n",
            "   macro avg       0.73      0.73      0.73      2000\n",
            "weighted avg       0.75      0.75      0.75      2000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1021  229]\n",
            " [ 272  478]]\n",
            "AUROC: 0.7270666666666666\n",
            "Epoch 14/20 - Training Loss: 0.0148 - Validation Loss: 1.1864 - Accuracy: 0.7310\n",
            "Accuracy: 0.731\n",
            "Precision: 0.7314378859796866\n",
            "Recall: 0.731\n",
            "F1-score: 0.731212279989904\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.78      0.78      1250\n",
            "           1       0.64      0.65      0.64       750\n",
            "\n",
            "    accuracy                           0.73      2000\n",
            "   macro avg       0.71      0.71      0.71      2000\n",
            "weighted avg       0.73      0.73      0.73      2000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[978 272]\n",
            " [266 484]]\n",
            "AUROC: 0.7138666666666666\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    vit.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = vit(images).logits\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * images.size(0)\n",
        "\n",
        "    # Calculate average loss for this epoch\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "\n",
        "    save_dir = \"/content/Model/Models-Train-11/\"\n",
        "    os.makedirs(save_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
        "\n",
        "    model_name = str(epoch+1) + \"_model.pth\"\n",
        "    save_path = os.path.join(save_dir, model_name)  # Specify the complete path to the model file\n",
        "    torch.save(vit.state_dict(), save_path)\n",
        "\n",
        "    # Validation\n",
        "    vit.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    predicted_classes = []\n",
        "    actual_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = vit(images).logits\n",
        "            val_loss += criterion(outputs, labels).item() * images.size(0)\n",
        "\n",
        "            probabilities = torch.softmax(outputs, dim=1)\n",
        "            predicted = torch.argmax(probabilities, dim=1)\n",
        "\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            predicted_classes.extend(predicted.cpu().numpy())\n",
        "            actual_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate average loss and accuracy for validation set\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "    accuracy = correct / len(val_loader.dataset)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Training Loss: {train_loss:.4f} - Validation Loss: {val_loss:.4f} - Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    # Compute evaluation metrics using the predicted_classes and actual_labels lists\n",
        "    accuracy = accuracy_score(actual_labels, predicted_classes)\n",
        "    precision = precision_score(actual_labels, predicted_classes, average='weighted')\n",
        "    recall = recall_score(actual_labels, predicted_classes, average='weighted')\n",
        "    f1 = f1_score(actual_labels, predicted_classes, average='weighted')\n",
        "\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1-score:\", f1)\n",
        "    print(classification_report(actual_labels, predicted_classes))\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(actual_labels, predicted_classes))\n",
        "    print(\"AUROC:\", roc_auc_score(actual_labels, predicted_classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dPB8YABeK7K"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "02306081bd8545358326896a3a572058": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ad40aaf3ab034d9fa2646677bba33e55",
              "IPY_MODEL_3c19e5f502f84f8f8b587f0706b68709",
              "IPY_MODEL_fea61fd8f8f54e0980fddd2cfbe20b73"
            ],
            "layout": "IPY_MODEL_e167b821a87b48b1b3582ee668c700e7"
          }
        },
        "0d4619f48cf5416697dfcf66ec40d9a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3c19e5f502f84f8f8b587f0706b68709": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b0e7faac8b034f2f9bbf62b092986346",
            "max": 160,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_93a6248fe7134107a872fa2974ee32a5",
            "value": 160
          }
        },
        "4eb3b05cdb4d4e81bf43f759a1b5c471": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "663646ba23f6498982b11a93b3531e46": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93a6248fe7134107a872fa2974ee32a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ad40aaf3ab034d9fa2646677bba33e55": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_663646ba23f6498982b11a93b3531e46",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e3214b8307ea43418360afe55ea4a459",
            "value": "Downloading (â€¦)rocessor_config.json: 100%"
          }
        },
        "b0e7faac8b034f2f9bbf62b092986346": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e167b821a87b48b1b3582ee668c700e7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3214b8307ea43418360afe55ea4a459": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fea61fd8f8f54e0980fddd2cfbe20b73": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4eb3b05cdb4d4e81bf43f759a1b5c471",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_0d4619f48cf5416697dfcf66ec40d9a2",
            "value": " 160/160 [00:00&lt;00:00, 9.38kB/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}