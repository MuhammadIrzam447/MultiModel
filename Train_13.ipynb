{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MuhammadIrzam447/MultiModel/blob/master/Train_13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !gdown https://drive.google.com/uc?id=1wgl3QGXZ4m2aLg3T-1TDXQqSP31RuXgL"
      ],
      "metadata": {
        "id": "aPyxZLsHY9Rv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip /content/hateful_train+test_unseen.zip"
      ],
      "metadata": {
        "id": "BOxHTKNxZGD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LhpxluXAy0g"
      },
      "outputs": [],
      "source": [
        "!pip install transformers evaluate datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBq4mm7cA4Cp"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import *\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlLN5LTVA5eJ"
      },
      "outputs": [],
      "source": [
        "# the model name\n",
        "model_name = \"google/vit-base-patch16-224\"\n",
        "# load the image processor\n",
        "image_processor = ViTImageProcessor.from_pretrained(model_name)\n",
        "# loading the pre-trained model\n",
        "model = ViTForImageClassification.from_pretrained(model_name).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5z-Sqz4BiPQ"
      },
      "outputs": [],
      "source": [
        "import urllib.parse as parse\n",
        "import os\n",
        "\n",
        "# a function to determine whether a string is a URL or not\n",
        "def is_url(string):\n",
        "    try:\n",
        "        result = parse.urlparse(string)\n",
        "        return all([result.scheme, result.netloc, result.path])\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "# a function to load an image\n",
        "def load_image(image_path):\n",
        "    if is_url(image_path):\n",
        "        return Image.open(requests.get(image_path, stream=True).raw)\n",
        "    elif os.path.exists(image_path):\n",
        "        return Image.open(image_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgfKn08vByse"
      },
      "outputs": [],
      "source": [
        "def get_prediction(model, url_or_path):\n",
        "  # load the image\n",
        "  img = load_image(url_or_path)\n",
        "  # preprocessing the image\n",
        "  pixel_values = image_processor(img, return_tensors=\"pt\")[\"pixel_values\"].to(device)\n",
        "  # perform inference\n",
        "  output = model(pixel_values)\n",
        "  # get the label id and return the class name\n",
        "  return model.config.id2label[int(output.logits.softmax(dim=1).argmax())]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8n-To7RsyPx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "e2fc768d-8bc9-493a-8173-19793f1b3ab2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Indian elephant, Elephas maximus'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "get_prediction(model, \"http://images.cocodataset.org/test-stuff2017/000000000128.jpg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading our Dataset"
      ],
      "metadata": {
        "id": "suO4z5NCDU8v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlxPT1XaD9VV"
      },
      "outputs": [],
      "source": [
        "# from datasets import load_dataset\n",
        "\n",
        "# # download & load the dataset\n",
        "# ds = load_dataset(\"food101\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading a Custom Dataset using `ImageFolder`\n",
        "Run the three below cells to load a custom dataset (that's not in the Hub) using `ImageFolder`"
      ],
      "metadata": {
        "id": "H9ZcQf_HDXl6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kF0He2BQcBSG"
      },
      "outputs": [],
      "source": [
        "# import requests\n",
        "# from tqdm import tqdm\n",
        "\n",
        "# def get_file(url):\n",
        "#   response = requests.get(url, stream=True)\n",
        "#   total_size = int(response.headers.get('content-length', 0))\n",
        "#   filename = None\n",
        "#   content_disposition = response.headers.get('content-disposition')\n",
        "#   if content_disposition:\n",
        "#       parts = content_disposition.split(';')\n",
        "#       for part in parts:\n",
        "#           if 'filename' in part:\n",
        "#               filename = part.split('=')[1].strip('\"')\n",
        "#   if not filename:\n",
        "#       filename = os.path.basename(url)\n",
        "#   block_size = 1024 # 1 Kibibyte\n",
        "#   tqdm_bar = tqdm(total=total_size, unit='iB', unit_scale=True)\n",
        "#   with open(filename, 'wb') as file:\n",
        "#       for data in response.iter_content(block_size):\n",
        "#           tqdm_bar.update(len(data))\n",
        "#           file.write(data)\n",
        "#   tqdm_bar.close()\n",
        "#   print(f\"Downloaded {filename} ({total_size} bytes)\")\n",
        "#   return filename"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HREIgilFbW_S"
      },
      "outputs": [],
      "source": [
        "# import zipfile\n",
        "# import os\n",
        "\n",
        "# def download_and_extract_dataset():\n",
        "#   # dataset from https://github.com/udacity/dermatologist-ai\n",
        "#   # 5.3GB\n",
        "#   train_url = \"https://s3-us-west-1.amazonaws.com/udacity-dlnfd/datasets/skin-cancer/train.zip\"\n",
        "#   # 824.5MB\n",
        "#   valid_url = \"https://s3-us-west-1.amazonaws.com/udacity-dlnfd/datasets/skin-cancer/valid.zip\"\n",
        "#   # 5.1GB\n",
        "#   test_url  = \"https://s3-us-west-1.amazonaws.com/udacity-dlnfd/datasets/skin-cancer/test.zip\"\n",
        "#   for i, download_link in enumerate([valid_url, train_url, test_url]):\n",
        "#     data_dir = get_file(download_link)\n",
        "#     print(\"Extracting\", download_link)\n",
        "#     with zipfile.ZipFile(data_dir, \"r\") as z:\n",
        "#       z.extractall(\"data\")\n",
        "#     # remove the temp file\n",
        "#     os.remove(data_dir)\n",
        "\n",
        "# # comment the below line if you already downloaded the dataset\n",
        "# download_and_extract_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# load the custom dataset\n",
        "ds = load_dataset(\"imagefolder\", data_dir=\"/content/hateful_ViT1\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "b9d3d38f10924ea5ac34d47264fb4787",
            "e47b70adf5964deba34f2c10f5fac547",
            "debc703989234afe80a312dab36aabff",
            "bbd997caba7746c8be484e43dfce79d8",
            "87c1ce909d8444b9aba6dfd003ec82ed",
            "c9614a15f0e042d5a46ca2b711c3bcd8",
            "1376ebeabd0647b09945e0fdf77d0437",
            "65350b81e5444cd2a97e78322b4d37ab",
            "b824f0b92d9a452ca15e6a0af7248c52",
            "de24d84e90c3481e8797bbc02aac73f7",
            "4fc7a74c320e41d895de86e37b4646de",
            "8217d438c6804e79906a3769b500b1bc",
            "997bba30ae984a10a8301e7e6cbca7f5",
            "c3e0e82030eb4e14beeffbb896dade8e",
            "10ea7cc2bfed41818c432d39f8ce895e",
            "b7866ebd5dce4b3fbb8c0e8b2f99cf1c",
            "f23538dbbdd149f79c6026bd2248258d",
            "2845c5d7cc504819b7c65611ea97c062",
            "326526eec98143f798ff97623120d37d",
            "4debe888989e4ee183da149e1e683ce7",
            "441799ffcab14e4ca3316ffd4c7c8a51",
            "17700eeaf5d647d582cbef9c81bda2f9"
          ],
          "height": 81
        },
        "id": "LjzUza9MwBgz",
        "outputId": "c8283af2-2a61-4a80-b080-3ec2754cb417"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Resolving data files:   0%|          | 0/26500 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b9d3d38f10924ea5ac34d47264fb4787"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Resolving data files:   0%|          | 0/2000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8217d438c6804e79906a3769b500b1bc"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Va4VIcoSbc6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 100% Missing Modality Dataset"
      ],
      "metadata": {
        "id": "aty6TXJPbdzG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"imagefolder\", data_dir=\"/content/hateful_ViT1\")"
      ],
      "metadata": {
        "id": "Ttyr-6hobHuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filename_ends_with_3(example):\n",
        "    return example[\"file_name\"].endswith(\"_3.png\")"
      ],
      "metadata": {
        "id": "bUJsCMxObZbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = ds.filter(filename_ends_with_3)\n",
        "print(ds[\"train\"][0:5])"
      ],
      "metadata": {
        "id": "XJY59IgnbYB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring the Data"
      ],
      "metadata": {
        "id": "QVjb_p9kDr5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dX_ABF25z-_w",
        "outputId": "8faed99d-77a0-4ab0-d857-0f29bc4ef1bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['image', 'label'],\n",
              "        num_rows: 26500\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['image', 'label'],\n",
              "        num_rows: 2000\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgxqXVVrITqO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f720bf4-9fa1-4221-a773-6c66a8ad3dea"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ClassLabel(names=['0', '1'], id=None)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "labels = ds[\"train\"].features[\"label\"]\n",
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozNlqVN3IgnR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "9b89f7ad-cd7f-4413-d0af-1710366f03e3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "labels.int2str(ds[\"train\"][532][\"label\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show_image_grid(dataset, split, grid_size=(4,4)):\n",
        "    # Select random images from the given split\n",
        "    indices = random.sample(range(len(dataset[split])), grid_size[0]*grid_size[1])\n",
        "    images = [dataset[split][i][\"image\"] for i in indices]\n",
        "    labels = [dataset[split][i][\"label\"] for i in indices]\n",
        "\n",
        "    # Display the images in a grid\n",
        "    fig, axes = plt.subplots(nrows=grid_size[0], ncols=grid_size[1], figsize=(8,8))\n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        ax.imshow(images[i])\n",
        "        ax.axis('off')\n",
        "        ax.set_title(ds[\"train\"].features[\"label\"].int2str(labels[i]))\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "FEV3yejjE7GV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show_image_grid(ds, \"train\")"
      ],
      "metadata": {
        "id": "YwU_YpNYE8af"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing the Data"
      ],
      "metadata": {
        "id": "W8jdhJCQKCOA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9x5QuhV8IwXB"
      },
      "outputs": [],
      "source": [
        "def transform(examples):\n",
        "  # convert all images to RGB format, then preprocessing it\n",
        "  # using our image processor\n",
        "  inputs = image_processor([img.convert(\"RGB\") for img in examples[\"image\"]], return_tensors=\"pt\")\n",
        "  # we also shouldn't forget about the labels\n",
        "  inputs[\"labels\"] = examples[\"label\"]\n",
        "  return inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vY7oXLDZJBaX"
      },
      "outputs": [],
      "source": [
        "# use the with_transform() method to apply the transform to the dataset on the fly during training\n",
        "dataset = ds.with_transform(transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbWrF63YQdsE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "071f1486-95ea-4411-8fd3-828c5f500367"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 224, 224])\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "for item in dataset[\"train\"]:\n",
        "  print(item[\"pixel_values\"].shape)\n",
        "  print(item[\"labels\"])\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnrUwcqwKTOV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25dfa1a5-be4f-445b-b415-119129490b6c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['0', '1']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# extract the labels for our dataset\n",
        "labels = ds[\"train\"].features[\"label\"].names\n",
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUukexdrJGob"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def collate_fn(batch):\n",
        "  return {\n",
        "      \"pixel_values\": torch.stack([x[\"pixel_values\"] for x in batch]),\n",
        "      \"labels\": torch.tensor([x[\"labels\"] for x in batch]),\n",
        "  }"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining the Metrics"
      ],
      "metadata": {
        "id": "mNT_iBYyKGAE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crXIbHCeJYFs"
      },
      "outputs": [],
      "source": [
        "from evaluate import load\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# load the accuracy and f1 metrics from the evaluate module\n",
        "accuracy = load(\"accuracy\")\n",
        "f1 = load(\"f1\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "  # compute the accuracy and f1 scores & return them\n",
        "  accuracy_score = accuracy.compute(predictions=np.argmax(eval_pred.predictions, axis=1), references=eval_pred.label_ids)\n",
        "  f1_score = f1.compute(predictions=np.argmax(eval_pred.predictions, axis=1), references=eval_pred.label_ids, average=\"macro\")\n",
        "\n",
        "  auroc_score = roc_auc_score(eval_pred.label_ids, np.argmax(eval_pred.predictions, axis=1))\n",
        "  print(f\"AUROC Score: {auroc_score:.4f}\")\n",
        "\n",
        "  return {**accuracy_score, **f1_score}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the Model"
      ],
      "metadata": {
        "id": "2WNJaZeYKJZH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1MmFNVLKw_2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1260af86-3277-4a50-a108-c1a7a2b4d076"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--vit-base-patch16-224/snapshots/3f49326eb077187dfe1c2a2bb15fbd74e6ab91e3/config.json\n",
            "Model config ViTConfig {\n",
            "  \"_name_or_path\": \"google/vit-base-patch16-224\",\n",
            "  \"architectures\": [\n",
            "    \"ViTForImageClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.0,\n",
            "  \"encoder_stride\": 16,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.0,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"0\",\n",
            "    \"1\": \"1\"\n",
            "  },\n",
            "  \"image_size\": 224,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"0\": \"0\",\n",
            "    \"1\": \"1\"\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"model_type\": \"vit\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_channels\": 3,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"patch_size\": 16,\n",
            "  \"qkv_bias\": true,\n",
            "  \"transformers_version\": \"4.33.1\"\n",
            "}\n",
            "\n",
            "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--google--vit-base-patch16-224/snapshots/3f49326eb077187dfe1c2a2bb15fbd74e6ab91e3/model.safetensors\n",
            "All model checkpoint weights were used when initializing ViTForImageClassification.\n",
            "\n",
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
            "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
            "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# load the ViT model\n",
        "model = ViTForImageClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=len(labels),\n",
        "    id2label={str(i): c for i, c in enumerate(labels)},\n",
        "    label2id={c: str(i) for i, c in enumerate(labels)},\n",
        "    ignore_mismatched_sizes=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install accelerate -U"
      ],
      "metadata": {
        "id": "ojq7BsY6cbCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install transformers[torch]"
      ],
      "metadata": {
        "id": "gIOh_AB-cmS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNnG1GUUK7nj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9117e84d-bc49-41bd-bf2f-9d4a8e2802b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n",
            "PyTorch: setting up devices\n"
          ]
        }
      ],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "  output_dir=\"/content/output\", # output directory\n",
        "  per_device_train_batch_size=32, # batch size per device during training\n",
        "  evaluation_strategy=\"steps\",    # evaluation strategy to adopt during training\n",
        "  num_train_epochs=10,             # total number of training epochs\n",
        "  # fp16=True,                    # use mixed precision\n",
        "  save_steps=1000,                # number of update steps before saving checkpoint\n",
        "  eval_steps=1000,                # number of update steps before evaluating\n",
        "  logging_steps=1000,             # number of update steps before logging\n",
        "  # save_steps=50,\n",
        "  # eval_steps=50,\n",
        "  # logging_steps=50,\n",
        "  save_total_limit=2,             # limit the total amount of checkpoints on disk\n",
        "  remove_unused_columns=False,    # remove unused columns from the dataset\n",
        "  push_to_hub=False,              # do not push the model to the hub\n",
        "  report_to='tensorboard',        # report metrics to tensorboard\n",
        "  load_best_model_at_end=True,    # load the best model at the end of training\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASOtjmcHdc2R",
        "outputId": "e8d76e05-c4dc-48ab-b884-d31dbb20b3ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['image', 'label'],\n",
              "        num_rows: 26500\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['image', 'label'],\n",
              "        num_rows: 2000\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3n8Cv48_QKzr"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                        # the instantiated 🤗 Transformers model to be trained\n",
        "    args=training_args,                 # training arguments, defined above\n",
        "    data_collator=collate_fn,           # the data collator that will be used for batching\n",
        "    compute_metrics=compute_metrics,    # the metrics function that will be used for evaluation\n",
        "    train_dataset=dataset[\"train\"],     # training dataset\n",
        "    eval_dataset=dataset[\"test\"],       # evaluation dataset\n",
        "    tokenizer=image_processor,          # the processor that will be used for preprocessing the images\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUUoQyh-QPED",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1780211b-a06e-4c24-cc3e-06b032afce0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 26,500\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 8,290\n",
            "  Number of trainable parameters = 85,800,194\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='8290' max='8290' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [8290/8290 3:11:16, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.535500</td>\n",
              "      <td>0.730685</td>\n",
              "      <td>0.669000</td>\n",
              "      <td>0.564416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.284700</td>\n",
              "      <td>0.775324</td>\n",
              "      <td>0.708000</td>\n",
              "      <td>0.665622</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.124100</td>\n",
              "      <td>0.949491</td>\n",
              "      <td>0.725000</td>\n",
              "      <td>0.702146</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.048400</td>\n",
              "      <td>1.274296</td>\n",
              "      <td>0.723000</td>\n",
              "      <td>0.706515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.017800</td>\n",
              "      <td>1.512437</td>\n",
              "      <td>0.753500</td>\n",
              "      <td>0.732612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.004600</td>\n",
              "      <td>1.561816</td>\n",
              "      <td>0.769500</td>\n",
              "      <td>0.743659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.000500</td>\n",
              "      <td>1.709508</td>\n",
              "      <td>0.754000</td>\n",
              "      <td>0.734656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>1.690851</td>\n",
              "      <td>0.754000</td>\n",
              "      <td>0.735117</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 2000\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUROC Score: 0.5829\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to /content/output/checkpoint-1000\n",
            "Configuration saved in /content/output/checkpoint-1000/config.json\n",
            "Model weights saved in /content/output/checkpoint-1000/pytorch_model.bin\n",
            "Image processor saved in /content/output/checkpoint-1000/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2000\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUROC Score: 0.6603\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to /content/output/checkpoint-2000\n",
            "Configuration saved in /content/output/checkpoint-2000/config.json\n",
            "Model weights saved in /content/output/checkpoint-2000/pytorch_model.bin\n",
            "Image processor saved in /content/output/checkpoint-2000/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2000\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUROC Score: 0.6995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to /content/output/checkpoint-3000\n",
            "Configuration saved in /content/output/checkpoint-3000/config.json\n",
            "Model weights saved in /content/output/checkpoint-3000/pytorch_model.bin\n",
            "Image processor saved in /content/output/checkpoint-3000/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2000\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUROC Score: 0.7080\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to /content/output/checkpoint-4000\n",
            "Configuration saved in /content/output/checkpoint-4000/config.json\n",
            "Model weights saved in /content/output/checkpoint-4000/pytorch_model.bin\n",
            "Image processor saved in /content/output/checkpoint-4000/preprocessor_config.json\n",
            "Deleting older checkpoint [/content/output/checkpoint-2000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2000\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUROC Score: 0.7292\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to /content/output/checkpoint-5000\n",
            "Configuration saved in /content/output/checkpoint-5000/config.json\n",
            "Model weights saved in /content/output/checkpoint-5000/pytorch_model.bin\n",
            "Image processor saved in /content/output/checkpoint-5000/preprocessor_config.json\n",
            "Deleting older checkpoint [/content/output/checkpoint-3000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2000\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUROC Score: 0.7361\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to /content/output/checkpoint-6000\n",
            "Configuration saved in /content/output/checkpoint-6000/config.json\n",
            "Model weights saved in /content/output/checkpoint-6000/pytorch_model.bin\n",
            "Image processor saved in /content/output/checkpoint-6000/preprocessor_config.json\n",
            "Deleting older checkpoint [/content/output/checkpoint-4000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2000\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUROC Score: 0.7323\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to /content/output/checkpoint-7000\n",
            "Configuration saved in /content/output/checkpoint-7000/config.json\n",
            "Model weights saved in /content/output/checkpoint-7000/pytorch_model.bin\n",
            "Image processor saved in /content/output/checkpoint-7000/preprocessor_config.json\n",
            "Deleting older checkpoint [/content/output/checkpoint-5000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2000\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUROC Score: 0.7331\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to /content/output/checkpoint-8000\n",
            "Configuration saved in /content/output/checkpoint-8000/config.json\n",
            "Model weights saved in /content/output/checkpoint-8000/pytorch_model.bin\n",
            "Image processor saved in /content/output/checkpoint-8000/preprocessor_config.json\n",
            "Deleting older checkpoint [/content/output/checkpoint-6000] due to args.save_total_limit\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from /content/output/checkpoint-1000 (score: 0.7306849360466003).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=8290, training_loss=0.12253846677449343, metrics={'train_runtime': 11481.0891, 'train_samples_per_second': 23.081, 'train_steps_per_second': 0.722, 'total_flos': 2.053537724786688e+19, 'train_loss': 0.12253846677449343, 'epoch': 10.0})"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "# start training\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akZ0-H5YQSuJ"
      },
      "outputs": [],
      "source": [
        "# trainer.evaluate(dataset[\"test\"])\n",
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xd0XJYKZU5U",
        "outputId": "80eb2e5e-28a5-478a-9bd1-486986c1db4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Define the source folder path (in Colab)\n",
        "source_folder_path = '/content/output/checkpoint-7000'\n",
        "\n",
        "# Define the destination folder path (in Google Drive)\n",
        "destination_folder_path = \"/content/drive/MyDrive/Colab Notebooks/Hateful-Memes/Vit/checkpoint-7000\"\n",
        "\n",
        "# Remove the existing destination folder (if it exists)\n",
        "if os.path.exists(destination_folder_path):\n",
        "    shutil.rmtree(destination_folder_path)\n",
        "\n",
        "# Copy the folder\n",
        "shutil.copytree(source_folder_path, destination_folder_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "pY4jH2jyVc7-",
        "outputId": "25d3f957-f00c-451b-9677-0170cd57a269"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Colab Notebooks/Hateful-Memes/Vit/checkpoint-7000'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAZFCk5Gd1p0"
      },
      "outputs": [],
      "source": [
        "# start tensorboard\n",
        "# %load_ext tensorboard\n",
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir ./vit-base-food/runs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_SsuMpFafPe"
      },
      "source": [
        "## Alternatively: Training using PyTorch Loop\n",
        "Run the two below cells to fine-tune using a regular PyTorch loop if you want."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C29idUGDd2yW"
      },
      "outputs": [],
      "source": [
        "# Training loop\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_dataset_loader = DataLoader(dataset[\"train\"], collate_fn=collate_fn, batch_size=batch_size, shuffle=True)\n",
        "valid_dataset_loader = DataLoader(dataset[\"validation\"], collate_fn=collate_fn, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# define the optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "log_dir = \"./image-classification/tensorboard\"\n",
        "summary_writer = SummaryWriter(log_dir=log_dir)\n",
        "\n",
        "num_epochs = 3\n",
        "model = model.to(device)\n",
        "# print some statistics before training\n",
        "# number of training steps\n",
        "n_train_steps = num_epochs * len(train_dataset_loader)\n",
        "# number of validation steps\n",
        "n_valid_steps = len(valid_dataset_loader)\n",
        "# current training step\n",
        "current_step = 0\n",
        "# logging, eval & save steps\n",
        "save_steps = 1000\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "  accuracy_score = accuracy.compute(predictions=eval_pred.predictions, references=eval_pred.label_ids)\n",
        "  f1_score = f1.compute(predictions=eval_pred.predictions, references=eval_pred.label_ids, average=\"macro\")\n",
        "  return {**accuracy_score, **f1_score}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2v6dNtUcd7-G"
      },
      "outputs": [],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    # set the model to training mode\n",
        "    model.train()\n",
        "    # initialize the training loss\n",
        "    train_loss = 0\n",
        "    # initialize the progress bar\n",
        "    progress_bar = tqdm(range(current_step, n_train_steps), \"Training\", dynamic_ncols=True, ncols=80)\n",
        "    for batch in train_dataset_loader:\n",
        "      if (current_step+1) % save_steps == 0:\n",
        "        ### evaluation code ###\n",
        "        # evaluate on the validation set\n",
        "        # if the current step is a multiple of the save steps\n",
        "        print()\n",
        "        print(f\"Validation at step {current_step}...\")\n",
        "        print()\n",
        "        # set the model to evaluation mode\n",
        "        model.eval()\n",
        "        # initialize our lists that store the predictions and the labels\n",
        "        predictions, labels = [], []\n",
        "        # initialize the validation loss\n",
        "        valid_loss = 0\n",
        "        for batch in valid_dataset_loader:\n",
        "            # get the batch\n",
        "            pixel_values = batch[\"pixel_values\"].to(device)\n",
        "            label_ids = batch[\"labels\"].to(device)\n",
        "            # forward pass\n",
        "            outputs = model(pixel_values=pixel_values, labels=label_ids)\n",
        "            # get the loss\n",
        "            loss = outputs.loss\n",
        "            valid_loss += loss.item()\n",
        "            # free the GPU memory\n",
        "            logits = outputs.logits.detach().cpu()\n",
        "            # add the predictions to the list\n",
        "            predictions.extend(logits.argmax(dim=-1).tolist())\n",
        "            # add the labels to the list\n",
        "            labels.extend(label_ids.tolist())\n",
        "        # make the EvalPrediction object that the compute_metrics function expects\n",
        "        eval_prediction = EvalPrediction(predictions=predictions, label_ids=labels)\n",
        "        # compute the metrics\n",
        "        metrics = compute_metrics(eval_prediction)\n",
        "        # print the stats\n",
        "        print()\n",
        "        print(f\"Epoch: {epoch}, Step: {current_step}, Train Loss: {train_loss / save_steps:.4f}, \" +\n",
        "              f\"Valid Loss: {valid_loss / n_valid_steps:.4f}, Accuracy: {metrics['accuracy']}, \" +\n",
        "              f\"F1 Score: {metrics['f1']}\")\n",
        "        print()\n",
        "        # log the metrics\n",
        "        summary_writer.add_scalar(\"valid_loss\", valid_loss / n_valid_steps, global_step=current_step)\n",
        "        summary_writer.add_scalar(\"accuracy\", metrics[\"accuracy\"], global_step=current_step)\n",
        "        summary_writer.add_scalar(\"f1\", metrics[\"f1\"], global_step=current_step)\n",
        "        # save the model\n",
        "        model.save_pretrained(f\"./vit-base-food/checkpoint-{current_step}\")\n",
        "        image_processor.save_pretrained(f\"./vit-base-food/checkpoint-{current_step}\")\n",
        "        # get the model back to train mode\n",
        "        model.train()\n",
        "        # reset the train and valid loss\n",
        "        train_loss, valid_loss = 0, 0\n",
        "      ### training code below ###\n",
        "      # get the batch & convert to tensor\n",
        "      pixel_values = batch[\"pixel_values\"].to(device)\n",
        "      labels = batch[\"labels\"].to(device)\n",
        "      # forward pass\n",
        "      outputs = model(pixel_values=pixel_values, labels=labels)\n",
        "      # get the loss\n",
        "      loss = outputs.loss\n",
        "      # backward pass\n",
        "      loss.backward()\n",
        "      # update the weights\n",
        "      optimizer.step()\n",
        "      # zero the gradients\n",
        "      optimizer.zero_grad()\n",
        "      # log the loss\n",
        "      loss_v = loss.item()\n",
        "      train_loss += loss_v\n",
        "      # increment the step\n",
        "      current_step += 1\n",
        "      progress_bar.update(1)\n",
        "      # log the training loss\n",
        "      summary_writer.add_scalar(\"train_loss\", loss_v, global_step=current_step)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Performing Inference"
      ],
      "metadata": {
        "id": "5nyMP4VRC_dG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nuOoflvoen7E"
      },
      "outputs": [],
      "source": [
        "# load the best model, change the checkpoint number to the best checkpoint\n",
        "# if the last checkpoint is the best, then ignore this cell\n",
        "best_checkpoint = 7000\n",
        "# best_checkpoint = 150\n",
        "model = ViTForImageClassification.from_pretrained(f\"./vit-base-food/checkpoint-{best_checkpoint}\").to(device)\n",
        "# model = ViTForImageClassification.from_pretrained(f\"./vit-base-skin-cancer/checkpoint-{best_checkpoint}\").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "PwI6sf8PPReE",
        "outputId": "851ba75d-374c-483f-8e32-2fd38de848f0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'sushi'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "get_prediction(model, \"https://images.pexels.com/photos/858496/pexels-photo-858496.jpeg?auto=compress&cs=tinysrgb&w=600&lazy=load\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkmjg6hGQ6DZ"
      },
      "outputs": [],
      "source": [
        "def get_prediction_probs(model, url_or_path, num_classes=3):\n",
        "    # load the image\n",
        "    img = load_image(url_or_path)\n",
        "    # preprocessing the image\n",
        "    pixel_values = image_processor(img, return_tensors=\"pt\")[\"pixel_values\"].to(device)\n",
        "    # perform inference\n",
        "    output = model(pixel_values)\n",
        "    # get the top k classes and probabilities\n",
        "    probs, indices = torch.topk(output.logits.softmax(dim=1), k=num_classes)\n",
        "    # get the class labels\n",
        "    id2label = model.config.id2label\n",
        "    classes = [id2label[idx.item()] for idx in indices[0]]\n",
        "    # convert the probabilities to a list\n",
        "    probs = probs.squeeze().tolist()\n",
        "    # create a dictionary with the class names and probabilities\n",
        "    results = dict(zip(classes, probs))\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0pFDs9CRhqX"
      },
      "outputs": [],
      "source": [
        "# example 1\n",
        "get_prediction_probs(model, \"https://images.pexels.com/photos/406152/pexels-photo-406152.jpeg?auto=compress&cs=tinysrgb&w=600\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urU-gg-gRjkN"
      },
      "outputs": [],
      "source": [
        "# example 2\n",
        "get_prediction_probs(model, \"https://images.pexels.com/photos/920220/pexels-photo-920220.jpeg?auto=compress&cs=tinysrgb&w=600\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHtsyIRLV-3A"
      },
      "outputs": [],
      "source": [
        "# example 3\n",
        "get_prediction_probs(model, \"https://images.pexels.com/photos/3338681/pexels-photo-3338681.jpeg?auto=compress&cs=tinysrgb&w=600\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbO_d45dXtwh"
      },
      "outputs": [],
      "source": [
        "# example 4\n",
        "get_prediction_probs(model, \"https://images.pexels.com/photos/806457/pexels-photo-806457.jpeg?auto=compress&cs=tinysrgb&w=600\", num_classes=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAhzhcbhXyYA"
      },
      "outputs": [],
      "source": [
        "get_prediction_probs(model, \"https://images.pexels.com/photos/1624487/pexels-photo-1624487.jpeg?auto=compress&cs=tinysrgb&w=600\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "aty6TXJPbdzG",
        "H_SsuMpFafPe"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b9d3d38f10924ea5ac34d47264fb4787": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e47b70adf5964deba34f2c10f5fac547",
              "IPY_MODEL_debc703989234afe80a312dab36aabff",
              "IPY_MODEL_bbd997caba7746c8be484e43dfce79d8"
            ],
            "layout": "IPY_MODEL_87c1ce909d8444b9aba6dfd003ec82ed"
          }
        },
        "e47b70adf5964deba34f2c10f5fac547": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9614a15f0e042d5a46ca2b711c3bcd8",
            "placeholder": "​",
            "style": "IPY_MODEL_1376ebeabd0647b09945e0fdf77d0437",
            "value": "Resolving data files: 100%"
          }
        },
        "debc703989234afe80a312dab36aabff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65350b81e5444cd2a97e78322b4d37ab",
            "max": 26500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b824f0b92d9a452ca15e6a0af7248c52",
            "value": 26500
          }
        },
        "bbd997caba7746c8be484e43dfce79d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de24d84e90c3481e8797bbc02aac73f7",
            "placeholder": "​",
            "style": "IPY_MODEL_4fc7a74c320e41d895de86e37b4646de",
            "value": " 26500/26500 [00:00&lt;00:00, 107208.77it/s]"
          }
        },
        "87c1ce909d8444b9aba6dfd003ec82ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9614a15f0e042d5a46ca2b711c3bcd8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1376ebeabd0647b09945e0fdf77d0437": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "65350b81e5444cd2a97e78322b4d37ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b824f0b92d9a452ca15e6a0af7248c52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "de24d84e90c3481e8797bbc02aac73f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4fc7a74c320e41d895de86e37b4646de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8217d438c6804e79906a3769b500b1bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_997bba30ae984a10a8301e7e6cbca7f5",
              "IPY_MODEL_c3e0e82030eb4e14beeffbb896dade8e",
              "IPY_MODEL_10ea7cc2bfed41818c432d39f8ce895e"
            ],
            "layout": "IPY_MODEL_b7866ebd5dce4b3fbb8c0e8b2f99cf1c"
          }
        },
        "997bba30ae984a10a8301e7e6cbca7f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f23538dbbdd149f79c6026bd2248258d",
            "placeholder": "​",
            "style": "IPY_MODEL_2845c5d7cc504819b7c65611ea97c062",
            "value": "Resolving data files: 100%"
          }
        },
        "c3e0e82030eb4e14beeffbb896dade8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_326526eec98143f798ff97623120d37d",
            "max": 2000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4debe888989e4ee183da149e1e683ce7",
            "value": 2000
          }
        },
        "10ea7cc2bfed41818c432d39f8ce895e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_441799ffcab14e4ca3316ffd4c7c8a51",
            "placeholder": "​",
            "style": "IPY_MODEL_17700eeaf5d647d582cbef9c81bda2f9",
            "value": " 2000/2000 [00:00&lt;00:00, 25162.00it/s]"
          }
        },
        "b7866ebd5dce4b3fbb8c0e8b2f99cf1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f23538dbbdd149f79c6026bd2248258d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2845c5d7cc504819b7c65611ea97c062": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "326526eec98143f798ff97623120d37d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4debe888989e4ee183da149e1e683ce7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "441799ffcab14e4ca3316ffd4c7c8a51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17700eeaf5d647d582cbef9c81bda2f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}